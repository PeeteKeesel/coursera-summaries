{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <a id='toc1_'></a>[Explainable Machine Learning](#toc0_)\n",
    "\n",
    "**Skills**\n",
    "1. Implement local explainable techniques like LIME, SHAP, and ICE plots using Python.\n",
    "2. Implement global explainable techniques such as [Partial Dependence Plots](https://scikit-learn.org/stable/modules/partial_dependence.html) (PDP) and [Accumulated Local Effects](https://en.wikipedia.org/wiki/Accumulated_local_effects) (ALE) plots in Python.\n",
    "3. Apply example-based explanation techniques to explain machine learning models using Python.\n",
    "4. Visualize and explain neural network models using SOTA techniques in Python.\n",
    "5. Critically evaluate interpretable attention and [saliency](https://en.wikipedia.org/wiki/Saliency_map) methods for transformer model explanations.\n",
    "6. Explore emerging approaches to explainability for large language models (LLMs) and generative computer vision models.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Table of contents**<a id='toc0_'></a>    \n",
    "- [Explainable Machine Learning](#toc1_)    \n",
    "- [Module 2️⃣](#toc2_)    \n",
    "  - [Visualizing NN Predictions](#toc2_1_)    \n",
    "    - [Feature Visualization](#toc2_1_1_)    \n",
    "      - [Pros & Cons](#toc2_1_1_1_)    \n",
    "    - [Feature Attribution](#toc2_1_2_)    \n",
    "      - [Vanilla Gradient](#toc2_1_2_1_)    \n",
    "        - [Process](#toc2_1_2_1_1_)    \n",
    "      - [Grad-CAM](#toc2_1_2_2_)    \n",
    "        - [Process](#toc2_1_2_2_1_)    \n",
    "      - [Pros & Cons](#toc2_1_2_3_)    \n",
    "  - [Explaining NNs](#toc2_2_)    \n",
    "    - [Network Dissection, 2017](#toc2_2_1_)    \n",
    "      - [Implementation](#toc2_2_1_1_)    \n",
    "      - [Pros & Cons](#toc2_2_1_2_)    \n",
    "    - [Concept Activation Vectors, 2018](#toc2_2_2_)    \n",
    "      - [Pros & Cons](#toc2_2_2_1_)    \n",
    "  - [Explainable Attention](#toc2_3_)    \n",
    "    - [A Review of Attention](#toc2_3_1_)    \n",
    "      - [Review Embeddings](#toc2_3_1_1_)    \n",
    "      - [Self Attention](#toc2_3_1_2_)    \n",
    "      - [Self-Attention vs Cross Attention](#toc2_3_1_3_)    \n",
    "    - [Visualizing Attention](#toc2_3_2_)    \n",
    "    - [Saliency Methods, 2020, as Alternatives](#toc2_3_3_)    \n",
    "      - [Process](#toc2_3_3_1_)    \n",
    "    - [Layer-wise Relevance Propagation, 2019](#toc2_3_4_)    \n",
    "      - [Process](#toc2_3_4_1_)    \n",
    "    - [Occlusion-based Saliency, 2017](#toc2_3_5_)    \n",
    "- [Resources](#toc3_)    \n",
    "\n",
    "<!-- vscode-jupyter-toc-config\n",
    "\tnumbering=false\n",
    "\tanchor=true\n",
    "\tflat=false\n",
    "\tminLevel=1\n",
    "\tmaxLevel=6\n",
    "\t/vscode-jupyter-toc-config -->\n",
    "<!-- THIS CELL WILL BE REPLACED ON TOC UPDATE. DO NOT WRITE YOUR TEXT IN THIS CELL -->"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <a id='toc2_'></a>[Module 2️⃣](#toc0_)\n",
    "\n",
    "## <a id='toc2_1_'></a>[Visualizing NN Predictions](#toc0_)\n",
    "\n",
    "### <a id='toc2_1_1_'></a>[Feature Visualization](#toc0_)\n",
    "> Process of making learned features in a NN explicit.\n",
    ">\n",
    "> Answers the question: *what does this neuron, channel or layer see?*\n",
    "\n",
    "With feature visualization we are trying to maximize the activation of a neuron $h$. \n",
    "\n",
    "$$\n",
    "    img^{*} = argmax_{img} \\underbrace{h_{n,x,y,z}}_{\\text{activation of a neuron}} (\\overbrace{img}^{\\text{input}})\n",
    "$$\n",
    "- $x,y=$ spatial position of neuron\n",
    "- $n=$ layer\n",
    "- $z=$ channel index\n",
    "\n",
    "For the mean activation of an entire channel $z$ in layer $n$: \n",
    "$$\n",
    "    img^{*} = argmax_{img} \\sum_{x,y}h_{n,x,y,z} (img)\n",
    "$$\n",
    "\n",
    "Note that `minimize = maximize(-)`. \n",
    "\n",
    "#### <a id='toc2_1_1_1_'></a>[Pros & Cons](#toc0_)\n",
    "| Pros | Cons |\n",
    "|------|------|\n",
    "| Unique insight into how NNs work | Many feature visualization images contain some features with no human interpretation | \n",
    "| Communicate in a non-technical way how NNs work | For large NNs, difficult to visualize complete network | \n",
    "| | Is not a complete picture of interactions | \n",
    "\n",
    "### <a id='toc2_1_2_'></a>[Feature Attribution](#toc0_)\n",
    "> Indicate how much each feature in your model contributes to a predicition for an instance.\n",
    "\n",
    "**Gradient-based Methods**\n",
    "- Vanilla Gradient (Saliency Maps)\n",
    "- DeconvNet\n",
    "- Grad-CAM\n",
    "- Guided Grad-CAM\n",
    "- SmoothGrad\n",
    "\n",
    "#### <a id='toc2_1_2_1_'></a>[Vanilla Gradient](#toc0_)\n",
    "> $\\textcolor{#ba4e00}{\\textbf{Vanilla Gradient (Saliency Maps)}}$ provides pixel-level importance.\n",
    ">\n",
    "> Calculate the gradient of the loss fucntion for the class of interest wrt the input pixels.\n",
    "\n",
    "##### <a id='toc2_1_2_1_1_'></a>[Process](#toc0_)\n",
    "1. Forward pass image of interest\n",
    "2. Compute the gradient of the class score of interest wrt the input pixels. \n",
    "   - set all other classes to zero.\n",
    "3. Visualize the gradients\n",
    "   - show absolute values or highlight negative/positive contributions.\n",
    "\n",
    "#### <a id='toc2_1_2_2_'></a>[Grad-CAM](#toc0_)\n",
    "> $\\textcolor{#ba4e00}{\\textbf{Grad-CAM}}$ provides region-level importance.\n",
    ">\n",
    "> = gradient weighted class activation map\n",
    ">\n",
    "> Analyzes which regions are activated in the feature maps of the last convolutional layers for a certain classification.\n",
    "\n",
    "**Primary goal** of GradCAM is to understand at which parts of an image a convolutional layer looks for a certain classification.\n",
    "- Does this by analyzing which regions are activated in the feature maps of the last convolutional layers.\n",
    "\n",
    "##### <a id='toc2_1_2_2_1_'></a>[Process](#toc0_)\n",
    "1. Forward prop the input image through the CNN.\n",
    "2. obtain the raw score for the class of interest. \n",
    "   - This is the activation of the neuron before the softmax layer.\n",
    "   - Set all other class activations to zero.\n",
    "3. Then back prop the gradient of the class of interest to the last convolutional layer before the fully connected layers. \n",
    "4. Weight each feature map or pixel by the gradient for the class, and \n",
    "5. Calculate an average of the feature maps, weighted per pixel by the gradient then apply lute to the average feature map.\n",
    "6. To visualize scale values to the interval between 0 and 1, upscale the image and overlay it over the original image.\n",
    "\n",
    "#### <a id='toc2_1_2_3_'></a>[Pros & Cons](#toc0_)\n",
    "\n",
    "| Pros | Cons |\n",
    "|------|------|\n",
    "| Visuals provide easily understandable explanations | Difficult to evaluate explanation (how do we know it is correct?) |\n",
    "| Faster computation thatn methods like LIME or SHAP | Results can be unreliable | \n",
    "| Many methods to choose from | | \n",
    "\n",
    "## <a id='toc2_2_'></a>[Explaining NNs](#toc0_)\n",
    "\n",
    "### <a id='toc2_2_1_'></a>[Network Dissection, 2017](#toc0_)\n",
    "> Links human concepts with individual NN units.\n",
    "\n",
    "- **Key hypothesis**: do convolutional neural networks learn disentangled features?\n",
    "- $\\textcolor{#ba4e00}{\\textbf{Disentangled features}}$ just means that can individual network units detect specific real world concepts.\n",
    "\n",
    "#### <a id='toc2_2_1_1_'></a>[Implementation](#toc0_)\n",
    "1. Get images with human labeled visual concepts. \n",
    "   - These could be pixel-wise labeled images with concepts of different abstraction levels. \n",
    "2. Measure CNN channel activations for images.\n",
    "3. Get the alignment of activations and labeled concepts.\n",
    "\n",
    "#### <a id='toc2_2_1_2_'></a>[Pros & Cons](#toc0_)\n",
    "| Pros | Cons |\n",
    "|------|------|\n",
    "| Expands upon insights from feature visualization | You need datasets that are labeled on the pixel level with the concepts (this takes a lot of effort to collect!) | \n",
    "| Communicate in a non-technical way how NNs work | Many units respond to the same concept and some to no concept at all | \n",
    "| Links units to concepts | Only aligns human concepts with positive activations (not with negative activations of channels) | \n",
    "| Detect concepts beyond the classes in the classification task | | \n",
    "\n",
    "### <a id='toc2_2_2_'></a>[Concept Activation Vectors, 2018](#toc0_)\n",
    "> A numerical representation of a concept in the activation space of a NN layer.\n",
    "\n",
    "$\\textcolor{#ba4e00}{\\textbf{TCAV}}$: For any given concept, TCAV measures the extent of that concept's influence on the model's prediction for a certain class.\n",
    "\n",
    "#### <a id='toc2_2_2_1_'></a>[Pros & Cons](#toc0_)\n",
    "| Pros | Cons |\n",
    "|------|------|\n",
    "| Customizable via concept dataset curation | Performs poorly on shallow NNs (concepts in deeper layers are more separable) | \n",
    "| Provides global explanations | Requires additional annotations to the dataset (costly) | \n",
    "| | Mostly used in images only |\n",
    "\n",
    "## <a id='toc2_3_'></a>[Explainable Attention](#toc0_)\n",
    "\n",
    "### <a id='toc2_3_1_'></a>[A Review of Attention](#toc0_)\n",
    "#### <a id='toc2_3_1_1_'></a>[Review Embeddings](#toc0_)\n",
    "- We want to represent a word as a fixed length vector\n",
    "  - word $\\rightarrow [1,4,5,62,2,33]$\n",
    "\n",
    "> $\\textcolor{#ba4e00}{\\textbf{Embeddings}}$ are a method of converting textual information into vectors of real numbers, capturing semnatic and syntactic aspects of the data.\n",
    ">\n",
    "> - Acts as a compact representation of the original data, capturing its essential aspects. \n",
    "\n",
    "#### <a id='toc2_3_1_2_'></a>[Self Attention](#toc0_)\n",
    "\n",
    "<img src=\"imgs/attention_1.png\" alt=\"Attention-1\" width=\"600\" height=\"200\">\n",
    "\n",
    "> The goal of self attention is to improve the original embeddings (vector embeddings $v_1, v_2, v_3, v_4$) with context. \n",
    "- We would ideally like our output to be new representations that are better than the original representations\n",
    "\n",
    "To get the new better representations we get scores $s_{ij}$ by multiplying each vector with each other.\n",
    "- We then normalize all scores $s_{ij}$, this yields weights $w_{ij}$. We do this s.t. all weights $w_{ij}$ sum to $1$.\n",
    "  - weights $w_{ij}=\\text{normalized scores}$\n",
    "\n",
    "Then reweight all the vectors. \n",
    "<img src=\"imgs/attention_2.png\" alt=\"Attention-2\" width=\"600\" height=\"200\">\n",
    "\n",
    "We do this for each word in our sequence. \n",
    "<img src=\"imgs/attention_3.png\" alt=\"Attention-3\" width=\"600\" height=\"200\">\n",
    "\n",
    "<img src=\"imgs/attention_4.png\" alt=\"Attention-4\" width=\"600\" height=\"200\">\n",
    "\n",
    "Until now, no weights are being trained. We need to **introduce trainable parameters**:\n",
    "\n",
    "<img src=\"imgs/attention_key_query_values.png\" alt=\"Attention-Key-Query-Values\" width=\"600\" height=\"200\">\n",
    "\n",
    "Now we have Key, Query and Value matrices. These are our trainable weights. The matrices have dimension $k \\times k$.\n",
    "\n",
    "<img src=\"imgs/attention_key_query_value_matrix.png\" alt=\"Attention-Key-Query-Value-Matrix\" width=\"600\" height=\"200\">\n",
    "\n",
    "<img src=\"imgs/self_attention_block.png\" alt=\"Self-Attention-Block\" width=\"600\" height=\"200\">\n",
    "\n",
    "> **TLDR**: $\\textcolor{#ba4e00}{\\textbf{Self Attention}}$ is the process of **adding more context**.\n",
    "\n",
    "But do we have enough attention? \n",
    "\n",
    "> $\\textcolor{#ba4e00}{\\textbf{Multi-Head Attention}}$: we parallelize attention mechanisms by having multiple heads $h$.\n",
    "\n",
    "<img src=\"imgs/multi_head_attention.png\" alt=\"Multi-Head-Attention\" width=\"600\" height=\"200\">\n",
    "\n",
    "#### <a id='toc2_3_1_3_'></a>[Self-Attention vs Cross Attention](#toc0_)\n",
    "- Self Attention operates within a single sequence. \n",
    "- $\\textcolor{#ba4e00}{\\textbf{Cross Attention}}$ is used between two different sequences. \n",
    "- How Cross Attention works: \n",
    "  - For each element in one sequence (query sequence), cross-attention computes attention scores based on its relationship with every element in the other sequence (key-value-sequence).\n",
    "  - This mechanism enables the model to selectively focus on relevant parts of the other sequence when generating an output. \n",
    "- Cross-attention is critical for tasks that involve understanding how elements from different sources related to one another.\n",
    "\n",
    "### <a id='toc2_3_2_'></a>[Visualizing Attention](#toc0_)\n",
    "\n",
    "**BertViz**, 2019\n",
    "- Visualizing attention weights illuminates one type of architecture within the model but **does not necessarily provide a direct explanation for predictions**\n",
    "- \n",
    "\n",
    "### <a id='toc2_3_3_'></a>[Saliency Methods, 2020, as Alternatives](#toc0_)\n",
    "> Input $\\textcolor{#ba4e00}{\\textbf{Saliency Methods}}$ reveal why one particular model prediction was made in terms of how relevant each input word was to that prediction.\n",
    "> \n",
    "> - to understand how the input text influences output predictions more directly.\n",
    "\n",
    "$\\textcolor{#ba4e00}{\\textbf{Integrated Gradients}}=$ The path integral of the gradients along the straightline path from the baseline $x'$ to the input $x$.\n",
    "- We consider the straight line path from the baseline $x'$ to the input $x$ and compute the gradients at all possible points along the path. \n",
    "- Integrated gradients are obtained by cumulating these gradients.\n",
    "\n",
    "#### <a id='toc2_3_3_1_'></a>[Process](#toc0_)\n",
    "1. Choose a baseline\n",
    "   - Select a baseline input that represents an absence of features. \n",
    "   - E.g., if the input is an image, the baseline could be an image with all pixels set to zero. \n",
    "2. Generate interpolated points\n",
    "   - Create a series of interpolated inputs between the baseline x prime and the actual input x. This can be done using a linear path.\n",
    "3. Compute your gradients \n",
    "   - For each interpolated input $x_i$\n",
    "   - Compute the gradient of the model's output wrt the input. This tells us how much each input feature at $x_i$ affects the output. \n",
    "4. Average the gradients. \n",
    "   - Average the gradients over all the interpolated inputs.\n",
    "   - This average gradient represents the contribution of each feature along the path from the baseline to the actual input. Then \n",
    "5. Scale by the input difference. \n",
    "   - Multiply each average gradient by the difference between the actual input and the baseline. This scales the gradient contributions to reflect the actual input features. \n",
    "\n",
    "### <a id='toc2_3_4_'></a>[Layer-wise Relevance Propagation, 2019](#toc0_)\n",
    "> Decompose the prediction of a NN computed over a sample down to relevance scores for the single input dimensions of the sample. \n",
    "\n",
    "#### <a id='toc2_3_4_1_'></a>[Process](#toc0_)\n",
    "1. Start with forward pass to obtain output\n",
    "2. Custom backward pass (at each layer redistributed the incoming relevance among inputs of that layer)\n",
    "3. Relevance redistributed until we reach input layers\n",
    "\n",
    "### <a id='toc2_3_5_'></a>[Occlusion-based Saliency, 2017](#toc0_)\n",
    "> Compute input saliency by occluding (erasing) input features and measuring effects on the model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <a id='toc3_'></a>[Resources](#toc0_)\n",
    "\n",
    "- [CNN Visualization](https://adamharley.com/nn_vis/cnn/3d.html)\n",
    "- Paper: [Google Feature Visualization Interactive](https://distill.pub/2017/feature-visualization/)\n",
    "- Paper: [2020, The elephant in the interpretability room: Why use attention as explanation when we have saliency methods](https://arxiv.org/pdf/2010.05607)"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
