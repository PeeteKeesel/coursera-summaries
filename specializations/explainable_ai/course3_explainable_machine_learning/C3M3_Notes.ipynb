{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <a id='toc1_'></a>[Explainable Machine Learning](#toc0_)\n",
    "\n",
    "**Skills**\n",
    "1. Implement local explainable techniques like LIME, SHAP, and ICE plots using Python.\n",
    "2. Implement global explainable techniques such as [Partial Dependence Plots](https://scikit-learn.org/stable/modules/partial_dependence.html) (PDP) and [Accumulated Local Effects](https://en.wikipedia.org/wiki/Accumulated_local_effects) (ALE) plots in Python.\n",
    "3. Apply example-based explanation techniques to explain machine learning models using Python.\n",
    "4. Visualize and explain neural network models using SOTA techniques in Python.\n",
    "5. Critically evaluate interpretable attention and [saliency](https://en.wikipedia.org/wiki/Saliency_map) methods for transformer model explanations.\n",
    "6. Explore emerging approaches to explainability for large language models (LLMs) and generative computer vision models.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Table of contents**<a id='toc0_'></a>    \n",
    "- [Explainable Machine Learning](#toc1_)    \n",
    "- [Module 3️⃣](#toc2_)    \n",
    "  - [XAI in LLMs](#toc2_1_)    \n",
    "    - [XAI in LLM Challenges](#toc2_1_1_)    \n",
    "    - [XAI in LLM Fine-tuning](#toc2_1_2_)    \n",
    "    - [XAI in LLM Prompting](#toc2_1_3_)    \n",
    "  - [XAI in Generative Computer Vision](#toc2_2_)    \n",
    "    - [XAI in Generative CV](#toc2_2_1_)    \n",
    "    - [XAI in GANs](#toc2_2_2_)    \n",
    "    - [XAI in Diffusion Models](#toc2_2_3_)    \n",
    "- [My Questions](#toc3_)    \n",
    "- [Resources](#toc4_)    \n",
    "\n",
    "<!-- vscode-jupyter-toc-config\n",
    "\tnumbering=false\n",
    "\tanchor=true\n",
    "\tflat=false\n",
    "\tminLevel=1\n",
    "\tmaxLevel=6\n",
    "\t/vscode-jupyter-toc-config -->\n",
    "<!-- THIS CELL WILL BE REPLACED ON TOC UPDATE. DO NOT WRITE YOUR TEXT IN THIS CELL -->"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <a id='toc2_'></a>[Module 3️⃣](#toc0_)\n",
    "## <a id='toc2_1_'></a>[XAI in LLMs](#toc0_)\n",
    "### <a id='toc2_1_1_'></a>[XAI in LLM Challenges](#toc0_)\n",
    "### <a id='toc2_1_2_'></a>[XAI in LLM Fine-tuning](#toc0_)\n",
    "### <a id='toc2_1_3_'></a>[XAI in LLM Prompting](#toc0_)\n",
    "\n",
    "## <a id='toc2_2_'></a>[XAI in Generative Computer Vision](#toc0_)\n",
    "### <a id='toc2_2_1_'></a>[XAI in Generative CV](#toc0_)\n",
    "### <a id='toc2_2_2_'></a>[XAI in GANs](#toc0_)\n",
    "### <a id='toc2_2_3_'></a>[XAI in Diffusion Models](#toc0_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <a id='toc3_'></a>[My Questions](#toc0_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <a id='toc4_'></a>[Resources](#toc0_)\n",
    "\n",
    "- [CNN Visualization](https://adamharley.com/nn_vis/cnn/3d.html)\n",
    "- Paper: [Google Feature Visualization Interactive](https://distill.pub/2017/feature-visualization/)\n",
    "- Paper: [2020, The elephant in the interpretability room: Why use attention as explanation when we have saliency methods](https://arxiv.org/pdf/2010.05607)"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
