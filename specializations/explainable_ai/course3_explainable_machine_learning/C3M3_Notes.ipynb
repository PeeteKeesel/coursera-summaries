{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <a id='toc1_'></a>[Explainable Machine Learning](#toc0_)\n",
    "\n",
    "**Skills**\n",
    "1. Implement local explainable techniques like LIME, SHAP, and ICE plots using Python.\n",
    "2. Implement global explainable techniques such as [Partial Dependence Plots](https://scikit-learn.org/stable/modules/partial_dependence.html) (PDP) and [Accumulated Local Effects](https://en.wikipedia.org/wiki/Accumulated_local_effects) (ALE) plots in Python.\n",
    "3. Apply example-based explanation techniques to explain machine learning models using Python.\n",
    "4. Visualize and explain neural network models using SOTA techniques in Python.\n",
    "5. Critically evaluate interpretable attention and [saliency](https://en.wikipedia.org/wiki/Saliency_map) methods for transformer model explanations.\n",
    "6. Explore emerging approaches to explainability for large language models (LLMs) and generative computer vision models.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Table of contents**<a id='toc0_'></a>    \n",
    "- [Explainable Machine Learning](#toc1_)    \n",
    "- [Module 3️⃣](#toc2_)    \n",
    "  - [XAI in LLMs](#toc2_1_)    \n",
    "    - [XAI in LLM Challenges](#toc2_1_1_)    \n",
    "    - [XAI in LLM Fine-tuning](#toc2_1_2_)    \n",
    "    - [XAI in LLM Prompting](#toc2_1_3_)    \n",
    "  - [XAI in Generative Computer Vision](#toc2_2_)    \n",
    "    - [XAI in Generative CV](#toc2_2_1_)    \n",
    "    - [XAI in GANs](#toc2_2_2_)    \n",
    "    - [XAI in Diffusion Models](#toc2_2_3_)    \n",
    "- [My Questions](#toc3_)    \n",
    "- [Resources](#toc4_)    \n",
    "\n",
    "<!-- vscode-jupyter-toc-config\n",
    "\tnumbering=false\n",
    "\tanchor=true\n",
    "\tflat=false\n",
    "\tminLevel=1\n",
    "\tmaxLevel=6\n",
    "\t/vscode-jupyter-toc-config -->\n",
    "<!-- THIS CELL WILL BE REPLACED ON TOC UPDATE. DO NOT WRITE YOUR TEXT IN THIS CELL -->"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <a id='toc2_'></a>[Module 3️⃣](#toc0_)\n",
    "## <a id='toc2_1_'></a>[XAI in LLMs](#toc0_)\n",
    "### <a id='toc2_1_1_'></a>[XAI in LLM Challenges](#toc0_)\n",
    "1. Size and complexity of LLMs\n",
    "   - e.g. GPT4, with unknown number of parameters likely in the trillions makes tracing their reasoning intractable\n",
    "2. High dimensional representations are entangled + abstract\n",
    "3. There exist few or now ground truths\n",
    "### <a id='toc2_1_2_'></a>[XAI in LLM Fine-tuning](#toc0_)\n",
    "> $\\textcolor{#ba4e00}{\\textbf{Fine-tuning}}$: A large base model (typically >1B params) trained on a corpus of unlabeled data is fine-tuned on a smaller dataset with labels or through RLHF.\n",
    "\n",
    "We can break down explanations in LLM fine-tuning into local and global explanations.\n",
    "\n",
    "#### Local Explanations \n",
    "- Feature attribution\n",
    "- Attention-based explantion\n",
    "- Example-based explanation\n",
    "- Natural language explantion\n",
    "\n",
    "##### Feature Attribution\n",
    "Types are:\n",
    "- $\\textcolor{#ba4e00}{\\textbf{Pertubation-based}}$: where you perturb input examples by removing, masking or altering input features. This can be embedding vectors, hidden units, words, or tokens, and then you evaluate model output changes.\n",
    "- $\\textcolor{#ba4e00}{\\textbf{Gradient-based}}$: where you determine the importance of each input feature by analyzing the partial derivatives of the output with respect to each input dimension. \n",
    "  - The magnitude of the derivatives reflects the sensitivity of the output to changes in the input. \n",
    "  - Integrated gradients are the primary approach to gradient-based explanation in LLMs.\n",
    "- $\\textcolor{#ba4e00}{\\textbf{Trans-SHAP}}$ (2023): mainly focuses on adapting SHAP to sub-word text input and providing sequential visualization explanations.\n",
    "  - Adapted SHAP to transformer-based language models.\n",
    "- $\\textcolor{#ba4e00}{\\textbf{Decomposition-based}}$: aim to break down the relevance score into linear contributions from the input. An example of this is layer wise relevance propagation (LRP).\n",
    "\n",
    "##### Example-based Explanations\n",
    "Types are:\n",
    "- $\\textcolor{#ba4e00}{\\textbf{Counterfactual explanation}}$: reveal what would have happened based on certain observed input changes.\n",
    "- $\\textcolor{#ba4e00}{\\textbf{Influential instance}}$: characterize the influence of individual training samples by measuring how much they affect the loss on test points.\n",
    "- $\\textcolor{#ba4e00}{\\textbf{Adversarial example}}$: Neural models are highly vulnerable to carefully crafted small modifications in the input data that can drastically alter the model's predictions, despite being nearly imperceptible to humans. These are called adversarial examples. Adversarial examples expose areas where models fail and are used during training to improve model robustness and accuracy.\n",
    "- $\\textcolor{#ba4e00}{\\textbf{Natural language explantions}}$: Explain a model's decision-making on an input sequence with generated text.\n",
    "  - **Approach**: train a language model using both original textual data and human-annotated explanations.\n",
    "\n",
    "#### Global Explanations \n",
    "The different types of global explantions are:\n",
    "\n",
    "##### Probing-based\n",
    "> $\\textcolor{#ba4e00}{\\textbf{Probing-based explanation}}$ we can look at either classifier-based probing or parameter-free probing. \n",
    "\n",
    "- In ***classifier-based probing***, the process is \n",
    "  1. freeze LLM parameters, then \n",
    "  2. generate representations from the LLM. \n",
    "  3. train a shallow classifier on those representations to predict linguistic properties. \n",
    "  - If you are probing for syntax, you are looking at things like parts of speech, morphology, or dependencies. \n",
    "  - Alternatively, if you are probing for semantics, you are examining coreferences, entities, and relations. \n",
    "- ***Parameter-free probing*** evaluates an LLM directly on tailored datasets without a classifier. It's important to note that, like many of these approaches, probing validity is debated by researchers. High performance may not mean true linguistic understanding by LLMs.\n",
    "\n",
    "###### Probing Process\n",
    "1. Freeze LLM parameters\n",
    "2. Generate representations from the LLM\n",
    "3. Train a shallow classifier on those representations to predict linguistic properties\n",
    "\n",
    "#### Neuron Activation\n",
    "> $\\textcolor{#ba4e00}{\\textbf{Neuron activation explanation}}$: examines individual neurons or dimensions rather than the whole vector space. It involves two steps.\n",
    "\n",
    "1. one identifying important neurons and \n",
    "2. two learning relations between linguistic properties and neurons. \n",
    "- The lack of ground truth annotations makes evaluating component-level explanations challenging.\n",
    "- \n",
    "#### Neuron-Activation Process\n",
    "1. Identify important neurons (unsupervised)\n",
    "2. Learn relations between linguistic properties and the individual ranked neurons in supervised tasks\n",
    "3. Verify via ablation experiments\n",
    "4. Generate natural language explanations\n",
    "5. Test\n",
    "\n",
    "#### Concept-based\n",
    "> $\\textcolor{#ba4e00}{\\textbf{Concept-based explanation}}$: allow us to map the inputs to a set of concepts and measure important scores of each predefined concept to model predictions. Testing concept activation vectors is an example of this.\n",
    "\n",
    "### <a id='toc2_1_3_'></a>[XAI in LLM Prompting](#toc0_)\n",
    "\n",
    "- $\\textcolor{#ba4e00}{\\textbf{Prompting}}$ involves strategically designing task-specific instructions in natural language to guide model output without altering parameters. \n",
    "- $\\textcolor{#ba4e00}{\\textbf{Prompt engineering}}$ includes instruction, context, and user input, which is used to guide the output of the pre-trained large language model.\n",
    "\n",
    "Types of Prompting:\n",
    "\n",
    "- $\\textcolor{#ba4e00}{\\textbf{In-context learning (ICL)}}$: when a model is shown task demonstrations as part of the prompt.\n",
    "- $\\textcolor{#ba4e00}{\\textbf{Chain-of-thought (CoT)}}$: involves prompting the model to describe its reasoning and go step by step through a problem.\n",
    "  - Question we are trying to answer: how does in-context learning influence model behavior?\n",
    "\n",
    "### XAI in Knowledge Augmentation (RAG)\n",
    "\n",
    "RAG Pipeline: \n",
    "<img src=\"imgs/rag_pipeline.png\" alt=\"RAG Pipeline\" width=\"600\">\n",
    "\n",
    "1. $\\textcolor{#ba4e00}{\\textbf{Vector Database}}$: The first concept of our $\\textcolor{#ba4e00}{\\textbf{RAG}}$ pipeline utilizes embeddings. \n",
    "   - A Vector Database is where vector embeddings are stored. \n",
    "   - You typically take a bunch of unstructured data like PDFs, images, or videos, and embed some portion of that data using an embedding model that transforms data into vector embeddings.\n",
    "\n",
    "<img src=\"imgs/rag_vector_db.png\" alt=\"RAG Vector DB\" width=\"600\">\n",
    "\n",
    "2. $\\textcolor{#ba4e00}{\\textbf{User query}}$: Also uses embeddings\n",
    "   - When the user asks a question, the text is transformed into a vector embedding, the same embedding model you use to create your vector database.\n",
    "   - E.g. if my user query is black leather boots, then that query will be embedded using my embedding model, and I will have a vector embedding that represents that query.\n",
    "\n",
    "<img src=\"imgs/rag_user_query.png\" alt=\"RAG User Query\" width=\"600\">\n",
    "\n",
    "3. $\\textcolor{#ba4e00}{\\textbf{Similarity Algorithm}}$: match the user query embedding to my vector database using a similarity algorithm. \n",
    "   - A similarity algorithm is used to find closest matches to the user query in the vector database. \n",
    "   - You can use any distance metric here, such as Euclidean distance. \n",
    "   - However, most people building RAG pipelines use cosine similarity. \n",
    "   - $\\textcolor{#ba4e00}{\\textbf{Cosine Similarity}}$ \n",
    "     - Is scale-invariant, so it can measure the similarity of vectors regardless of the magnitude. \n",
    "     - It's more suitable for high dimensional spaces due to its focus on an angle rather than an absolute distance, which can be affected by the curse of dimensionality. \n",
    "     - It is interpretable because values are bounded between negative one and one. \n",
    "\n",
    "<img src=\"imgs/rag_similarity.png\" alt=\"RAG Similarity\" width=\"600\">\n",
    "\n",
    "4. Add the LMM to the loop.\n",
    "\n",
    "<img src=\"imgs/rag_llm.png\" alt=\"RAG LLM\" width=\"600\">\n",
    "\n",
    "**How to improve Explanability of RAG itself?**\n",
    "1. Make embedding spaces more explainable through visualization.\n",
    "\n",
    "$\\textcolor{#ba4e00}{\\textbf{Latent/Embedding Space}}$: multidimensional space onto which concepts are mapped.\n",
    "- Interestingly we can map different information sources like text and images to the same latent space, as long as we have a dataset that connects them together, like a large dataset of images and captions from the Internet.\n",
    "\n",
    "## <a id='toc2_2_'></a>[XAI in Generative Computer Vision](#toc0_)\n",
    "### <a id='toc2_2_1_'></a>[XAI in Generative CV](#toc0_)\n",
    "For diffusion models and GANs, there is no clear input-output mapping or decision boundary. \n",
    "\n",
    "- XAI for generative computer vision models like GANs and diffusion models is crucial due to their complexity and lack of transparency.\n",
    "- These models learn high-dimensional, entangled representations in their latent spaces, making it difficult to trace their reasoning and understand how they generate images.\n",
    "\n",
    "The source emphasizes the importance of XAI for several reasons:\n",
    "- **Transparency and accountability**: Understanding how these models work can help build trust and ensure responsible use.\n",
    "- **Bias and fairness**: XAI can identify and mitigate biases present in the training data or inherited from sources like the internet.\n",
    "- **Creative control and iteration**: Explanations can empower artists and creators to refine prompts and achieve desired results.\n",
    "- **Safety and ethical considerations**: Explainability can help identify risks associated with the generation of harmful or misleading content.\n",
    "- **Model improvement**: Insights gained from XAI can improve model architecture, training data, and optimization.\n",
    "\n",
    "### <a id='toc2_2_2_'></a>[XAI in GANs](#toc0_)\n",
    "\n",
    "<img src=\"imgs/gans.png\" alt=\"GANs\" width=\"600\">\n",
    "\n",
    "- GANs are a powerful type of generative model used for creating realistic synthetic images.\n",
    "- GANs function through a two-player game between a generator network and a discriminator network. \n",
    "  - The generator aims to create images that resemble the training data, while the discriminator tries to distinguish between real and fake images. \n",
    "    - Discriminator is only used for training and usually discarded after training.\n",
    "  - The competition drives both networks to improve, leading to the generation of high-quality synthetic images.\n",
    "\n",
    "<img src=\"imgs/gan_generator.png\" alt=\"GAN Generator\" width=\"600\">\n",
    "\n",
    "<img src=\"imgs/gan_discriminator.png\" alt=\"GAN Discriminator\" width=\"600\">\n",
    "\n",
    "The latent space are the representations learned by the model.\n",
    "\n",
    "- However, the internal workings of GANs are often complex and difficult to understand, making them \"black boxes.\"\n",
    "- This lack of transparency motivates the need for XAI techniques, which aim to make the decision-making process of GANs more interpretable.\n",
    "- There are several XAI methods, each with unique applications and insights into GAN behavior:\n",
    "- $\\textcolor{#ba4e00}{\\textbf{Network Dissection}}$: This technique involves analyzing the activations of individual neurons in the GAN to identify those that correspond to specific concepts or features in the generated images. For example, researchers were able to isolate neurons that activate in response to the presence of specific objects, like cars or faces, demonstrating the GAN's ability to learn meaningful representations. Network dissection can also be used to manipulate neuron activations, allowing researchers to intentionally modify the generated images.\n",
    "\n",
    "<img src=\"imgs/network_dissection_in_gans.png\" alt=\"Network Dissection in GAN\" width=\"600\">\n",
    "\n",
    "- $\\textcolor{#ba4e00}{\\textbf{Latent-space Visualization and Manipulation}}$: GANs operate in a latent space, a high-dimensional representation of the training data. Visualizing and manipulating this space can provide insights into how the GAN generates images. For example, researchers have used techniques like PCA to identify directions in the latent space that correspond to specific image attributes, such as gender or head rotation. Perturbing the latent space representations in these directions allows for controlled manipulation of the generated images.\n",
    "\n",
    "<img src=\"imgs/gan_latent_space_guided_pertubations.png\" alt=\"Latent Space Guided Pertubations in GAN\" width=\"600\">\n",
    "\n",
    "<img src=\"imgs/gan_latent_space_guided_pertubations_2.png\" alt=\"Latent Space Guided Pertubations in GAN\" width=\"600\">\n",
    "\n",
    "- $\\textcolor{#ba4e00}{\\textbf{Integration of Local Explanations}}$: This approach involves integrating XAI techniques like *saliency maps*, *LIME*, and *SHAP* (DeepSHAP) directly into the training process of the GAN. \n",
    "  - This allows for the generation of explanations alongside the synthetic images, providing insights into the importance of different features in the input for the GAN's decisions. \n",
    "  - An example of this is the XAI-GAN, which uses explanations to guide the training of the generator, potentially improving the quality and interpretability of the generated images.\n",
    "\n",
    "Evaluating the quality of generated images is crucial in XAI for GANs. Metrics like $\\textcolor{#ba4e00}{\\textbf{Inception Score}}$ and $\\textcolor{#ba4e00}{\\textbf{Frechet Inception Distance (FID)}}$ are commonly used for this purpose. \n",
    "- The Inception Score measures both image quality and diversity, \n",
    "- while FID quantifies the similarity between the distributions of real and generated images.\n",
    "\n",
    "### <a id='toc2_2_3_'></a>[XAI in Diffusion Models](#toc0_)\n",
    "\n",
    "- **What are Diffusion Models?** Diffusion models are generative models that excel in image generation tasks. They operate on a principle of gradual noise addition and reversal. An initial image is progressively corrupted by noise until it becomes pure Gaussian noise. The model then learns to reverse this diffusion process, effectively denoising, to generate high-quality images from random noise.\n",
    "  - The key idea behind diffusion models is to define a Markov chain that starts with a data distribution like images, and gradually adds noise until the final state is pure gaussian noise.\n",
    "  - During the reverse process, the model learns to generate samples from the data distribution by removing the noise step-by-step.\n",
    "\n",
    "<img src=\"imgs/diffusion_models.png\" alt=\"Diffusion Models\" width=\"600\">\n",
    "\n",
    "- **How do Diffusion Models Work?**\n",
    "  - **Forward Diffusion:**  The model learns to add noise gradually to the original data, creating a series of increasingly noisy versions.\n",
    "  - **Reverse Diffusion:** The model is trained to reverse this process, starting with pure noise and gradually removing noise to reconstruct the original data distribution. The user's text prompt is encoded and used to guide this denoising process. \n",
    "\n",
    "<img src=\"imgs/diffusion_training_process.png\" alt=\"Diffusion Training Process\" width=\"600\">\n",
    "\n",
    "Methods for visualizing diffusion model image generation:\n",
    "- $\\textcolor{#ba4e00}{\\textbf{diffusion randomized input sampling explanation (DF-RISE)}}$: visualizes the denoising regions that the diffusion model focuses on at each step, by generating saliency maps using randomized input masking and a structural similarity function tailored for generative models.\n",
    "- $\\textcolor{#ba4e00}{\\textbf{diffusion gradient-weighted class activation mapping (DF-CAM)}}$: visualizes the internal workings of the diffusion model by highlighting the specific visual concepts the model attends to at each denoising step using gradient-weighted activation maps.\n",
    "- $\\textcolor{#ba4e00}{\\textbf{Exponential Time Step Sampling}}$: adjusts the time step sampling strategy during the reverse diffusion process to concentrate on specific stages, early, middle, or late, allowing analysis of the visual concepts emphasized at different time stages.\n",
    "- $\\textcolor{#ba4e00}{\\textbf{Relevance Score}}$: relevance score between the generated image and the input text prompt to quantify and analyze the visual concepts manifested at different time steps and stages.\n",
    "\n",
    "<img src=\"imgs/gan_visualization_techniques.png\" alt=\"LIME vs DF-RISe vs DF-CAM\" width=\"600\">\n",
    "\n",
    "- **Components of Diffusion Models:**\n",
    "  - **Variational Autoencoder (VAE):** This component translates images to and from a compressed latent space, acting like a translator trained on a large image dataset. \n",
    "  - **Conditioning Element Encoder:** This encodes the user prompt (text or image) into the latent space.\n",
    "  - **Denoising U-Net:** This guides the reverse diffusion process. It predicts the amount of noise in a random latent image and iteratively removes it, guided by user input.\n",
    "- **Advantages of Diffusion Models:**\n",
    "  - Generally more stable to train than GANs, which can suffer from mode collapse and instabilities.\n",
    "  - Open-source models like Stable Diffusion have democratized access to image generation.\n",
    "- **Explainability in Diffusion Models:** Emerging research focuses on explaining how diffusion models generate images. Some key methods include:\n",
    "  - **Counterfactual Explanations:**  Exploring how changing specific input elements impacts the output.\n",
    "  - **Visualization Techniques:**\n",
    "      - DF-RISE (Diffusion Randomized Input Sampling Explanation) -  Visualizes denoising regions the model focuses on at each step.\n",
    "      - DF-CAM (Diffusion Gradient-weighted Class Activation Mapping) - Highlights specific visual concepts attended to at each denoising step.\n",
    "      - Exponential Time Step Sampling -  Analyzes visual concepts emphasized at different time stages.\n",
    "      - Relevance Score -  Quantifies the connection between generated images and input text prompts.\n",
    "  - **DAAM (Cross-Attention Pixel Attribution Visualization):**  Shows which words in a prompt influence which image regions by analyzing attention scores within the model.\n",
    "- **Variability in Diffusion Models:**  Diffusion models inherently produce variable outputs due to random initialization, probability sampling, and non-linear activations. The level of variability can differ between models based on their architecture and training procedures. \n",
    "- **Quantifying Consistency:** Balancing creativity with consistency is crucial in real-world applications.  Methods like using multimodal embedding models (e.g., CLIP) to measure the similarity between multiple generated images are being explored. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <a id='toc4_'></a>[Resources](#toc0_)\n",
    "\n",
    "- [CNN Visualization](https://adamharley.com/nn_vis/cnn/3d.html)\n",
    "- Paper: [Google Feature Visualization Interactive](https://distill.pub/2017/feature-visualization/)\n",
    "- Paper: [2020, The elephant in the interpretability room: Why use attention as explanation when we have saliency methods](https://arxiv.org/pdf/2010.05607)\n",
    "- Embedding Projector: https://projector.tensorflow.org/\n",
    "- [Explainable AI for Computer Vision (XAI4CV)](https://xai4cv.github.io/)\n",
    "- https://colab.research.google.com/github/SIDN-IAP/global-model-repr/blob/master/notebooks/gandissect_solutions.ipynb "
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
