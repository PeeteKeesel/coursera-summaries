{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Table of contents**<a id='toc0_'></a>    \n",
    "- [Module 1](#toc1_)    \n",
    "- [Module 2](#toc2_)    \n",
    "  - [Challenges & Trade-Offs in Developing AI Systems](#toc2_1_)    \n",
    "  - [Overview of XAI Techniques and Approaches](#toc2_2_)    \n",
    "    - [Explanation Techniques](#toc2_2_1_)    \n",
    "      - [Local Explanations](#toc2_2_1_1_)    \n",
    "        - [LIME - Local interpretable model agnostic explanations](#toc2_2_1_1_1_)    \n",
    "        - [Anchors](#toc2_2_1_1_2_)    \n",
    "        - [SHAP - SHapley Additive exPlanations](#toc2_2_1_1_3_)    \n",
    "        - [ICE - Individual Conditional Expectation](#toc2_2_1_1_4_)    \n",
    "      - [Global Explanations](#toc2_2_1_2_)    \n",
    "        - [Functional decomposition](#toc2_2_1_2_1_)    \n",
    "        - [Feature Interaction](#toc2_2_1_2_2_)    \n",
    "      - [Example-Based Explanations](#toc2_2_1_3_)    \n",
    "        - [Prototype-based Explanations](#toc2_2_1_3_1_)    \n",
    "        - [Counterfactual Explanations](#toc2_2_1_3_2_)    \n",
    "    - [Deep Learning Network Explanations](#toc2_2_2_)    \n",
    "      - [Feature Visualization](#toc2_2_2_1_)    \n",
    "      - [Feature Attribution](#toc2_2_2_2_)    \n",
    "      - [Network Dissection](#toc2_2_2_3_)    \n",
    "      - [Concept Activation Vectors](#toc2_2_2_4_)    \n",
    "  - [XAI in GenAI](#toc2_3_)    \n",
    "  - [Resources](#toc2_4_)    \n",
    "\n",
    "<!-- vscode-jupyter-toc-config\n",
    "\tnumbering=false\n",
    "\tanchor=true\n",
    "\tflat=false\n",
    "\tminLevel=1\n",
    "\tmaxLevel=6\n",
    "\t/vscode-jupyter-toc-config -->\n",
    "<!-- THIS CELL WILL BE REPLACED ON TOC UPDATE. DO NOT WRITE YOUR TEXT IN THIS CELL -->"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <a id='toc1_'></a>[Module 1](#toc0_)\n",
    "- **Transparency**: Provides documentation for the system's decisions and actions (details about model architecture, training data, optimizations, etc.) to ensure compliance and accountability.\n",
    "  - *Examples*: Model Cards, Datasheets for Datasets, Fairness Indicators, Algorithmic Impact Assessments\n",
    "- **Interpretability**: An interpretable model provides both visibility into its mechanisms and insight into how it arrives at its predictions. Provides insights into what features are important, how they are related, or what rules/patterns are learned.\n",
    "  - *Examples*: Inherently Interpretable Model - Decision Trees, Monotonic NNs\n",
    "- **Explainability**: Aims to make any AI system, including opaque DL models, more explainable. Involves developing techniques to explain the outputs/decisions of black-box AI models (usually) after they are trained.\n",
    "  - *Examples*: Post-hoc Explanations - SHAP, Saliency Maps, Concept Activation Vectors\n",
    "\n",
    "**Why is explainability important for AI systems?**\n",
    "1. Trust & accountability \n",
    "2. Identifying biases\n",
    "3. Human-AI collaboration\n",
    "4. Scientific advancement \n",
    "\n",
    "**Algorithmic Bias**\n",
    "- **Training data bias**\n",
    "  - *Example*: If the data used to train an AI model is biased or lacks representation from certain groups, the model may learn and perpetuate those biases. If a facial recognition system is trained primarily on images of people from one particular ethnicity, it may perform poorly on recognizing faces from other ethnic groups. The way data is collected, sampled, or labeled can introduce biases.\n",
    "- **Sample bias**\n",
    "  - The way data is collected, sampled, or labeled can introduce biases\n",
    "  - *Example*: If a dataset for resume screening is predominantly composed of resumes from a particular geographic region or industry, the resulting model may be biased against candidates from other regions or industries.\n",
    "- **Poxy variable bias**\n",
    "  - Sometimes algorithms may rely on proxy variables that are correlated with protected characteristics like race, gender, or age, leading to indirect discrimination against those groups.\n",
    "- **Bias arising from human-AI interaction**:   \n",
    "  - Such as biased language or behavior exhibited by users, which can influence the systems outputs over time.\n",
    "- **Bias resulting from algorithms themselves (i.e. architecture, optimization criteria)**\n",
    "\n",
    "> XAI can detect these biases and discrimination in AI systems.\n",
    "\n",
    "<img src=\"imgs/sources_of_bias.png\" alt=\"Sources of Bias\" width=\"400\">\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <a id='toc2_'></a>[Module 2](#toc0_)\n",
    "\n",
    "## <a id='toc2_1_'></a>[Challenges & Trade-Offs in Developing AI Systems](#toc0_)\n",
    "\n",
    "> High stakes decision-making, use inherently interpretable models over trying to explain black-box models using XAI for \n",
    "\n",
    "- The word *“explanation”* in XAI refers to an understanding of how a model works, as opposed to an explanation of how the world works\n",
    "\n",
    "## <a id='toc2_2_'></a>[Overview of XAI Techniques and Approaches](#toc0_)\n",
    "\n",
    "- **Interpretable Machine Learning**: defined as developing machine learning models that are inherently understandable and self-explanatory.\n",
    "- **Occam's Razor**: If you have two competing ideas to explain the same phenomenon, you should prefer the simpler one.\n",
    "- Linear models are highly interpretable as their predictions are based on a weighted sum of input features.\n",
    "  - The coefficients directly indicate the importance and directionality of each feature.\n",
    "- Enforcing sparsity, for example, via lasso or elastic net regularization, can enhance interpretability by driving coefficients of irrelevant features to zero, effectively performing feature selection.\n",
    "- Generalized models try to solve some of the problems with linear regression. For example, generalized additive models combine linear models with nonlinear feature transformations, while maintaining an additive and interpretable structure, showing how each feature shape impacts predictions.\n",
    "- Regression and generalized models give us interpretable coefficients. \n",
    "- The sign positive or negative of a coefficient indicates whether the associated feature has a positive or negative relationship with the variable. \n",
    "- The magnitude of a coefficient represents the strength of that relationship. \n",
    "- Larger coefficients indicate a stronger influence of that feature on the target variable.\n",
    "- Decision trees like CART and GOSDT are also intrinsically interpretable models. Their hierarchical tree structure of if-then-else decision rules provides a natural way to trace how predictions are made for different instances based on their feature values. Rule-based models encode knowledge in the form of human-readable rules.\n",
    "- Tree-based models split the data multiple times based on certain cutoff values in the features. Different subsets of the data set are created through this splitting with each instance belonging to one subset. The terminal, also known as leaf nodes, are the final subsets. To predict the outcome in each leaf node, the average outcome of the training data in this node is used. \n",
    "\n",
    "There are also interpretable Neural Networks:\n",
    "1. **Disentagled Neural Networks**\n",
    "> models that learn representations where each neuron or feature map corresponds to a specific interpretable concept. For example, edges, textures, or object parts. This can provide visibility into the model's internal reasoning.\n",
    "2. **Prototype-based Networks** (*PropoPNet*)\n",
    "> models that learn prototypical examples of each class and use similarity to these prototypes as the basis for predictions, which can be more interpretable than complex decision boundaries\n",
    "   - integrate prototypes or examples into the neural network\n",
    "1. **Monotonic Neural Networks** (*MonoNet*)\n",
    "> models that constrain the neural network to produce outputs that are monotonically increasing or decreasing with respect to the input features. This can make the model's behavior more intuitive and predictable.\n",
    "   - ensure model predictions vary in a consistent direction as feature change, aligning with human intuitions.\n",
    "1. **Representation Networks** (*Kolmogorov Arnold Network*)\n",
    "> have no linear weights at all. Every weight parameter is replaced by a univariate function parameterized as a spline. KANs can be intuitively visualized and can easily interact with human users.\n",
    "   - introduced alternatives to weights using spline representations. \n",
    "\n",
    "<img src=\"imgs/shallow_sparse_modular_nns.png\" alt=\"Types of NNs 1\" width=\"400\">\n",
    "\n",
    "<img src=\"imgs/disentangled_prototypical_monotonic_nns.png\" alt=\"Types of NNs 2\" width=\"400\">\n",
    "\n",
    "**Mechanistic Interpretability (MI):**\n",
    "> process of reverse engineering neural networks from learned weights down to human interpretable algorithms.\n",
    "\n",
    "Based on three speculative claims about neural networks.\n",
    "1. **Features** are the fundamental unit of neural networks. They correspond to directions or a linear combination of neurons in a layer.\n",
    "2. **Circuits**: Features are connected by weights forming circuits. A circuit is a computational subgraph of a network. A circuit consists of a set of features and the weighted edges that go between them.\n",
    "3. **Universality**: poses that analogous features and circuits form across models and tasks.\n",
    "\n",
    "**Superposition**\n",
    "> According to the superposition hypothesis, neural networks, as we observe them, are simulations of larger networks, where each neuron is a disentangled feature.   \n",
    "\n",
    "### <a id='toc2_2_1_'></a>[Explanation Techniques](#toc0_)\n",
    "\n",
    "There are explainable machine-learning approaches for nearly all models and domains. \n",
    "- **Local explanation methods** like LIME and SHAP focus on explaining individual predictions. \n",
    "   - They approximate the original model's behavior locally around the instance of interest using an interpretable model like linear regression. \n",
    "- In contrast to local explanations, **global explanations** aim to capture overall model behavior. \n",
    "  - Methods like functional decomposition, feature interaction, and partial dependence plots are considered global explanations. \n",
    "- **Example based explanations** operate by providing representative examples to support a model's prediction. \n",
    "  - Key methods include finding the most influential training instances that significantly sway a particular prediction if removed.\n",
    "- **NN Explanations**: Neural networks have their own challenges regarding explainability and thus approaches tailored for specific neural networks. \n",
    "\n",
    "#### <a id='toc2_2_1_1_'></a>[Local Explanations](#toc0_)\n",
    "##### <a id='toc2_2_1_1_1_'></a>[LIME - Local interpretable model agnostic explanations](#toc0_)\n",
    "\n",
    "- use interpretable models to explain individual predictions of a black box machine learning model.\n",
    "- Using LIME for images is really interesting. Image variations are created by segmenting the image into superpixels or interconnected pixels with similar colors, and turning them on or off by replacing each pixel with a user-defined color like gray. The user is also able to specify a probability for turning off a superpixel at each permutation.\n",
    "\n",
    "##### <a id='toc2_2_1_1_2_'></a>[Anchors](#toc0_)\n",
    "\n",
    "> ot of similarities to the lime approach. Again, we are explaining individual predictions, but this time, instead of using a linear model to approximate the local decision boundary as with LIME, we are now finding a decision rule that sufficiently anchors the prediction.\n",
    "\n",
    "##### <a id='toc2_2_1_1_3_'></a>[SHAP - SHapley Additive exPlanations](#toc0_)\n",
    "\n",
    "> The SHAP method proposes to approximate Shapley values instead of outright calculating them\n",
    "\n",
    "##### <a id='toc2_2_1_1_4_'></a>[ICE - Individual Conditional Expectation](#toc0_)\n",
    "\n",
    "> plot one line per instance that displays how the instances prediction changes when a particular feature changes. By visualizing all of our instances, we can improve local explainability, while also gaining better global understanding.\n",
    "\n",
    "#### <a id='toc2_2_1_2_'></a>[Global Explanations](#toc0_)\n",
    "\n",
    "> aim to capture overall model behavior. \n",
    "\n",
    "- include functional decomposition, feature interaction, permutation feature importance, and visualizations, including partial dependence plots and accumulated local effects or ALE plots. \n",
    "\n",
    "##### <a id='toc2_2_1_2_1_'></a>[Functional decomposition](#toc0_)\n",
    "\n",
    "<img src=\"imgs/feature_decomposition.png\" alt=\"Types of NNs 2\" width=\"400\">\n",
    "\n",
    "\n",
    "> divides complex models into simpler constituent parts. Each part or function can then be analyzed separately, making it easier to understand the overall behavior of the model. It breaks a model into the main effects, how each feature affects the prediction, independent of the values in the other feature. Interaction effect, the joint effect of the features. The intercept, what the prediction is when all feature effects are set to zero. \n",
    "\n",
    "##### <a id='toc2_2_1_2_2_'></a>[Feature Interaction](#toc0_)\n",
    "\n",
    "<img src=\"imgs/feature_interaction.png\" alt=\"Types of NNs 2\" width=\"400\">\n",
    "\n",
    "> When we consider decomposition, we first decompose a model prediction into a constant term, a term for each feature, and a term for the interaction between factors.\n",
    "\n",
    "- We use the H-statistic for feature interaction. A partial dependence plot, PDP or PD, shows the marginal effect one or two features have on the predicted outcome of a model. \n",
    "- If this looks familiar, it should. It is the average of the lines of an ICE plot. Just like with ICE plots, the PDP can show the relationship between a feature and the target. PDPs do not have clear interpretations when features are correlated. In real life, features are usually correlated, or relationships between features are not well understood. \n",
    "- This is why accumulated local effects, or ALE, plots were introduced in 2020.\n",
    "  - ALE plots include local effects and accumulation. ALE plots focus on local changes in the prediction when a feature value changes, unlike PDPs, which average out the effects over the entire feature space.\n",
    "  - Instead of plotting the local effects directly, ALE plots accumulate these effects across the range of a feature. This helps in understanding the global trend of how the feature influences predictions.\n",
    "\n",
    "**PDP Plots**\n",
    "> - average out the effects over the entire feature space.\n",
    "\n",
    "**ALE Plots**\n",
    "> - focus on local changes in the prediction when a feature value changes\n",
    "> \n",
    "> - to understand the global trend of how the feature influences predictions\n",
    "\n",
    "#### <a id='toc2_2_1_3_'></a>[Example-Based Explanations](#toc0_)\n",
    "##### <a id='toc2_2_1_3_1_'></a>[Prototype-based Explanations](#toc0_)\n",
    "\n",
    "<img src=\"imgs/prototype_based_explanations.png\" alt=\"Types of NNs 2\" width=\"400\">\n",
    "\n",
    "> - aims to explain the predictions of a black box model by identifying representative examples or prototypes from the data.\n",
    ">\n",
    "> - The main thesis is that we represent the model's knowledge in terms of prototypical instances or patterns.\n",
    "\n",
    "##### <a id='toc2_2_1_3_2_'></a>[Counterfactual Explanations](#toc0_)\n",
    "\n",
    "<img src=\"imgs/counterfactual_explanations.png\" alt=\"Types of NNs 2\" width=\"400\">\n",
    "\n",
    "> describe a causal situation. \n",
    "> \n",
    "> - If x had not occurred, y would not have occurred. \n",
    "> \n",
    "> - We can simulate counterfactuals for predictions of black box machine learning models by changing the feature values of an instance before making the predictions and analyzing how the prediction changes. \n",
    "> \n",
    "> - A counterfactual explanation of a prediction describes the smallest change to the feature values that changes the prediction to a predefined output.\n",
    "\n",
    "### <a id='toc2_2_2_'></a>[Deep Learning Network Explanations](#toc0_)\n",
    "\n",
    "#### <a id='toc2_2_2_1_'></a>[Feature Visualization](#toc0_)\n",
    "> What is happening inside a NN? \n",
    "\n",
    "- is the process of making learned features in a neural network explicit.\n",
    "- answers the question, what does this neuron channel or layer see? \n",
    "- With Feature Visualization, we are looking to maximize the activation of a neuron.\n",
    "\n",
    "#### <a id='toc2_2_2_2_'></a>[Feature Attribution](#toc0_)\n",
    "> indicates how much each feature in your model contributes to a prediction for an instance.\n",
    "\n",
    "<img src=\"imgs/feature_attribution.png\" alt=\"Types of NNs 2\" width=\"400\">\n",
    "\n",
    "#### <a id='toc2_2_2_3_'></a>[Network Dissection](#toc0_)\n",
    "> links human concepts with individual neural network units.\n",
    "\n",
    "<img src=\"imgs/network_dissection.png\" alt=\"Types of NNs 2\" width=\"400\">\n",
    "\n",
    "- implementation of network dissection is fairly straightforward. \n",
    "- First, get images with human labeled visual concepts. These should be pixelwise labeled images with concepts of different abstraction levels. \n",
    "- Second, measure CNN channel activations for images, and \n",
    "- lastly, get the alignment of activations and labeled concepts. \n",
    "\n",
    "This method can be used to probe any convolutional layer.\n",
    "\n",
    "#### <a id='toc2_2_2_4_'></a>[Concept Activation Vectors](#toc0_)\n",
    "> are a numerical representation of a concept in the activation space of a neural network layer.\n",
    "\n",
    "<img src=\"imgs/concept_activation_vectors.png\" alt=\"Types of NNs 2\" width=\"400\">\n",
    "\n",
    "\n",
    "- For any given concept, TCAV measures the extent of that concept's influence on the model's prediction for a certain class."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <a id='toc2_3_'></a>[XAI in GenAI](#toc0_)\n",
    "\n",
    "**Fine-tuning**\n",
    "> A large base model (typically >1B parameters) trained on a corpus of unlabeled data is fine-tuned on a smaller dataset with labels or through RLHF.\n",
    "\n",
    "<img src=\"imgs/fine_tuning.png\" alt=\"\" width=\"400\">\n",
    "\n",
    "<img src=\"imgs/local_vs_global_explanations.png\" alt=\"\" width=\"400\">\n",
    "\n",
    "<img src=\"imgs/local_explanations_figure.png\" alt=\"\" width=\"400\">\n",
    "\n",
    "### Feature Attribution\n",
    "#### Pertubation-based\n",
    "> where you perturb input examples by removing, masking or altering input features. This can be embedding vectors, hidden units, words, or tokens, and then you evaluate model output changes.\n",
    "#### Gradient-based \n",
    "> where you determine the importance of each input feature by analyzing the partial derivatives of the output with respect to each input dimension. The magnitude of the derivatives reflects the sensitivity of the output to changes in the input.\n",
    ">\n",
    "> - Integrated gradients are the primary approach to gradient-based explanations in LLMs\n",
    "#### SHAP\n",
    "#### Decomposition-based\n",
    "> aim to break down the relevance score into linear contributions from the input. \n",
    "> \n",
    "> - An example of this is layer wise relevance propagation or LRP.\n",
    "\n",
    "#### Example-based Explanations\n",
    "##### Counterfactual explanations\n",
    "> reveal what would have happened based on certain observed input changes.\n",
    "##### Influential instance\n",
    "> characterize the influence of individual training samples by measuring how much they affect the loss on test points.\n",
    "##### Adversarial example\n",
    "> = Neural models are highly vulnerable to carefully crafted small modifications in the input data that can drastically alter the model's predictions, despite being nearly imperceptible to humans.\n",
    ">\n",
    "> - expose areas where models fail and are used during training to improve model robustness and accuracy.\n",
    "##### Natural language explanations\n",
    "> Explain a model's decision-making on an input sequence with generated text. \n",
    "> \n",
    "> - The approach is to train a language model using both original textual data and human-annotated explanations.\n",
    "\n",
    "#### Global Explanations\n",
    "##### Probing-based explanation\n",
    ">  we can look at either classifier-based probing or parameter-free probing.\n",
    ">\n",
    "1. Freeze LLM parameters \n",
    "2. Generate representations from the LLM\n",
    "3. Train a shallow classifier on those representations to predict linguistic properties\n",
    "\n",
    "##### Neuron activation explanation\n",
    "> examines individual neurons or dimensions rather than the whole vector space. \n",
    "> \n",
    "It involves two steps,  \n",
    "1. identifying important neurons. \n",
    "2. learn relations between linguistic properties and individual ranked neurons in supervised tasks.\n",
    "3. Verify via ablation experiments.\n",
    "4. Generate natural language explanations.\n",
    "5. Test how well they allow the model to simulate the real neurons activation behavior on new test examples.\n",
    "\n",
    "##### Concept-based explanation\n",
    "> allow us to map the inputs to a set of concepts and measure important scores of each predefined concept to model predictions.\n",
    "\n",
    "### Prompting \n",
    "#### In-context learning\n",
    "> When a model is shown tasks demonstrations as part of the prompt.\n",
    "#### Chain-of-thought (CoT)\n",
    "> Prompting the model to describe its reaosning and go step-by-step through a problem.\n",
    "\n",
    "### Embeddings \n",
    "- Embeddings = method of converting textual information into vectors of real numbers, capturing semantic and syntactic aspects of the data. \n",
    "- Embeddings are mapped into a multidimensional space that we call embedding or latent space\n",
    "- They capture semantic relationships, making it possible for words with similar meanings to have similar representations.\n",
    "\n",
    "**Key Features**\n",
    "- Dimensionality reduction\n",
    "- Captures semantics\n",
    "- Encodes meaning based on word usage, context and distance measures\n",
    "\n",
    "### RAG\n",
    "\n",
    "<img src=\"imgs/rag_pipeline.png\" alt=\"\" width=\"400\">\n",
    "\n",
    "RAG Pipeline:\n",
    "\n",
    "1. you have a vector database that is created by embedding your unstructured data.\n",
    "2. you have a user query that is also embedded using the same embedding model as the one used to create your vector database.\n",
    "3. You use a similarity algorithm to find the closest matches between items in your vector database and your user query.\n",
    "4. The closest matches are then incorporated into your prompt and sent to the LLM.\n",
    "5. The LLM generates a response to the user's query, and this response is sent back to the user.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Notes\n",
    "\n",
    "**Cosine Similarity is Scale Invariant**\n",
    "- it can measure the similarity of vectors regardless of the magnitude\n",
    "\n",
    "**How can one visualize the latent space?**\n",
    "- **PCA** = Principal Component Analysis\n",
    "  - to simplify and find global linear relationships and patterns in the data\n",
    "- **t-SNE** = t-distributed Stochastic Neighbor Embedding\n",
    "  - involves constructing a lower dimensional representation where similar data points are placed closer together.\n",
    "  - use t-SNE to emphasize visualization, reveal local patterns and clusters.\n",
    "- **UMAP** = Uniform manifold approximation and projection\n",
    "  - uses manifold learning and non-linear dimensionality reduction technique to understand the underlying structure or shape of the data.\n",
    "  - focuses on campturing non-linear relationships in the data\n",
    "  - You should use UMAP to preserve local structure and handle complex non-linear relationships.\n",
    "- **PaCMAP**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <a id='toc2_4_'></a>[Resources](#toc0_)\n",
    "\n",
    "- [Stop Explaining Black Box Machine Learning Models for High Stakes Decisions and Use Interpretable Models Instead](https://arxiv.org/pdf/1811.10154)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
