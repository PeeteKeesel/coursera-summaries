{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Module 1\n",
    "- **Transparency**: Provides documentation for the system's decisions and actions (details about model architecture, training data, optimizations, etc.) to ensure compliance and accountability.\n",
    "  - *Examples*: Model Cards, Datasheets for Datasets, Fairness Indicators, Algorithmic Impact Assessments\n",
    "- **Interpretability**: An interpretable model provides both visibility into its mechanisms and insight into how it arrives at its predictions. Provides insights into what features are important, how they are related, or what rules/patterns are learned.\n",
    "  - *Examples*: Inherently Interpretable Model - Decision Trees, Monotonic NNs\n",
    "- **Explainability**: Aims to make any AI system, including opaque DL models, more explainable. Involves developing techniques to explain the outputs/decisions of black-box AI models (usually) after they are trained.\n",
    "  - *Examples*: Post-hoc Explanations - SHAP, Saliency Maps, Concept Activation Vectors\n",
    "\n",
    "**Why is explainability important for AI systems?**\n",
    "1. Trust & accountability \n",
    "2. Identifying biases\n",
    "3. Human-AI collaboration\n",
    "4. Scientific advancement \n",
    "\n",
    "**Algorithmic Bias**\n",
    "- **Training data bias**\n",
    "  - *Example*: If the data used to train an AI model is biased or lacks representation from certain groups, the model may learn and perpetuate those biases. If a facial recognition system is trained primarily on images of people from one particular ethnicity, it may perform poorly on recognizing faces from other ethnic groups. The way data is collected, sampled, or labeled can introduce biases.\n",
    "- **Sample bias**\n",
    "  - The way data is collected, sampled, or labeled can introduce biases\n",
    "  - *Example*: If a dataset for resume screening is predominantly composed of resumes from a particular geographic region or industry, the resulting model may be biased against candidates from other regions or industries.\n",
    "- **Poxy variable bias**\n",
    "  - Sometimes algorithms may rely on proxy variables that are correlated with protected characteristics like race, gender, or age, leading to indirect discrimination against those groups.\n",
    "- **Bias arising from human-AI interaction**:   \n",
    "  - Such as biased language or behavior exhibited by users, which can influence the systems outputs over time.\n",
    "- **Bias resulting from algorithms themselves (i.e. architecture, optimization criteria)**\n",
    "\n",
    "> XAI can detect these biases and discrimination in AI systems.\n",
    "\n",
    "<img src=\"imgs/sources_of_bias.png\" alt=\"Sources of Bias\" width=\"400\">\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
