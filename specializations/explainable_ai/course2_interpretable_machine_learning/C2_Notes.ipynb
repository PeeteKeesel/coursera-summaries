{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <a id='toc1_'></a>[Interpretable Machine Learning](#toc0_)\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Table of contents**<a id='toc0_'></a>    \n",
    "- [Interpretable Machine Learning](#toc1_)    \n",
    "- [Module 1️⃣](#toc2_)    \n",
    "  - [Introduction to Interpretable ML](#toc2_1_)    \n",
    "  - [Regression Models](#toc2_2_)    \n",
    "    - [Linear Regression](#toc2_2_1_)    \n",
    "      - [Pros & Cons](#toc2_2_1_1_)    \n",
    "      - [Assumptions](#toc2_2_1_2_)    \n",
    "    - [Logistic Regression](#toc2_2_2_)    \n",
    "      - [Assumptions](#toc2_2_2_1_)    \n",
    "      - [Logistic Function](#toc2_2_2_2_)    \n",
    "      - [Logit Function](#toc2_2_2_3_)    \n",
    "      - [Log Odds](#toc2_2_2_4_)    \n",
    "      - [Pros & Cons](#toc2_2_2_5_)    \n",
    "  - [Generalized (Linear) Model](#toc2_3_)    \n",
    "    - [Generalized Additive Models](#toc2_3_1_)    \n",
    "- [Module 2️⃣](#toc3_)    \n",
    "  - [Decision Trees](#toc3_1_)    \n",
    "    - [CART](#toc3_1_1_)    \n",
    "      - [Implementation](#toc3_1_1_1_)    \n",
    "        - [Variance](#toc3_1_1_1_1_)    \n",
    "        - [Gini Index](#toc3_1_1_1_2_)    \n",
    "      - [Interpreting CART](#toc3_1_1_2_)    \n",
    "    - [Sparse Decision Trees](#toc3_1_2_)    \n",
    "  - [Decision Rules and RuleFit](#toc3_2_)    \n",
    "  - [Neural Network Interpretability](#toc3_3_)    \n",
    "    - [PotoPNet, 2019](#toc3_3_1_)    \n",
    "    - [MonoNet, 2023](#toc3_3_2_)    \n",
    "    - [KAN, 2024](#toc3_3_3_)    \n",
    "- [Module 3️⃣](#toc4_)    \n",
    "  - [Mechanistic Interpretability Concepts](#toc4_1_)    \n",
    "    - [Introduction to Mechanistic Interpretability](#toc4_1_1_)    \n",
    "    - [Mechanistic Interpretability Concepts](#toc4_1_2_)    \n",
    "  - [Circuits and Superposition](#toc4_2_)    \n",
    "    - [The Superposition Hypothesis, 2022](#toc4_2_1_)    \n",
    "  - [Representation Learning & LLMs](#toc4_3_)    \n",
    "    - [Merchanistic Interpretability Representation Learning](#toc4_3_1_)    \n",
    "- [Resources](#toc5_)    \n",
    "\n",
    "<!-- vscode-jupyter-toc-config\n",
    "\tnumbering=false\n",
    "\tanchor=true\n",
    "\tflat=false\n",
    "\tminLevel=1\n",
    "\tmaxLevel=6\n",
    "\t/vscode-jupyter-toc-config -->\n",
    "<!-- THIS CELL WILL BE REPLACED ON TOC UPDATE. DO NOT WRITE YOUR TEXT IN THIS CELL -->"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <a id='toc2_'></a>[Module 1️⃣](#toc0_)\n",
    "\n",
    "**Learning Objectives**\n",
    "- Describe interpretable machine learning and differentiate between interpretability and explainability.\n",
    "- Explain and implement regression models in Python.\n",
    "- Demonstrate knowledge of generalized models in Python.\n",
    "\n",
    "## <a id='toc2_1_'></a>[Introduction to Interpretable ML](#toc0_)\n",
    "**Interpretability**\n",
    "> An interpretable model provides both visibility into its mechanisms and insiht into how it arrives at its predictions. Provides insights into what features are important, how they are related, or what rules/patterns are learned \n",
    ">\n",
    "> *Examples:* Inherently interpretable models - Decision Trees, Monotonic NNs\n",
    "\n",
    "## <a id='toc2_2_'></a>[Regression Models](#toc0_)\n",
    "### <a id='toc2_2_1_'></a>[Linear Regression](#toc0_)\n",
    "> The goal of Lin Reg is to create a linear model that minimizes the sum of squared residuals. \n",
    "\n",
    "$$\n",
    "    SSE = \\sum_{i=1}^{n}(y_i - \\hat{y}_i)^2\n",
    "$$\n",
    "\n",
    "**Ordinary Least Squares**\n",
    "> goal is to find the line or hyperplane in higher dimensions that best fits the observable data points by minimizing the sum of squared residuals.\n",
    "\n",
    "- Relies on several assumptions: \n",
    "  1. relationship between the predictors and outcomes is linear\n",
    "  2. Observations are independent of one another.\n",
    "  3. Residuals have constant variance across all levels of predictors = homoscedasticity\n",
    "  4. Residuals are normally distributed\n",
    "\n",
    "**Regression**\n",
    "> A methodology used for modeling and analysis of numerical data.\n",
    ">\n",
    "> - relationships between 2+ variables are evaluated\n",
    "\n",
    "<img src=\"imgs/regression_formula.png\" alt=\"Sources of Bias\" width=\"400\">\n",
    "\n",
    "$$\n",
    "    y = \\beta_0 + \\beta_1 X_1 + \\dots + \\beta_j X_j + \\epsilon\n",
    "$$\n",
    "\n",
    "**How to interpret the coefficients $\\beta$?**\n",
    "- $\\pm$: \n",
    "  - indicates whether the associated feature has a positive or negative relationship with the target variable.\n",
    "- magnitude: \n",
    "  - represents the strength of that relationship. \n",
    "  - Larger coefficients indicate a stronger influence of that feature on the target variable.\n",
    "\n",
    "**Feature importance of features in Lin Reg:**\n",
    "> = absolute value of the features t-statistic\n",
    ">\n",
    "> t-statistic = estimated weight scaled with its standard error.\n",
    "\n",
    "$$\n",
    "  t_{\\hat{\\beta_j}} = \\frac{\\hat{\\beta_j}}{SE(\\hat{\\beta_j})}\n",
    "$$\n",
    "\n",
    "- **Effect Plot**\n",
    "  - calculate the effects, which is the weight per feature times the feature value of an instance\n",
    "\n",
    "#### <a id='toc2_2_1_1_'></a>[Pros & Cons](#toc0_)\n",
    "\n",
    "| Pros | Cons |\n",
    "|------|------|\n",
    "| How predictions are produced is transparent | Can only represent linear relationships |\n",
    "| Lots of documnetation, used widely across domains | Usually not as accurate because the real world is complex and nonlinear |\n",
    "| Based on solid statistical theory | The interpretation of a weight is dependent on other features |\n",
    "\n",
    "#### <a id='toc2_2_1_2_'></a>[Assumptions](#toc0_)\n",
    "- Linearity (lin relationship between $X$ and $y$)\n",
    "- Independence (observations are independent to one another)\n",
    "- Homoscedasticity (variance of the residual errors is constant across all values of the independent variables)\n",
    "- Normality (residual errors follow $\\mathcal{N}$)\n",
    "- No multicollinearity (independent variables should not be highly correlated with each other)\n",
    "- No autocorrelation (the residual errors are not correlated with each other)\n",
    "- No endogeneity (independent variables are not correlated with the error term)\n",
    "\n",
    "### <a id='toc2_2_2_'></a>[Logistic Regression](#toc0_)\n",
    "> wraps lin reg eq in a logistic fct. \n",
    "> \n",
    "> - squeezes outputs on lin reg to $[0, 1]$.\n",
    "> - = lin model for the log odds \n",
    "\n",
    "- **log-odds**\n",
    "  - **odds** = probability or likelihood of a particular outcome\n",
    "    - e.g. $\\mathbb{P}$ of binary class $1$\n",
    "\n",
    "$$\n",
    "  ln( \\underbrace{\\frac{\\mathbb{P}(y=1)}{1 - \\mathbb{P}(y=1)})}_{\\text{Odds}} = \\underbrace{log(\\frac{\\mathbb{P}(y=1)}{\\mathbb{P}(y=0)})}_{LogOdds} = \\beta_0 + \\beta_1x_1 + \\dots + \\beta_p x_p \n",
    "  \\\\\n",
    "  \\frac{\\mathbb{P}(y=1)}{1 - \\mathbb{P}(y=1)}) = Odds = exp(\\beta_0 + \\beta_1x_1 + \\dots + \\beta_p x_p)\n",
    "$$\n",
    "\n",
    "#### <a id='toc2_2_2_1_'></a>[Assumptions](#toc0_)\n",
    "- Linearity \n",
    "- No multicollinearity \n",
    "- Independence of observations \n",
    "- No influential outliers \n",
    "- Absence of perfect separation\n",
    "- Large sample size\n",
    "\n",
    "#### <a id='toc2_2_2_2_'></a>[Logistic Function](#toc0_)\n",
    "> used to model the probability of a binary outcome in logistic regression. \n",
    "> \n",
    "> transforms the linear combination of the input features into a probability value 0-1. \n",
    "\n",
    "$$\n",
    "  \\sigma(z) = \\frac{1}{1 + e^{-z}}\n",
    "$$\n",
    "\n",
    "#### <a id='toc2_2_2_3_'></a>[Logit Function](#toc0_)\n",
    "> inverse of the logistic function. \n",
    "> \n",
    "> transforms the probability of the binary outcome back into the log odds, a linear scale\n",
    "\n",
    "$$\n",
    "logit(p) = log(\\frac{p}{1-p})\n",
    "$$\n",
    "\n",
    "#### <a id='toc2_2_2_4_'></a>[Log Odds](#toc0_)\n",
    "> logarithm of the odds of the probability of an event occurring.\n",
    "> \n",
    "> The odds themselves are the ratio of the probability of the event occurring to the probability of the event not occurring. \n",
    ">\n",
    "> - Odds > 1 = positive\n",
    "> - Odds < 1 = negative\n",
    "\n",
    "$$\n",
    "  LogOdds = log(\\frac{p}{1-p})\n",
    "$$\n",
    "\n",
    "#### <a id='toc2_2_2_5_'></a>[Pros & Cons](#toc0_)\n",
    "\n",
    "| Pros | Cons |\n",
    "|------|------|\n",
    "| How predictions are produced is transparent | Can only represent linear relationships |\n",
    "| Lots of documnetation, used widely across domains | Usually not as accurate because the real world is complex and nonlinear |\n",
    "| Based on solid statistical theory | Interpretation more difficult than lin reg because the interpreation of the weights is multiplicative and not additive | \n",
    "| Can give you probabilities in addition to classification | If there is a feature that would perfectly separate the two classes, the weight for that feature would not converge and the model wouldn't be able to be trained. Because the optimal weight would be infinite. | \n",
    "\n",
    "\n",
    "\n",
    "## <a id='toc2_3_'></a>[Generalized (Linear) Model](#toc0_)\n",
    "\n",
    "> Idea: Keep the weighted sum of the features, but allow non-Gaussian outcome distributions and connect the expected mean of this distribution and the weighted sum through a possibly nonlinear function.\n",
    "\n",
    "$$\n",
    "  \\overbrace{g}^{\\text{link function}} ( \\underbrace{\\mathbb{E}_Y(y|x)}_{\\text{probability distribution from the exponential famility that defines} \\; E_Y} ) = x^T\\beta\n",
    "$$\n",
    "\n",
    "- If target outcome does not follow a Gaussian distribution\n",
    "- Logistic regression\n",
    "  - is a GLM that assumes the Bernoullu distribution and uses the logit function as its link function\n",
    "\n",
    "| Pros | Cons |\n",
    "|------|------|\n",
    "| How predictions are produced is transparent | Most modifications of the linear model make the model less interpretable | \n",
    "| Lots of documentation, used widely across domains | Any link function complicates interpretation|\n",
    "| Based on solid statistical theory | | \n",
    "| Allows modeling of non-Gaussian outcomes | |\n",
    "\n",
    "### <a id='toc2_3_1_'></a>[Generalized Additive Models](#toc0_)\n",
    ">  if the relationship between our features and outcome is not linear\n",
    "\n",
    "*If the relationship between our features and outcome is not linear?* we could \n",
    "- transform feature\n",
    "- categorize feature\n",
    "- use GAMs\n",
    "\n",
    "$$\n",
    "  f(\\mathbb{E}_Y(y|x)) = \\beta_0 + f_1(x_1) + f_2(x_2) + \\dots + f_p(x_p)\n",
    "$$\n",
    "\n",
    "- We can learn the functions $f$ by using *Spline Functions*\n",
    "> **Splines** are constructed from simpler basis functions, and used to approximate more complex functions\n",
    "\n",
    "| Pros | Cons |\n",
    "|------|------|\n",
    "| How predictions are produced is transparent | Most modifications of the linear model make the model less interpretable | \n",
    "| Lots of documentation, used widely across domains | | \n",
    "| Based on solid statistical theory | | \n",
    "| Allows nonlinear relationships to be modeled | |"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <a id='toc3_'></a>[Module 2️⃣](#toc0_)\n",
    "\n",
    "**Learning Objectives**\n",
    "- Explain and implement decision trees in Python.\n",
    "- Demonstrate knowledge of decision rules in Python.\n",
    "- Define and explain neural network interpretable model approaches, including prototype-based networks, monotonic networks, and Kolmogorov-Arnold networks.\n",
    "\n",
    "## <a id='toc3_1_'></a>[Decision Trees](#toc0_)\n",
    "> Tree-based models split the data multiple times based on certain cutoff values in the features.\n",
    "\n",
    "### <a id='toc3_1_1_'></a>[CART](#toc0_)\n",
    "> = Classification & Regression Trees\n",
    "\n",
    "$$\n",
    "    \\hat{y} = \\hat{f}(x) = \\sum_{m=1}^{M} c_m I\\{x \\in R_m\\}\n",
    "$$\n",
    "\n",
    "- $\\hat{y}=$ predicted value for a given instance $x$\n",
    "- $x=$ instance\n",
    "- $M=$ total number of leaf nodes (or subsets) in the model\n",
    "- $R_m=$ subset (leaf node), each instance $x$ falls into one subset\n",
    "- $c_m=$ constant value associated with the $m$-th leaf node $R_m$\n",
    "- $I_{\\{ x \\in R_m \\}}=$ is the function that returns $1$ if $x$ is in the subset $R_m$ and $0$ otherwise\n",
    "\n",
    "#### <a id='toc3_1_1_1_'></a>[Implementation](#toc0_)\n",
    "The implementation of the CART algorithm starts with the \n",
    "- splitting of the data into different subsets based on feature values. \n",
    "- CART determines the cutoff point for a feature that ... of the target variable.\n",
    "  - ... minimizes the variance for regression, or \n",
    "  - ... Gini index for classification \n",
    "- This process of splitting is done recursively. \n",
    "- At each node, the algorithm selects the best feature and the best value of that feature to split the data, \n",
    "  - based on some criterion like the \n",
    "    - Gini impurity for classification or the \n",
    "    - MSE for regression. \n",
    "\n",
    "##### <a id='toc3_1_1_1_1_'></a>[Variance](#toc0_)\n",
    "> measures how spread out the target values are around the mean in a node. \n",
    "\n",
    "##### <a id='toc3_1_1_1_2_'></a>[Gini Index](#toc0_)\n",
    "> measures the impurity or class distribution in a node\n",
    "\n",
    "#### <a id='toc3_1_1_2_'></a>[Interpreting CART](#toc0_)\n",
    "> If your instance lies in the subset (leaf node), the predicted outcome is the mean value of $y$ of the instances in that node\n",
    "\n",
    "- If we want to examine feature importance, we look at all the splits for which the feature was used, measure how much it has reduced the variance or Gini index compared to the parent node.\n",
    "- The sum of all importances is scaled to $100$, so each importance can be interpreted as share of the overall model importance.\n",
    "\n",
    "| Pros | Cons |\n",
    "|------|------|\n",
    "| Great for capturing interactions between features in the data | Trees don't deal well with linear relationships (they create step functions, which are inefficient) |\n",
    "| A natural visualization | Unstable (Small changes in the training dataset can create a completely different tree. Why? Because each split depends on the parent split.) | \n",
    "| You can use features without any transformations | As trees get larger, they become harder to interpret | \n",
    "\n",
    "### <a id='toc3_1_2_'></a>[Sparse Decision Trees](#toc0_)\n",
    "\n",
    "**GOSDT** = generalized and scalable optimal sparse decision trees\n",
    "- Solves the problem of Decision Trees: \n",
    "  - DTs are created from the top down and pruned. This means that if you make a mistake at the top, that will propagate through your whole model.\n",
    "- *Idea*: \n",
    "  - Minimize an objective that is a function of tree and data over all possible trees you can construct on this data. \n",
    "  - We want to minimize both the \n",
    "    - misclassification error and the \n",
    "    - number of leaves to encourage sparsity and thus interpretability.\n",
    "- Can be summarized into *Branch and Bound* (= idea is reducing the search space of all DTs)\n",
    "  - Analytical bounds help in efficiently pruning the search space\n",
    "    1. **Hierarchical Objective Lower Bound** \n",
    "       - If $R_{bestSoFar} < b(tree_{fixed})$ then this tree and its children are all suboptimal \n",
    "    2. **Hierarchical Objective Lower Bound with Lookahead**\n",
    "       - If $R_{bestSoFar} < b(tree_{fixed}) + C$ (our tree with at least one child) then this tree & chuildren are suboptimal\n",
    "    3. **Leaf Bound**\n",
    "       - Max # leaves of any optimal child tree $< \\# leaves(tree) + (R_{bestSoFar} - b(tree))/C$\n",
    "    4. **Incremental Progress Bound(s)**\n",
    "       - Each split must provide a reduction in loss of at least $C$ \n",
    "- Scalable, due to its bit vector representation, which represents each sub-problem by its contents as bit vectors\n",
    "\n",
    "## <a id='toc3_2_'></a>[Decision Rules and RuleFit](#toc0_)\n",
    "\n",
    "> = simple IF-THEN statements consisting of a condition and a prediction learned through an algorithm.\n",
    "\n",
    "One important concept when implementing decision rules is the trade off between coverage and support and accuracy in confidence.\n",
    "- **Coverage/Support**\n",
    "  - Percentage of instances to which the condition of a rule applies.\n",
    "- **Accuracy**\n",
    "  - Measure of how accurate the rule is in predicting the correct class for the instances to which the condition of the rule applies. \n",
    "\n",
    "Algorithms for learning rules: \n",
    "- **OneR**\n",
    "  - From all the features, OneR selected the one that carries the most information about the outcome of interest and creates decision rules from this feature.\n",
    "- **Sequential Covering**\n",
    "  - A general procedure that iteratively learns rules and removes the data points that are covered by the new rule.\n",
    "\n",
    "| Pros | Cons | \n",
    "|------|------|\n",
    "| Very easy to interpret | Mostly used only for classification |\n",
    "| More compact than decision trees | Features need to be categorical | \n",
    "| Prediction is fast | Some algorithms are prone to overfitting | \n",
    "| Usually automatically generate sparse models | Don't deal well with linear relationships (they create step functions, which are inefficient) | \n",
    "\n",
    "- **RuleFit**\n",
    "  - Combines decision trees with linear models by generating rules from an ensemble of decision trees and then fitting a sparse linear model on those rules\n",
    "  - Learns sparse linear model with the original features AND also a number of new features that are decision rules.\n",
    "  - [Steps](https://www.coursera.org/learn/interpretable-machine-learning/lecture/53KLa/rulefit)\n",
    "    1. Generate Rules\n",
    "    2. Create Sparse Linear Model\n",
    "\n",
    "| Pros | Cons | \n",
    "|------|------|\n",
    "| Easy to interpret | Interpretability worsens with increasing number of features in the model. |\n",
    "| Adds feature interactions to linear models. | Interpretation is tricky for overlapping rules. | \n",
    "| Works for both classification and regression |  | \n",
    "\n",
    "## <a id='toc3_3_'></a>[Neural Network Interpretability](#toc0_)\n",
    "\n",
    "<img src=\"imgs/types_of_interpretable_nns.png\" alt=\"Sources of Bias\" width=\"400\">\n",
    "\n",
    " Improved interpretability in neural networks comes in the form of \n",
    " - **shallow neural networks**,\n",
    "   - simpler neural network architectures with fewer layers and nodes that can be more interpretable than deeper and more complex models\n",
    "   - The relationships between inputs and outputs are more readily apparent\n",
    " - **sparse neural networks**, and \n",
    "   - models that have many network connections pruned or set to zero, and they can be more interpretable as the remaining connections represent the most important features\n",
    " - **modular neural networks**.\n",
    "   - models composed of specialized subcomponents, for example, object detection or classification or reasoning. These can be more interpretable than monolithic architectures.\n",
    "\n",
    "But there are also inherently interpretable models:\n",
    "- **disentangled neural networks**, \n",
    "  - models that learn representations where each neuron or feature map corresponds to a specific interpretable concept, like edges, textures, or object parts\n",
    "  - this can provide visibility into the model's internal reasonin\n",
    "- **prototype based neural networks**, and \n",
    "  - models that learn prototypical examples of each class and use similarity to these prototypes as the basis for predictions, \n",
    "  - this can be more interpretable than complex decision boundaries\n",
    "- **monotonic neural networks**\n",
    "  - models that constrain the neural network to produce outputs that are monotonically increasing or decreasing with respect to the input features\n",
    "  - this can make the model's behavior more intuitive and predictable\n",
    "\n",
    "### <a id='toc3_3_1_'></a>[PotoPNet, 2019](#toc0_)\n",
    "> = Prototype-based Neural Networks\n",
    ">\n",
    "> An inherently interpretable, prototype-based neural network, specifically for image classification.\n",
    "\n",
    "<img src=\"imgs/prototype_based_explanations.png\" alt=\"Sources of Bias\" width=\"400\">\n",
    "\n",
    "- Prototype-based explanations aim to explain the predictions of a black box model by identifying representative examples or prototypes from the data. \n",
    "- The main thesis is that we represent the model's knowledge in terms of prototypical instances or patterns.\n",
    "\n",
    "> The main thesis is that we represent the model's knowledge in terms of prototypical instances or patterns. \n",
    "> \n",
    "> - When adding layers within your neural network that do this, you create an inherently interpretable model, rather than explaining the predictions of a black box model using prototypes\n",
    "\n",
    "**Architecture**\n",
    "\n",
    "<img src=\"imgs/protopnet_architecture.png\" alt=\"Sources of Bias\" width=\"400\">\n",
    "\n",
    "- Architecture\n",
    "  - convolutional layers for feature extraction\n",
    "  - prototype layer to learn prototypical parts, and finally, a \n",
    "  - fully connected layer for classification.\n",
    "- Training\n",
    "  - When training ProtoPNet, stochastic gradient descent is used to learn a meaningful latent space, and prototypes are projected to the nearest training patches. \n",
    "  - Then convex optimization is used for the last layer weights.\n",
    "\n",
    "| Pros | Cons | \n",
    "|------|------|\n",
    "| Interpretable by design (not post-hoc) | Accuracy gap still exists with best models | \n",
    "| Comparable accuracy to baseline models | Additional training complexity | \n",
    "| Combined models reach state-of-the-art accuracy | | \n",
    "\n",
    "### <a id='toc3_3_2_'></a>[MonoNet, 2023](#toc0_)\n",
    "> = Monotonic Neural Networks\n",
    "\n",
    "- **Monotonic Relationship**: As the value of a specific feature increases (or decreases), the predicted output should either increase or decrease accordingly.\n",
    "\n",
    "- contains monotonically connected layers that ensure monotonic relationships between high level features and outputs\n",
    "\n",
    "- MonoNet consists of two different blocks, \n",
    "  - an unconstrained block, which learns arbitrary representations, and \n",
    "  - a monotonic block that enforces a monotonic relationship between the hidden layer and output.\n",
    "\n",
    "| Pros | Cons | \n",
    "|------|------|\n",
    "| Retains approximation capabilities of deep networks | Requires additional effort for interpreting unconstrained block |\n",
    "| Enahnces interpretability via monotonic constraints | Monotonocity constraint may be too restrictive for some tasks |\n",
    "| | Not fully interpretable due to unconstrained block |\n",
    "\n",
    "### <a id='toc3_3_3_'></a>[KAN, 2024](#toc0_)\n",
    "> = Kolmogorov-Arnold Networks\n",
    "> \n",
    "> KANs have no linear weights at all - every weight parameter is replaced by a univariate function parametirized as a spline. \n",
    "\n",
    "- KANs can be intuitively visualized and can easily interact with human users.\n",
    "\n",
    "<img src=\"imgs/kans_process.png\" alt=\"Sources of Bias\" width=\"400\" height=\"300\">\n",
    "\n",
    "**Process**\n",
    "1. We pass inputs to univariate function denoted by Phi, and then we sum them. \n",
    "2. To train a KAN, we first randomly initialize parameters of [B-splines](https://en.wikipedia.org/wiki/B-spline) for each function in each layer. \n",
    "3. Then we forward pass x through the network. \n",
    "4. We calculate loss with respect to ground truth, and then we perform back propagation to update the B-spline parameters, and we repeat the process. \n",
    "\n",
    "- They are inspired by the [Kolmogorov-Arnold representation theorem](https://en.wikipedia.org/wiki/Kolmogorov%E2%80%93Arnold_representation_theorem), which states that any multivariate continuous function can be expressed as a superposition of univariate continuous functions and a simple addition operation.\n",
    "- KANs leverage this idea to approximate complex multivariate functions using a structured network with fewer layers compared to traditional neural networks.\n",
    "\n",
    "**Intuition**\n",
    "- Decompose Complexity: Instead of directly learning high-dimensional mappings, KANs break the task into simpler one-dimensional functions.\n",
    "- Efficiency: They use a fixed architecture to efficiently approximate any continuous function with theoretical guarantees of representation.\n",
    "- Structure: The network typically consists of two stages:\n",
    "  1. A set of univariate functions applied to linear combinations of the inputs.\n",
    "  2. A final layer that sums and applies another univariate function.\n",
    "\n",
    "| Pros | Cons | \n",
    "|------|------|\n",
    "| KANs are able to be more sparse than traditional NNs | Currently slow to train and not GPU-efficient | \n",
    "| KANs are very simple and easy to understand | New, so their robustness on high dimensional data is still unknown | \n",
    "| | Requires sparsification and pruning to be interpretable |"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <a id='toc4_'></a>[Module 3️⃣](#toc0_)\n",
    "\n",
    "## <a id='toc4_1_'></a>[Mechanistic Interpretability Concepts](#toc0_)\n",
    "### <a id='toc4_1_1_'></a>[Introduction to Mechanistic Interpretability](#toc0_)\n",
    "**Hypothesis**\n",
    "> NNs do learn understandable algorithms but have no incentives to make these algorithms representable in a way that humans can understand.\n",
    "\n",
    "**Mechanistic Interpretability**\n",
    "> Process of reverse engineering NNs from learned weights down to human-interpretable algorithms.\n",
    "\n",
    "Sepculative Claims:\n",
    "- **Features**: \n",
    "  - fundamental unit of neural networks\n",
    "  - correspond to directions or a linear combination of neurons in a layer\n",
    "- **Circuits**:\n",
    "  - computational subgraph of a network\n",
    "  - consists of a set of features and the weighted edges that go between them\n",
    "- **Universality**:\n",
    "  - poses that analogous features and circuits form across models and tasks\n",
    "\n",
    "\n",
    "### <a id='toc4_1_2_'></a>[Mechanistic Interpretability Concepts](#toc0_)\n",
    "\n",
    "MI defines the following:\n",
    "- **Feature**:\n",
    "  - The linear representation hypothesis suggests that NNs represent meaningful concepts (features) as directions in their activation spaces.\n",
    "- **Circuit**:\n",
    "  - The subset of a model's weights and nonlinearities used to map a set of earlier features to a set of later features.\n",
    "\n",
    "We make assumptions about the NNs structure because we cannot physically take a NN apart to exploit structure: \n",
    "1. **Decomposability**:\n",
    "   - says that network representations can be described in terms of independently understandable features.\n",
    "2. **Linearity**:\n",
    "   - says that features are represented by directions.\n",
    "3. **Universality**:\n",
    "   - Different models trained to do similar tasks on similar data with architectures are likely to converge on the same way to represent some information (the same circuits will show up in different models)\n",
    "\n",
    "**Superposition**\n",
    "- can be thought of as neural network simulating larger networks, which pushes features away from corresponding to neurons. \n",
    "- Under superposition, features cannot align with the basis because the model embeds more features than there are neurons.\n",
    "\n",
    "**MI Methods**\n",
    "- **Pertubation-based methods**\n",
    "  - involve perturbing or modifying the input, activations, or internal components of the model to observe the changes in the output or behavior, thereby attributing importance or causality to specific components or features. \n",
    "  - Examples include ablation, activation patching, causal tracing, path patching, and causal scrubbing\n",
    "- **Output attribution**\n",
    "  - aims to attribute the final output, logics, or loss to specific input features or internal components, providing insights into their contributions or importance\n",
    "- **Scoring and probing methods**\n",
    "  - compute scores or metrics that quantify certain properties or behaviors of the model, such as the degree of compositionality or the inductive biases present in the model.\n",
    "- **Visualization and Intepretation Methods**\n",
    "  - visualizing or identifying the input features or examples that maximize or strongly activate specific components in the model\n",
    "- **Representation Learning Methods**\n",
    "  - like dictionary learning with sparse autoencoders, learns interpretable representations or decompositions of the model's internal representations, potentially revealing the underlying factors or components that the model is relying on.\n",
    "- **Architectural Modifications**\n",
    "  - like linearizing layer norm involves modifying or linearizing specific architectural components of the model to facilitate interpretability or disentanglement of the model's internal representations.\n",
    "\n",
    "## <a id='toc4_2_'></a>[Circuits and Superposition](#toc0_)\n",
    "\n",
    "- Evidence for Features \n",
    "  - Curve detectors \n",
    "  - High-low frequency detectors \n",
    "  - Pose-invariant dog head detector\n",
    "- Evidence for Circuits \n",
    "  - Curve detectors \n",
    "  - Oriented dog head detection\n",
    "  - Cars in superposition \n",
    "- Evidence for Universality \n",
    "  - Observations \n",
    "\n",
    "### <a id='toc4_2_1_'></a>[The Superposition Hypothesis, 2022](#toc0_)\n",
    "https://www.coursera.org/learn/interpretable-machine-learning/lecture/5CG1R/the-superposition-hypothesis\n",
    "\n",
    "- Ideally: each Neuron maps to one feature\n",
    "- In Reality: One neuron can map to multiple features that are seemingly unrelated\n",
    "\n",
    "We have no idea how to match the neuron to a specific feature or feature set. \n",
    "\n",
    "> $\\textcolor{violet}{\\text{According to the superposition hypothesis, neural networks, as we observe them, are simulations of larger networks, where each neuron is a disentangled feature.}}$\n",
    "\n",
    "We are compressing a bunch of information into a neural network that is decompressing that data, learning it, and then compressing it back into the observed neural network. According to the hypothesis, these idealized neurons are projected onto the actual network as almost orthogonal vectors over the neurons.\n",
    "\n",
    "<img src=\"imgs/superposition_sparsity.png\" alt=\"Sources of Bias\" width=\"600\" height=\"250\">\n",
    "\n",
    "> As sparsity increases, models use superposition to represent more features than dimensions.\n",
    "\n",
    "*Can NNs noisily represent more features than they have neurons?*\n",
    "- As sparsity increases, we start to see superposition emerge.\n",
    "- The model represents more features by having them not be orthogonal to each other.\n",
    "- It starts with less important features and gradually affects the more important ones.\n",
    "\n",
    "When model is dense, all neurons are dedicated to a single feature.\n",
    "\n",
    "> When we increase sparsity, polysemantic features start to emerge. \n",
    "\n",
    "For interpretability, we need to be able to resolve superposition? \n",
    "\n",
    "**How to resolve superposition?**\n",
    "- Create models without superposition.\n",
    "- Find an overcomplete basis that describes how features are represented in models with superposition.\n",
    "- Hybrid approaches in which one changes models, not resolving superposition, but making it easier for a second stage of analysis to find an overcomplete basis that describes it.\n",
    "\n",
    "## <a id='toc4_3_'></a>[Representation Learning & LLMs](#toc0_)\n",
    "\n",
    "### <a id='toc4_3_1_'></a>[Merchanistic Interpretability Representation Learning](#toc0_)\n",
    "\n",
    "The autoencoder architecture consists of two main components, an encoder and a decoder. \n",
    "- The encoder maps the input data to a lower dimensional representation, known as latent or hidden representation. This is also called the bottleneck. \n",
    "- The decoder then attempts to reconstruct the original input data from this latent representation. \n",
    "In sparse autoencoders, the hidden layers of the encoder are encouraged to be sparse, meaning that only a few neurons are active or non zero for any given input. \n",
    "\n",
    "There are two common ways to impose sparsity in sparse autoencoders.\n",
    "1. L1 Regularization\n",
    "   - the L1 norm of the activations in the hidden layer is added to the loss function as a regularization term. This encourages the activations to be sparse, as the L1 norm promotes sparsity by pushing the weights toward zero.\n",
    "2. KL Divergence \n",
    "   - the average activation of the neurons in the hidden layer is constrained to be close to a desired small value, like 0.05. This is achieved by adding a term to the loss function that minimizes the Kullback-Leibler, or KL divergence between the observed average activation and the desired sparse activation.\n",
    "\n",
    "By imposing sparsity constraints, sparse autoencoders are encouraged to learn a compact and informative representation of the input data. \n",
    "- The sparse representation can provide better interpretability and robustness compared to dense representations. \n",
    "\n",
    "> **Universality** refers to the speculative claim that analogous features and circuits form across models and tasks and this is not evidence of universality."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <a id='toc5_'></a>[Resources](#toc0_)\n",
    "\n",
    "- [Interpretable Machine Learning: Fundamental\n",
    "Principles and 10 Grand Challenges](https://arxiv.org/pdf/2103.11251)\n",
    "- [GitHub - imodels](https://github.com/csinva/imodels?tab=readme-ov-file) - Python package for concise, transparent, and accurate predictive modeling.\n",
    "- [Google Colab - Transformer Lens](https://colab.research.google.com/github/neelnanda-io/TransformerLens/blob/main/demos/Main_Demo.ipynb#scrollTo=3tJuK9FYQ5lI)\n",
    "- Ablation - https://dynalist.io/d/n2ZWtnoYHrU1s4vnFSAQ519J#z=fh-HJyz1CgUVrXuoiban6bYx\n",
    "- [distill.pub - Zoom In: An Introduction to Circuits](https://distill.pub/2020/circuits/zoom-in/)\n",
    "- [Toy Models of Superposition](https://transformer-circuits.pub/2022/toy_model/index.html)\n",
    "- OpenAI - [Feature Visualization Tool](https://openaipublic.blob.core.windows.net/sparse-autoencoder/sae-viewer/index.html#/)"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
