{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Interpretable Machine Learning \n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Module 1 \n",
    "## Introduction to Interpretable ML\n",
    "**Interpretability**\n",
    "> An interpretable model provides both visibility into its mechanisms and insiht into how it arrives at its predictions. Provides insights into what features are important, how they are related, or what rules/patterns are learned \n",
    ">\n",
    "> *Examples:* Inherently interpretable models - Decision Trees, Monotonic NNs\n",
    "\n",
    "## Regression Models \n",
    "### Linear Regression\n",
    "> The goal of Lin Reg is to create a linear model that minimizes the sum of squared residuals. \n",
    "\n",
    "$$\n",
    "    SSE = \\sum_{i=1}^{n}(y_i - \\hat{y}_i)^2\n",
    "$$\n",
    "\n",
    "**Ordinary Least Squares**\n",
    "> goal is to find the line or hyperplane in higher dimensions that best fits the observable data points by minimizing the sum of squared residuals.\n",
    "\n",
    "- Relies on several assumptions: \n",
    "  1. relationship between the predictors and outcomes is linear\n",
    "  2. Observations are independent of one another.\n",
    "  3. Residuals have constant variance across all levels of predictors = homoscedasticity\n",
    "  4. Residuals are normally distributed\n",
    "\n",
    "**Regression**\n",
    "> A methodology used for modeling and analysis of numerical data.\n",
    ">\n",
    "> - relationships between 2+ variables are evaluated\n",
    "\n",
    "<img src=\"imgs/regression_formula.png\" alt=\"Sources of Bias\" width=\"400\">\n",
    "\n",
    "$$\n",
    "    y = \\beta_0 + \\beta_1 X_1 + \\dots + \\beta_j X_j + \\epsilon\n",
    "$$\n",
    "\n",
    "**How to interpret the coefficients $\\beta$?**\n",
    "- $\\pm$: \n",
    "  - indicates whether the associated feature has a positive or negative relationship with the target variable.\n",
    "- magnitude: \n",
    "  - represents the strength of that relationship. \n",
    "  - Larger coefficients indicate a stronger influence of that feature on the target variable.\n",
    "\n",
    "**Feature importance of features in Lin Reg:**\n",
    "> = absolute value of the features t-statistic\n",
    ">\n",
    "> t-statistic = estimated weight scaled with its standard error.\n",
    "\n",
    "$$\n",
    "  t_{\\hat{\\beta_j}} = \\frac{\\hat{\\beta_j}}{SE(\\hat{\\beta_j})}\n",
    "$$\n",
    "\n",
    "- **Effect Plot**\n",
    "  - calculate the effects, which is the weight per feature times the feature value of an instance\n",
    "\n",
    "#### Pros & Cons\n",
    "\n",
    "| Pros | Cons |\n",
    "|------|------|\n",
    "| How predictions are produced is transparent | Can only represent linear relationships |\n",
    "| Lots of documnetation, used widely across domains | Usually not as accurate because the real world is complex and nonlinear |\n",
    "| Based on solid statistical theory | The interpretation of a weight is dependent on other features |\n",
    "\n",
    "#### Assumptions\n",
    "- Linearity (lin relationship between $X$ and $y$)\n",
    "- Independence (observations are independent to one another)\n",
    "- Homoscedasticity (variance of the residual errors is constant across all values of the independent variables)\n",
    "- Normality (residual errors follow $\\mathcal{N}$)\n",
    "- No multicollinearity (independent variables should not be highly correlated with each other)\n",
    "- No autocorrelation (the residual errors are not correlated with each other)\n",
    "- No endogeneity (independent variables are not correlated with the error term)\n",
    "\n",
    "### Logistic Regression\n",
    "> wraps lin reg eq in a logistic fct. \n",
    "> \n",
    "> - squeezes outputs on lin reg to $[0, 1]$.\n",
    "> - = lin model for the log odds \n",
    "\n",
    "- **log-odds**\n",
    "  - **odds** = probability or likelihood of a particular outcome\n",
    "    - e.g. $\\mathbb{P}$ of binary class $1$\n",
    "\n",
    "$$\n",
    "  ln( \\underbrace{\\frac{\\mathbb{P}(y=1)}{1 - \\mathbb{P}(y=1)})}_{\\text{Odds}} = \\underbrace{log(\\frac{\\mathbb{P}(y=1)}{\\mathbb{P}(y=0)})}_{LogOdds} = \\beta_0 + \\beta_1x_1 + \\dots + \\beta_p x_p \n",
    "  \\\\\n",
    "  \\frac{\\mathbb{P}(y=1)}{1 - \\mathbb{P}(y=1)}) = Odds = exp(\\beta_0 + \\beta_1x_1 + \\dots + \\beta_p x_p)\n",
    "$$\n",
    "\n",
    "#### Assumptions\n",
    "- Linearity \n",
    "- No multicollinearity \n",
    "- Independence of observations \n",
    "- No influential outliers \n",
    "- Absence of perfect separation\n",
    "- Large sample size\n",
    "\n",
    "#### Logistic Function \n",
    "> used to model the probability of a binary outcome in logistic regression. \n",
    "> \n",
    "> transforms the linear combination of the input features into a probability value 0-1. \n",
    "\n",
    "$$\n",
    "  \\sigma(z) = \\frac{1}{1 + e^{-z}}\n",
    "$$\n",
    "\n",
    "#### Logit Function \n",
    "> inverse of the logistic function. \n",
    "> \n",
    "> transforms the probability of the binary outcome back into the log odds, a linear scale\n",
    "\n",
    "$$\n",
    "logit(p) = log(\\frac{p}{1-p})\n",
    "$$\n",
    "\n",
    "#### Log Odds\n",
    "> logarithm of the odds of the probability of an event occurring.\n",
    "> \n",
    "> The odds themselves are the ratio of the probability of the event occurring to the probability of the event not occurring. \n",
    ">\n",
    "> - Odds > 1 = positive\n",
    "> - Odds < 1 = negative\n",
    "\n",
    "$$\n",
    "  LogOdds = log(\\frac{p}{1-p})\n",
    "$$\n",
    "\n",
    "#### Pros & Cons \n",
    "\n",
    "| Pros | Cons |\n",
    "|------|------|\n",
    "| How predictions are produced is transparent | Can only represent linear relationships |\n",
    "| Lots of documnetation, used widely across domains | Usually not as accurate because the real world is complex and nonlinear |\n",
    "| Based on solid statistical theory | Interpretation more difficult than lin reg because the interpreation of the weights is multiplicative and not additive | \n",
    "| Can give you probabilities in addition to classification | If there is a feature that would perfectly separate the two classes, the weight for that feature would not converge and the model wouldn't be able to be trained. Because the optimal weight would be infinite. | \n",
    "\n",
    "\n",
    "\n",
    "## Generalized (Linear) Model\n",
    "\n",
    "> Idea: Keep the weighted sum of the features, but allow non-Gaussian outcome distributions and connect the expected mean of this distribution and the weighted sum through a possibly nonlinear function.\n",
    "\n",
    "$$\n",
    "  \\overbrace{g}^{\\text{link function}} ( \\underbrace{\\mathbb{E}_Y(y|x)}_{\\text{probability distribution from the exponential famility that defines} \\; E_Y} ) = x^T\\beta\n",
    "$$\n",
    "\n",
    "- If target outcome does not follow a Gaussian distribution\n",
    "- Logistic regression\n",
    "  - is a GLM that assumes the Bernoullu distribution and uses the logit function as its link function\n",
    "\n",
    "| Pros | Cons |\n",
    "|------|------|\n",
    "| How predictions are produced is transparent | Most modifications of the linear model make the model less interpretable | \n",
    "| Lots of documentation, used widely across domains | Any link function complicates interpretation|\n",
    "| Based on solid statistical theory | | \n",
    "| Allows modeling of non-Gaussian outcomes | |"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Resources \n",
    "\n",
    "- [Interpretable Machine Learning: Fundamental\n",
    "Principles and 10 Grand Challenges](https://arxiv.org/pdf/2103.11251)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
