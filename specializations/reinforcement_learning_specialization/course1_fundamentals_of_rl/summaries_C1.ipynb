{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Table of contents**<a id='toc0_'></a>    \n",
    "- [Course 1Ô∏è‚É£ - Fundamentals of Reinforcement Learning](#toc1_)    \n",
    "  - [Week üïê](#toc1_1_)    \n",
    "    - [The K-Armed Bandit Problem](#toc1_1_1_)    \n",
    "      - [Sequential Decision Making with Evaluative Feedback](#toc1_1_1_1_)    \n",
    "    - [What to Learn? Estimating Action Values](#toc1_1_2_)    \n",
    "      - [Learning Action Values](#toc1_1_2_1_)    \n",
    "      - [Estimating Action Values Incrementally](#toc1_1_2_2_)    \n",
    "    - [Exploration vs Exploitation Tradeoff](#toc1_1_3_)    \n",
    "      - [What is the trade-off?](#toc1_1_3_1_)    \n",
    "      - [Optimistic Initial Values](#toc1_1_3_2_)    \n",
    "      - [Upper-Confidence Bound (UCB) Action Selection](#toc1_1_3_3_)    \n",
    "  - [Week üïë](#toc1_2_)    \n",
    "    - [Introduction to Markov Decision Processes](#toc1_2_1_)    \n",
    "    - [The Goal of Reinforcement Learning](#toc1_2_2_)    \n",
    "    - [Michael Littman: The Reward Hypothesis](#toc1_2_3_)    \n",
    "    - [Continuing Tasks](#toc1_2_4_)    \n",
    "  - [Week üïí](#toc1_3_)    \n",
    "  - [Week üïì](#toc1_4_)    \n",
    "\n",
    "<!-- vscode-jupyter-toc-config\n",
    "\tnumbering=false\n",
    "\tanchor=true\n",
    "\tflat=false\n",
    "\tminLevel=1\n",
    "\tmaxLevel=6\n",
    "\t/vscode-jupyter-toc-config -->\n",
    "<!-- THIS CELL WILL BE REPLACED ON TOC UPDATE. DO NOT WRITE YOUR TEXT IN THIS CELL -->"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This page contains summaries and personal notes obtained during my studying for the [Reinforcement Learning Specializations](https://www.coursera.org/specializations/reinforcement-learning) from the [University of Alberta](https://www.ualberta.ca/index.html) on [Coursera](https://www.coursera.org/). Note that some of the content is completely taken from the course and therefore the credit goes to the University of Alberta and Coursera. \n",
    "\n",
    "# <a id='toc1_'></a>[Course 1Ô∏è‚É£ - Fundamentals of Reinforcement Learning](#toc0_)\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <a id='toc1_1_'></a>[Week üïê](#toc0_)\n",
    "\n",
    "### <a id='toc1_1_1_'></a>[The K-Armed Bandit Problem](#toc0_)\n",
    "#### <a id='toc1_1_1_1_'></a>[Sequential Decision Making with Evaluative Feedback](#toc0_)\n",
    "\n",
    "- __Decision making under uncertainty__ can be formalized by the __k-armed bandit problem__\n",
    "- Fundamental ideas: __actions__, __rewards__, __value functions__\n",
    "\n",
    "### <a id='toc1_1_2_'></a>[What to Learn? Estimating Action Values](#toc0_)\n",
    "#### <a id='toc1_1_2_1_'></a>[Learning Action Values](#toc0_)\n",
    "\n",
    "- __Sample-average method__ can be used to estimate action values\n",
    "- The __greedy action__ is the action with the highest value estimate\n",
    "\n",
    "<u>Action-Values</u>\n",
    "\n",
    "$$\n",
    "\\begin{align*}\n",
    "q_*(a) &\\dot{=} \\mathbb{E}[ R_t | A_t = a ] \\qquad \\forall a \\in \\{1, \\dots, k\\} \\\\\n",
    "       &= \\sum_r \\underbrace{p(r | a)}_{\\text{prob. of observing reward} \\; r}r\n",
    "\\end{align*}\n",
    "$$ \n",
    "\n",
    "- The __value__ of an action $a$ is the __expected reward__ when that action is taken\n",
    "- The goal is to __maximize__ the __expected reward__: $argmax_a \\, q_*(a)$\n",
    "- $q_*(a)$ is not known, so we __estimate__ it\n",
    "\n",
    "<u>Sample-Average Method</u>\n",
    "\n",
    "$$\n",
    "\\begin{align*}\n",
    "    Q_t(a) \\dot{=} \\frac{\\text{sum of rewards when} \\; a \\; \\text{taken prior to} \\; t}{\\text{number of times} \\; a \\; \\text{taken prior to} \\; t} = \\frac{\\sum_{i=1}^{t-1}R_i}{t-1} = \\frac{1}{t-1} \\sum_{i=1}^{t-1}R_i\n",
    "\\end{align*}\n",
    "$$\n",
    "\n",
    "- Can be used to estimate action values $Q(a)$\n",
    "\n",
    "#### <a id='toc1_1_2_2_'></a>[Estimating Action Values Incrementally](#toc0_)\n",
    "\n",
    "- Derived incremental sample average method\n",
    "- Generalized the __incremental update rule__ into a more __general update rule__\n",
    "- A __constant stepsize parameter__ can be sued to solve __a non-stationary bandit problem__\n",
    "\n",
    "<u>Incremental Update Rule</u>\n",
    "\n",
    "$$\n",
    "\\begin{align*}           \n",
    "    \\underbrace{Q_{t+1}}_{NewEstimate} \n",
    "            &= \\underbrace{Q_t}_{OldEstimate} +\n",
    "               \\underbrace{\\alpha_t}_{StepSize}(\n",
    "               \\underbrace{R_t}_{Target} -\n",
    "               \\underbrace{Q_t}_{OldEstimate})\n",
    "\\end{align*}\n",
    "$$\n",
    "\n",
    "### <a id='toc1_1_3_'></a>[Exploration vs Exploitation Tradeoff](#toc0_)\n",
    "#### <a id='toc1_1_3_1_'></a>[What is the trade-off?](#toc0_)\n",
    "\n",
    "- We discussed the __tradeoff__ between __exploration and exploitation__\n",
    "- We introduced __epsilon-greedy__ which is a simple method for balancing exploration and exploitation\n",
    "\n",
    "<u>Exploration versus Exploitation</u>\n",
    "\n",
    "- __Exploration__ - _improve_ knowledge for _long-term_ benefit\n",
    "- __Exploitation__ - _exploit_ knowledge for _short-term_ benefit\n",
    "\n",
    "<u>Epsilon-Greedy Action Selection</u> \n",
    "\n",
    "$$\n",
    "\\begin{align*}\n",
    "    A_t \\leftarrow \\left \\{\n",
    "    \\begin{array}{ll}\n",
    "        \\text{argmax}_a Q_t(a) & \\text{with probability} \\; 1 - \\epsilon \\\\\n",
    "        a \\sim Uniform(\\{a_1, \\dots, a_k \\}) & \\text{with probability} \\; \\epsilon \n",
    "    \\end{array}\n",
    "\\right.\n",
    "\\end{align*}\n",
    "$$\n",
    "\n",
    "- is a method to choose when to exploit and when to explore\n",
    "\n",
    "#### <a id='toc1_1_3_2_'></a>[Optimistic Initial Values](#toc0_)\n",
    "\n",
    "- __Optimistic initial values__ encourage _early exploration_ \n",
    "- Described limitations of __optimistic initial values__\n",
    "\n",
    "<u>Limitations of optimistic initial values</u>\n",
    "\n",
    "- Optimistic initial values only _drive early exploration_\n",
    "- They are not well-suited for _non-stationary problems_ \n",
    "- We may not know what the _optimistic initial value_ should be\n",
    "\n",
    "#### <a id='toc1_1_3_3_'></a>[Upper-Confidence Bound (UCB) Action Selection](#toc0_)\n",
    "\n",
    "- __Upper-Confidence Bound action-selection__ uses _uncertainty_ in the value estimates for balancing exploration and exploitation\n",
    "\n",
    "<u>Upper-Confidence Bound (UCB) Action Selection</u>\n",
    "\n",
    "$$\n",
    "\\begin{align*}\n",
    "    A_t \\dot{=} argmax \\Big[ \\underbrace{Q_t(a)}_{\\text{Exploit}} + c \\underbrace{\\sqrt{\\frac{ln \\, t}{N_t(a)}}}_{\\text{Explore}} \\Big]\n",
    "\\end{align*}\n",
    "$$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <a id='toc1_2_'></a>[Week üïë](#toc0_)\n",
    "\n",
    "![problemOfRl]({{ site.baseurl }}/assets/images/problem_of_rl_via_mdp.png)\n",
    "\n",
    "### <a id='toc1_2_1_'></a>[Introduction to Markov Decision Processes](#toc0_)\n",
    "\n",
    "- _MDPs_ provide a general framework for sequential decision making\n",
    "- The __dynamics__ of an MDP are defined by a probability distribution\n",
    "\n",
    "<u>Markov property</u>\n",
    "\n",
    "For a stochastic process, given a sequence of states $S_1, \\dots, S_n$, the probability of transitioning to the next state $S_{n+1}$ depends only on the current state $S_n$ and not on the sequence of states that led to $S_n$. \n",
    "\n",
    "$$\n",
    "\\begin{align*}\n",
    "    \\mathbb{P}(S_{n+1} | \\underbrace{S_n, S_{n-1}, \\dots, S_1}_{\\text{Entire history up to} \\, S_n}) = \\mathbb{P}(S_{n+1} | S_n)\n",
    "\\end{align*}\n",
    "$$\n",
    "\n",
    "where $\\mathbb{P}(S_{n+1} | S_n))$ is the probability of moving to state $S_{n+1}$ given the current state $S_n$. \n",
    "\n",
    "- States that the future state of a process depends only on the current state and not on the sequence of events that preceded it\n",
    "- Posits that the _future is independent of the past_, given the present\n",
    "- Does not mean that the state representation tells all that would be useful to know, only that it has not forgotten anything that would be useful to know\n",
    "\n",
    "### <a id='toc1_2_2_'></a>[The Goal of Reinforcement Learning](#toc0_)\n",
    "\n",
    "- The __goal of an agent__ is to __maximize the expected return__\n",
    "- In __episodic tasks__ the agent environment interaction breaks up into __episodes__\n",
    "\n",
    "<u>Goal of an Agent: Format definition</u>\n",
    "\n",
    "$$\n",
    "\\begin{align*}\n",
    "    \\mathbb{E}[\\underbrace{G_t}_{\\text{Rdm. var.}}] = \\mathbb{E}[ R_{t+1} + R_{t+2} + R_{t+3} + \\dots + \\underbrace{R_T}_{\\text{Reward of final timestep}} ]\n",
    "\\end{align*}\n",
    "$$ \n",
    "\n",
    "### <a id='toc1_2_3_'></a>[Michael Littman: The Reward Hypothesis](#toc0_)\n",
    "\n",
    "<u>Goals as Rewards</u>\n",
    "\n",
    "- $1$ for goal, $0$ otherwise: goal-reward representation\n",
    "- $-1$ for not goal, $0$ once goal reached: action-penalty representation\n",
    "\n",
    "<u>Whence Rewards?</u>\n",
    "\n",
    "- Programming\n",
    "  - Coding\n",
    "  - Human-in-the-loop (source of reward is a human)\n",
    "- Examples\n",
    "  - Mimic reward\n",
    "  - Inverse RL (goes from behavior to rewards. a trainer demostrates an example of the desired behavior and the learner figures out what rewards the trainer must have been maximizing that makes this behavior optimal)\n",
    "- Optimization (rewards can be derived indirectly through an optimization process. If there's some high-level behavior we can create a score for, an optimization approach can search for rewards that encourage that behavior)\n",
    "  - Evolutionary optimization \n",
    "  - Meta RL (multiple agents)\n",
    "\n",
    "<u>Challenges to the Reward Hypothesis</u>\n",
    "\n",
    "- Target is something other than expected cumulative reward:\n",
    "  - How represent risk-sensitive behavior? \n",
    "  - How capture diversity in behavior? \n",
    "- Good match for high-level human behavior? \n",
    "  - Blind reward pursuers aren't good people\n",
    "  - We create our \"purpose\" over years, lifetimes\n",
    "\n",
    "### <a id='toc1_2_4_'></a>[Continuing Tasks](#toc0_)\n",
    "\n",
    "- In __continuing tasks__, the agent-environment interaction goes on indefinitely\n",
    "- __Discounting__ is used to ensure returns are finite\n",
    "- Return can be defined __recursively__\n",
    "\n",
    "| Episodic Tasks | Continuing Tasks | \n",
    "| -------------- | ---------------- | \n",
    "| Interaction breaks naturally into __episodes__ | Interaction goes on __continually__ | \n",
    "| Each episode ends in a __terminal state__ $T$ | No __terminal state__ | \n",
    "| Episodes are __independent__ | |\n",
    "| $G_t \\dot{=} R_{t+1} + R_{t+2} + R_{t+3} + \\cdots + R_T$  | $G_t \\dot{=} R_{t+1} + R_{t+2} + R_{t+3} + \\cdots = \\infty?$ |  \n",
    "\n",
    "<u>Recursive nature of returns</u>\n",
    "\n",
    "$$\n",
    "\\begin{align*}\n",
    "    G_t &\\dot{=} R_{t+1} + \\gamma R_{t+2} + \\gamma^2 R_{t+3} + \\gamma^3 R_{t+4} + \\cdots \\\\\n",
    "        &= R_{t+1} + \\gamma ( \\underbrace{R_{t+2} + \\gamma R_{t+3} + \\gamma^2 R_{t+4} + \\cdots}_{G_{t+1}} ) \\\\\n",
    "    G_t &= R_{t+1} + \\gamma G_{t+1}\n",
    "\\end{align*}\n",
    "$$ \n",
    "\n",
    "<u>Effect of $\\gamma$ on agent behavior</u>\n",
    "\n",
    "$$\n",
    "\\begin{align*}\n",
    "    G_t &\\dot{=} R_{t+1} + \\color{Blue}{\\gamma} R_{t+2} + \\color{Blue}{\\gamma^2} R_{t+3} + \\cdots + \\color{Blue}{\\gamma^{k-1}} R_{t+k} + \\cdots \\\\\n",
    "        &= \\sum_{k=0}^{\\infty} \\gamma^k R_{t+k+1}\n",
    "\\end{align*}\n",
    "$$ \n",
    "\n",
    "- $\\gamma \\in [0, 1)$ is a discounting factor\n",
    "- Used to __discount__ rewards in the future\n",
    "- Makes sure that $G_t$ is finite\n",
    "- Idea is that immediate rewards contribute more to the sum\n",
    "- If $\\gamma = 0$: (_Short-sighted agent_) Agent only cared about the immediate reward. $G_t = R_{t+1}$\n",
    "- If $\\gamma = 1$: (_Far-sighted agent_) Agent takes future rewards into account more strongly. $G_t = R_{t+1} + \\gamma G_{t+1}$\n",
    "\n",
    "Remember that the sum of an infinite geometric series is defined as: \n",
    "\n",
    "$$\n",
    "\\begin{align*}\n",
    "    a + ar + ar^2 + ar^3 + \\cdots \\\\\n",
    "    S = \\sum_{n=1}^{\\infty} ar^{n-1} = \\frac{a}{1-r}\n",
    "\\end{align*}\n",
    "$$ \n",
    "\n",
    "where $a$ is the start term and $r$ is the common ratio. The common ratio is a value for which the values in a series gets consistently multiplied by [[Wikipedia, Infinite Geometric Series](https://www.idealminischool.ca/idealpedia/index.php/Infinite_Geometric_Series#:~:text=The%20general%20formula%20for%20finding,r%20is%20the%20common%20ratio.)]."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "## <a id='toc1_3_'></a>[Week üïí](#toc0_)\n",
    "\n",
    "<u>Summary</u>\n",
    "\n",
    "- __Policies__ tell an agent how to behave in their environment\n",
    "  - _Deterministic_ policies: Map a state to an action\n",
    "    - $s \\xrightarrow{\\pi} a$\n",
    "    - Choose an action with $\\pi (a)$\n",
    "  - _Stochastic_ policies: Map a state to a distribution of actions over all possible actions\n",
    "    - $s \\xrightarrow{\\pi} \\Delta (a)$\n",
    "    - Choose an action with $\\pi(a | s)$\n",
    "  - A policy depends only on the __current state__. Not on e.g. time or previous states. This is a restriction on the state, not the agent. \n",
    "    - Thus, the state should provide the agent with all the information it needs to make a good decision.\n",
    "- __Value functions__ estimate future return (= total reward) under a specific policy.\n",
    "  - Simplify things by aggregating many possible future returns into a single number\n",
    "  - _State-value_ functions: \n",
    "    - $v_{\\color{Blue}{\\pi}}(\\color{Red}{s}) \\dot{=} \\mathbb{E}_{\\color{Blue}{\\pi}}[ \\color{Green}{G_t} | \\color{Red}{S_t = s} ]$\n",
    "    - Expected return from current state $s$, if the agent follows $\\pi$ afterwards.\n",
    "  - _Action-value_ functions: \n",
    "    - $q_{\\color{Blue}{\\pi}}(\\color{Red}{s}, \\color{Red}{a}) \\dot{=} \\mathbb{E}_{\\color{Blue}{\\pi}}[ \\color{Green}{G_t} | \\color{Red}{S_t = s}, \\color{Red}{A_t = a} ]$\n",
    "    - Expected return from state $s$ if the agent first selects $a$ and then follows $\\pi$ afterwards\n",
    "- __Bellmann equations__ define a relationship between the value of a state, or state-action pair, and its possible successor states\n",
    "  - Bellmann equation for the _state-value_ function: \n",
    "    - $v_{\\pi}(s) = \\sum_{a}\\pi(s | a) \\sum_{s'}\\sum_{r} p(s',r | s,a) [r + \\gamma v_{\\pi}(s')]$ \n",
    "    - gives the value of the current state as a sum over the values of all the successor states, and immediate rewards\n",
    "  - Bellmann equation for the _state-action_ function:\n",
    "    - $q_{\\pi}(s, a) = \\sum_{s'}\\sum_{r} p(s',r | s,a) [r + \\gamma \\sum_{a'} \\pi(a' | s') q_{\\pi}(s', a')]$\n",
    "    - gives the value of a particular state-action pair as the sum over the values of all possible next state-action pairs and rewards\n",
    "  - Can be directly used to find the value function\n",
    "  - Help us evaluate policies\n",
    "  - But, they can't find a policy that attains as much reward as possible\n",
    "- __The ultimate goal__: to find a policy that obtains as much reward as possible\n",
    "  - __Optimal policy__: achieves the highest value possible in every state\n",
    "    - There is always $\\geq 1$ optimal policies\n",
    "    - _Optimal state-value function_ $v_*$: highest possible value in every state\n",
    "      - $v_{\\pi_*}(s) = \\max_{\\pi} v_{\\pi}(s)$\n",
    "      - every optimal policy shares the same optimal state-value function\n",
    "    - _Optimal action-value function_ $q_*$:\n",
    "      - $q_{\\pi_*}(s,a) = max_{\\pi} q_{\\pi}(s,a) \\; \\text{for all} \\; s \\in \\mathcal{S} \\; \\text{and} \\; a \\in \\mathcal{A}$ \n",
    "    - __Optimal Bellmann equations__\n",
    "      - $v_{\\pi_*}(s) = max_a \\sum_{s'}\\sum_{r} p(s',r | s,a) [r + \\gamma v_*(s')]$ \n",
    "      - $q_{\\pi_*}(s, a) = \\sum_{s'}\\sum_{r} p(s',r | s,a) [r + \\gamma \\, max_a q_*(s', a')]$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <a id='toc1_4_'></a>[Week üïì](#toc0_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
