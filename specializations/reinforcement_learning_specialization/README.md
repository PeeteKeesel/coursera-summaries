# Reinforcement Learning Specialization

This folder contains notebooks as well as personal summaries and notes for the [Reinforcement Learning Specializations](https://www.coursera.org/specializations/reinforcement-learning) from the [University of Alberta](https://www.ualberta.ca/index.html) on [Coursera](https://www.coursera.org/). The notebooks are contents of the course. The solutions are my own work. All the rest has been created by Coursera and the University of Alberta and thus, the credit goes to

```
Coursera
University of Alberta
```

## üéØ Learning Objectives 

- Course :one: 

<details>
  <summary>show learning objectives</summary>

```
Week üïê 
  - Understand the prerequisites, goals and roadmap for the course.
Week üïë
  - Understand Markov Decision Processes (MDP)
  - Describe how the dynamics of an MDP are defined
  - Understand the graphical representation of a Markov Decision Process
  - Explain how many diverse processes can be written in terms of the MDP framework
  - Describe how rewards relate to the goal of an agent
  - Understand episodes and identify episodic tasks
  - Formulate returns for continuing tasks using discounting
  - Describe how returns at successive time steps are related to each other
  - Understand when to formalize a task as episodic or continuing
Week üïí
	- Recognize that a policy is a distribution over actions for each possible state
	- Describe the similarities and differences between stochastic and deterministic policies
	- Generate examples of valid policies for a given MDP
	- Describe the roles of state-value and action-value functions in reinforcement learning
	- Describe the relationship between value functions and policies
	- Create examples of valid value functions for a given MDP
	- Derive the Bellman equation for state-value functions
	- Derive the Bellman equation for action-value functions
	- Understand how Bellman equations relate current and future values
	- Use the Bellman equations to compute value functions
	- Define an optimal policy
	- Understand how a policy can be at least as good as every other policy in every state
	- Identify an optimal policy for given MDPs
	- Derive the Bellman optimality equation for state-value functions
	- Derive the Bellman optimality equation for action-value functions
	- Understand how the Bellman optimality equations relate to the previously introduced Bellman equations
	- Understand the connection between the optimal value function and optimal policies
	- Verify the optimal value function for given MDPs
Week üïì
	- Understand the distinction between policy evaluation and control
	- Explain the setting in which dynamic programming can be applied, as well as its limitations
	- Outline the iterative policy evaluation algorithm for estimating state values under a given policy
	- Apply iterative policy evaluation to compute value functions
	- Understand the policy improvement theorem
	- Use a value function for a policy to produce a better policy for a given MDP
	- Outline the policy iteration algorithm for finding the optimal policy
	- Understand ‚Äúthe dance of policy and value‚Äù
	- Apply policy iteration to compute optimal policies and optimal value functions
	- Understand the framework of generalized policy iteration
	- Outline value iteration, an important example of generalized policy iteration
	- Understand the distinction between synchronous and asynchronous dynamic programming methods
	- Describe brute force search as an alternative method for searching for an optimal policy
	- Describe Monte Carlo as an alternative method for learning a value function
	- Understand the advantage of Dynamic programming and ‚Äúbootstrapping‚Äù over these alternative strategies for finding the optimal policy
```

</details>

- Course :two: 
- Course :three: 

## üèõÔ∏è Structure

The specialisation is structured into three courses. Each course covers a multitude of topics split into three to four weeks. 

<table>
  <tr>
    <th>üìÖWeek</th>
    <td>üí°Topic</td>
    <th>üî¨Lab</th>
    <th>üìùQuizzes</th>
  </tr>
  <!-- ------------------------------------------------------------ -->
  <!-- COURSE 1 -->                
  <!-- ------------------------------------------------------------ -->
  <tr>
    <td colspan="4" align="center">
      Course 1Ô∏è‚É£:<br><a href="https://github.com/PeeteKeesel/coursera-summaries/blob/main/specializations/reinforcement_learning_specialization/course1_fundamentals_of_rl">
        <code>Fundamentals of Reinforcement Learning</code>
      </a>     
    </td>
  </tr>
  <tr>
    <td rowspan="1" align="center">1Ô∏è‚É£</td>
    <td>An Introduction to Sequential Decision-Making</td>
    <td><a href="TODO">ipynb</a></td>
    <td rowspan="1">
        <a href="TODO">quizzes</a>
    </td>
  </tr>
  <tr>
    <td rowspan="1" align="center">2Ô∏è‚É£</td>
    <td>Markov Decision Processes</td>
    <td><a href="TODO">ipynb</a></td>
    <td rowspan="1">
        <a href="TODO">quizzes</a>
    </td>
  </tr> 
  <tr>
    <td rowspan="1" align="center">3Ô∏è‚É£</td>
    <td>Value Functions & Bellman Equations</td>
    <td><a href="TODO">ipynb</a></td>
    <td rowspan="1">
        <a href="TODO">quizzes</a>
    </td>
  </tr> 
  <tr>
    <td rowspan="1" align="center">4Ô∏è‚É£</td>
    <td>Dynamic Programming</td>
    <td><a href="TODO">ipynb</a></td>
    <td rowspan="1">
        <a href="TODO">quizzes</a>
    </td>
  </tr>      
</table>

## üìö Resources