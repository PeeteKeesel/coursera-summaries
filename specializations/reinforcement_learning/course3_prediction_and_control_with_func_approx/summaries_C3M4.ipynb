{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Table of contents**<a id='toc0_'></a>    \n",
    "- [Episodic Sarsa with Function Approximation](#toc1_)    \n",
    "- [Control with Function Approximation](#toc2_)    \n",
    "- [Expected Sarsa with Function Approximation](#toc3_)    \n",
    "- [Exploration under Function Approximation](#toc4_)    \n",
    "- [Average Reward](#toc5_)    \n",
    "  - [Average Reward: A New Way of Formulating Control Problems](#toc5_1_)    \n",
    "- [tldr](#toc6_)    \n",
    "\n",
    "<!-- vscode-jupyter-toc-config\n",
    "\tnumbering=false\n",
    "\tanchor=true\n",
    "\tflat=false\n",
    "\tminLevel=1\n",
    "\tmaxLevel=6\n",
    "\t/vscode-jupyter-toc-config -->\n",
    "<!-- THIS CELL WILL BE REPLACED ON TOC UPDATE. DO NOT WRITE YOUR TEXT IN THIS CELL -->"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <a id='toc1_'></a>[Episodic Sarsa with Function Approximation](#toc0_)\n",
    "\n",
    "$\\mathbf{OBJECTIVES}$\n",
    "\n",
    "**Lesson 1: Episodic Sarsa with Function Approximation**\n",
    "\n",
    "- Explain the update for Episodic Sarsa with function approximation \n",
    "- Introduce the feature choices, including passing actions to features or stacking state features \n",
    "- Visualize value function and learning curves \n",
    "- Discuss how this extends to Q-learning easily, since it is a subset of Expected Sarsa \n",
    "\n",
    "**Lesson 2: Exploration under Function Approximation**\n",
    "\n",
    "- Understanding optimistically initializing your value function as a form of exploration\n",
    "\n",
    "**Lesson 3: Average Reward**\n",
    "\n",
    "- Describe the average reward setting \n",
    "- Explain when average reward optimal policies are different from discounted solutions \n",
    "- Understand how differential value functions are different from discounted value functions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "# <a id='toc2_'></a>[Control with Function Approximation](#toc0_)\n",
    "\n",
    "<p align=\"center\">\n",
    "  <img width=\"600\" height=\"300\" src=\"imgs/c3m4-computing-action-values.png\">\n",
    "</p>\n",
    "\n",
    "- to move from TD to SARSA, we need action value functions\n",
    "- **Action-dependent features**: Extend state features by stacking them per action ‚Äî only the selected action's features are active.\n",
    "- **Linear approximation**: Each action has its own segment of the weight vector; Q-values are computed via dot product with stacked features.\n",
    "- **Neural networks**: Equivalent behavior via multiple outputs, one per action, from shared state features.\n",
    "  - we input the states **and** and the actions\n",
    "- **Alternative (generalization)**: Input both state and action into a single-output network to generalize over actions.\n",
    "- **Sarsa with approximation**: Use parameterized Q-functions and gradient-based updates (like semi-gradient TD) to learn.\n",
    "\n",
    "<p align=\"center\">\n",
    "  <img width=\"600\" height=\"400\" src=\"imgs/c3m4-episodic-sarsa.png\">\n",
    "</p>\n",
    "\n",
    "# <a id='toc3_'></a>[Expected Sarsa with Function Approximation](#toc0_)\n",
    "\n",
    "$\\textbf{Sarsa}$\n",
    "$$\n",
    "    Q(S_t, A_t) \\leftarrow Q(S_t, A_t) + \\alpha( R_{t+1} + \\gamma Q({\\color{red}{S_{t+1}, A_{t+1}}}) - Q(S_t, A_t) )\n",
    "$$\n",
    "\n",
    "$\\textbf{Expected Sarsa}$\n",
    "$$\n",
    "    Q(S_t, A_t) \\leftarrow Q(S_t, A_t) + \\alpha( R_{t+1} + \\gamma {\\color{red}{\\sum_{a'}\\pi(a' | S_{t+1})Q(S_{t+1, a'})}} - Q(S_t, A_t))\n",
    "$$\n",
    "- To compute the expectation, we simply sum over the action values weighted by their probability under the target policy. \n",
    "\n",
    "$\\textbf{Sarsa with Function Approximation}$\n",
    "$$\n",
    "    \\mathbf{w} \\leftarrow \\mathbf{w} + \\alpha( R_{t+1} + \\gamma \\hat{q}({\\color{red}{S_{t+1}, A_{t+1}}}, \\mathbf{w}) - \\hat{q}(S_t, A_t, \\mathbf{w})) \\nabla\\hat{q}(S_t, A_t, \\mathbf{w})\n",
    "$$\n",
    "\n",
    "$\\textbf{Expected Sarsa with Function Approximation}$\n",
    "$$\n",
    "    \\mathbf{w} \\leftarrow \\mathbf{w} + \\alpha( R_{t+1} + \\gamma{\\color{red}{\\sum_{a'}\\pi(a' | S_{t+1})\\hat{q}(S_{t+1}, a', \\mathbf{w})}} - \\hat{q}(S_t, A_t, \\mathbf{w})) \\nabla\\hat{q}(S_t, A_t, \\mathbf{w})\n",
    "$$\n",
    "where $q_{\\pi}(s, a) \\approx \\hat{q}(s,a,\\mathbf{w})$.\n",
    "\n",
    "$\\textbf{Expected Sarsa to Q-Learning}$\n",
    "$$\n",
    "    \\mathbf{w} \\leftarrow \\mathbf{w} + \\alpha( R_{t+1} + \\gamma{\\color{red}{\\max_{a'}\\hat{q}(S_{t+1}, a', \\mathbf{w})}} - \\hat{q}(S_t, A_t, \\mathbf{w})) \\nabla\\hat{q}(S_t, A_t, \\mathbf{w})\n",
    "$$\n",
    "where $q_{\\pi}(s, a) \\approx \\hat{q}(s,a,\\mathbf{w}) = \\mathbf{w}^T\\mathbf{x}(s,a)$.\n",
    "\n",
    "# <a id='toc4_'></a>[Exploration under Function Approximation](#toc0_)\n",
    "\n",
    "- **Optimistic initial values** work well in tabular settings ‚Äî they encourage the agent to try actions by pretending they‚Äôre better than they are.\n",
    "- With **function approximation**, it's harder: updates affect many states at once, so optimism can fade quickly, even before visiting all states.\n",
    "- Some methods like **tile coding** allow more localized updates, which helps keep optimism alive longer.\n",
    "- **Neural networks** often generalize too much, making optimism harder to maintain.\n",
    "- **Epsilon-greedy** is simple and works with any function approximator, but it‚Äôs **random**, not systematic.\n",
    "- Good exploration with function approximation is still an open challenge ‚Äî for now, we use simple tools like epsilon-greedy.\n",
    "\n",
    "$\\mathbf{\\epsilon}\\textbf{-greedy}$\n",
    "$$\n",
    "\\begin{align}\n",
    "    1-\\epsilon &\\qquad\\qquad A_t = \\argmax_a\\hat{q}(S_t, a, \\mathbf{w}) \\\\ \n",
    "    \\epsilon &\\qquad\\qquad A_t = \\text{Random action}\n",
    "\\end{align}\n",
    "$$\n",
    "\n",
    "# <a id='toc5_'></a>[Average Reward](#toc0_)\n",
    "## <a id='toc5_1_'></a>[Average Reward: A New Way of Formulating Control Problems](#toc0_)\n",
    "\n",
    "- Avg Reward Formulation = new way of formulating continuing problems \n",
    "\n",
    "- **Average reward $r(\\pi)$**: Measures long-term performance as reward per time step (no discounting needed).\n",
    "\n",
    "$\\textbf{The Average Reward Objective:}$\n",
    "$$\n",
    "\\begin{align}\n",
    "    r(\\pi) = \n",
    "        \\underbrace{\\sum_s \\mu_\\pi(s)}_{\\substack{\\text{State} \\\\ \\text{visitation}}}\n",
    "        \\underbrace{\\sum_a \\pi(a|s)}_{\\substack{\\text{Expected reward in the state} \\\\ \\text{under policy } \\pi}}\n",
    "        \\underbrace{\\sum_{s',r} p(s', r | s,a) \\, r}_{\\substack{\\text{Immediate reward given } s,a \\\\ \\text{(weighted by how frequently } \\pi \\text{ visits } s)}}\n",
    "\\end{align}    \n",
    "$$\n",
    "$\\textbf{Returns for Average Reward:}$\n",
    "$$\n",
    "\\begin{align}\n",
    "    G_t = R_{t+1} - r(\\pi) + R_{t+2} - r(\\pi) + R_{t+3} - r(\\pi) + \\dots\n",
    "\\end{align}    \n",
    "$$\n",
    "The subparts $R_{t+i} - r(\\pi), i={1,2,\\dots}$ are called $\\textbf{Differential Returns}$.\n",
    "- $\\textbf{Differential Returns}$ \n",
    "  - $=$ the sum of rewards into the future with the average reward subtracted from each one.\n",
    "  - Represents how much better it is to take an action in a state then on average under a certain policy. \n",
    "  - Can only be used to compare actions if the same policy is followed on subsequent time steps. \n",
    "  - To compare policies, their average reward should be used instead.\n",
    "\n",
    "- **Why it's useful**: Unlike discounting, it treats near and far rewards equally ‚Äî ideal for continuing tasks.\n",
    "- **Differential return**: Measures how much more (or less) reward you get compared to the average ‚Äî used to evaluate actions within a fixed policy.\n",
    "- **Stacked loops example**: Shows that discount-based policies can prefer different paths than average reward-based ones, depending on Œ≥.\n",
    "- **Differential Sarsa**: Like regular Sarsa, but subtracts the average reward estimate (ùëÖÃÑ) during updates.\n",
    "\n",
    "$\\textbf{Value Functions for Average Reward:}$\n",
    "$$\n",
    "\\begin{align}\n",
    "    G_t        &= R_{t+1} - r(\\pi) + R_{t+2} - r(\\pi) + R_{t+3} - r(\\pi) + \\dots \\\\\n",
    "    q_\\pi(s,a) &= \\mathbb{E}_\\pi[G_t | S_t=s, A_t=a] \\\\\n",
    "    q_\\pi(s,a) &= \\sum_{s',r}p(s',r | s,a)(r - r(\\pi) + \\sum_{a'}\\pi(a'|s')q_\\pi(s',a'))\n",
    "\\end{align}    \n",
    "$$\n",
    "\n",
    "The second quantity captures how much more reward the agent will get by starting in a particular state than it would get on average over all states if it followed a fixed policy.\n",
    "\n",
    "<p align=\"center\">\n",
    "  <img width=\"700\" height=\"400\" src=\"imgs/c3m4-differential-sarsa.png\">\n",
    "</p>\n",
    "\n",
    "> üìå Use **average reward** to compare policies and **differential return** to compare actions under a policy."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Intrinsic Rewards\n",
    "\n",
    "<p align=\"center\">\n",
    "  <img width=\"700\" height=\"400\" src=\"imgs/c3m4-revised-autonomous-agent.png\">\n",
    "</p>\n",
    "\n",
    "(1) A diagram of the standard reinforcement learning agent where the reward function is assumed to be part of the environment. The credit is part of the environment and then the agent's task is to optimize some cumulative measure overlap reward function.\n",
    "\n",
    "(2) Another diagram in which we blow up the agent into let's call it the organism, in which inside the agent is a critic which then rewards the agent and then the agent optimizes as an external environment, which played the role of the previous environment which the extrinsic reward would lie. Here,\n",
    "- Agent reward is ***internal/intrinsic*** to the agent.\n",
    "- Parameters to be designed by agent-designer.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <a id='toc6_'></a>[tldr](#toc0_)\n",
    "\n",
    "> Topic: *how to do control when using function approximation*\n",
    "\n",
    "**How to estimate action values with function approximation?**\n",
    "\n",
    "- If the action space is discrete, it's probably easiest to stack the state features. \n",
    "- If the action space is continuous or you want to generalize over actions, the action can be passed as an input like any other state variable.\n",
    "\n",
    "**Control Algorithms (Function Approximation)**\n",
    "\n",
    "- Covers: SARSA, Expected SARSA, Q-Learning\n",
    "- Based on tabular versions from Course 2\n",
    "- Key difference: use gradients to update weights: $\\mathbf{w} \\leftarrow \\mathbf{w} + \\alpha\\delta\\nabla\\hat{q}(s,a,\\mathbf{w})$\n",
    "\n",
    "**Exploration Strategies**\n",
    "\n",
    "- **Optimistic Initialization**: works with structured features (e.g. tile coding); unreliable with neural nets.\n",
    "- $\\epsilon$-greedy**: works regardless of the function approximator.\n",
    "\n",
    "**Average Reward Framework** (=new way to think about the continuing control problem)\n",
    "\n",
    "- Alternative to discounting:\n",
    "  - Maximize average reward per time step\n",
    "- Introduced:\n",
    "  - Differential Return\n",
    "  - Differential Value\n",
    "  - Differential Semi-gradient SARSA\n",
    "\n",
    "**Remember**\n",
    "\n",
    "- SARSA, expected SARSA, and Q-Learning are all extensions of the tabular control algorithms\n",
    "- Differential SARSA is also in the left half of the algorithm map, but unlike the algorithms we covered earlier in the week, it uses the average reward framework. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "This week, we talked about how to do control when using function approximation, let's go over the main ideas. First, we showed you how to estimate action values with function approximation. If the action space is discrete, it's probably easiest to stack the state features. If the action space is continuous or you want to generalize over actions, the action can be passed as an input like any other state variable. Let's get some context about the next part of the module by looking at the algorithm map. Function approximation puts us on the left side of the map, the focus of the first lecture was on the control algorithms in the bottom left corner, SARSA, expected SARSA, and Q-Learning. These are all extensions of the tabular control algorithms we covered in Course 2, the only difference between these algorithms and their tabular counterparts are the update equations. The updates are all adapted for function approximation in the same way, using the gradient to update the weights. We also saw how episodic SARSA could be used to solve the mountain car problem. In this case, the larger step size is 0.5 was able to learn more quickly. Next, we talked about exploration, optimistic initialization can be used with some structured feature representations like tele-coding. But in general, it's not clear how to optimistically initialize values with nonlinear function approximators like neural networks, and it might not behave as expected, for example the optimism may fade too quickly. Epsilon-greedy can be used regardless of the function approximator. Finally, we talked about a new way to think about the continuing control problem. Instead of maximizing the discounted return from the current state, we can think about maximizing the average reward that a policy receives overtime. We defined differential returns and differential values, these enable the agent to assess the relative value of actions in the average reward setting. Finally, we introduced differential semi-gradient SARSA that approximates differential values to learn policies. Differential SARSA is also in the left half of the algorithm map, but unlike the algorithms we covered earlier in the week, it uses the average reward framework. w\n",
    "\n",
    "We extended our tabular control algorithms to function approximation, discussed how exploration changes, and introduced a new way to think about the control problem. Next week, we'll talk all about how to do reinforcement learning without learning the value function."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "coursera-summaries",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
