{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Table of contents**<a id='toc0_'></a>    \n",
    "- [Agent Architecture Meeting with Martha: Overview of Design Choices](#toc1_)    \n",
    "- [Review: Non-linear Approximation with Neural Networks](#toc2_)    \n",
    "\n",
    "<!-- vscode-jupyter-toc-config\n",
    "\tnumbering=false\n",
    "\tanchor=true\n",
    "\tflat=false\n",
    "\tminLevel=1\n",
    "\tmaxLevel=6\n",
    "\t/vscode-jupyter-toc-config -->\n",
    "<!-- THIS CELL WILL BE REPLACED ON TOC UPDATE. DO NOT WRITE YOUR TEXT IN THIS CELL -->"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <a id='toc1_'></a>[Agent Architecture Meeting with Martha: Overview of Design Choices](#toc0_)\n",
    "\n",
    "$\\textbf{Which Function Approximator to Use?}$\n",
    "\n",
    "- Tile coding \n",
    "  - The # of features grows exponentially with the input dim\n",
    "- Neural network\n",
    "  - The more nodes \n",
    "    - $\\rightarrow$ the more <span style=\"color:green\">representational power</span>\n",
    "    - $\\rightarrow$ the more <span style=\"color:red\">parameters</span>\n",
    "  - Pick activation functions\n",
    "    - $tanh$\n",
    "      - <span style=\"color:orange\">Have issues of saturation</span>\n",
    "    - $ltu$: Linear threshold unit\n",
    "      - <span style=\"color:orange\">Flat regions make it hard to train the NN</span>\n",
    "    - $ReLu$: decent choice\n",
    "  - How to train the NN?\n",
    "    - `adagrad`\n",
    "      - <span style=\"color:red\">decays step size rowards 0</span>\n",
    "    - `RMSProp`\n",
    "      - <span style=\"color:green\">uses information about the curvature of the loss to improve the descent step</span>\n",
    "      - <span style=\"color:red\">doesn't incorporate momentum</span>\n",
    "    - `ADAM`-optimizer\n",
    "      - <span style=\"color:green\">curvature information</span>\n",
    "      - <span style=\"color:green\">momentum</span>\n",
    "- Which exploration method to use? \n",
    "  - **Optimistic initial values**\n",
    "    - Reasonable choice if we ewere using a linear function approximator with $\\geq 0$ values\n",
    "    - But with NNs it's difficult to maintain optimistic values\n",
    "  - **$\\epsilon$-greedy**\n",
    "    - <span style=\"color:red\">It's exploration completely ignores whatever information the action values might have.</span> It is equally likely to explore an action with really negative value as an action with moderate value. \n",
    "  - **Softmax-Policy**\n",
    "    - Probability of selecting an action is proportional to the value of that action. This way we are <span style=\"color:green\">less likely to explore actions that we think are really bad</span>.\n",
    "- $\\tau$: Temperature parameter\n",
    "  - Controls how much the agent focuses on the highest value actions\n",
    "  - If $\\tau$ large: agent is more stochastic & selects more of the actions\n",
    "  - If $\\tau$ very large: agent behaves nearly like a uniform random policy\n",
    "  - If $\\tau$ very small: agent mostly selects the greedy action\n",
    "- Strategy for Lunar Lander: \n",
    "  - The agent takes actions according to its current Softmax policy and uses expected SARSA updates"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <a id='toc2_'></a>[Review: Non-linear Approximation with Neural Networks](#toc0_)\n",
    "\n",
    "- **Tile coding** is one way to create a fixed set of features for a prediction. \n",
    "- **NNs** provide a strategy for learning a useful set of features."
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
