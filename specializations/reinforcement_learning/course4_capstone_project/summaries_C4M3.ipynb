{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Table of contents**<a id='toc0_'></a>    \n",
    "- [Lunar Lander MDP](#toc1_)    \n",
    "- [Review: Expected SARSA](#toc2_)    \n",
    "- [Review: Q-Learning](#toc3_)    \n",
    "- [Review: Average Reward- A New Way of Formulating Control Problems](#toc4_)    \n",
    "  - [Summary](#toc4_1_)    \n",
    "- [Review: Actor-Critic Algorithm](#toc5_)    \n",
    "- [tldr](#toc6_)    \n",
    "\n",
    "<!-- vscode-jupyter-toc-config\n",
    "\tnumbering=false\n",
    "\tanchor=true\n",
    "\tflat=false\n",
    "\tminLevel=1\n",
    "\tmaxLevel=6\n",
    "\t/vscode-jupyter-toc-config -->\n",
    "<!-- THIS CELL WILL BE REPLACED ON TOC UPDATE. DO NOT WRITE YOUR TEXT IN THIS CELL -->"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <a id='toc1_'></a>[Lunar Lander MDP](#toc0_)\n",
    "\n",
    "$\\textbf{How to solve the Lunar Lander MDP? Which algorithm to use?}$\n",
    "\n",
    "1. Can we represent the value function using only a table? \n",
    "   - The agent observes the position, orientation, velocity and contact sensors of the lunar module. \n",
    "   - Six of the eight state variables are continuous, which means that we cannot represent them with a table. \n",
    "   - And in any case we'd like to take advantage of generalization to learn faster.\n",
    "2. would this be well formulated as an average word problem?\n",
    "   - Think about the dynamics of this problem. \n",
    "   - The lunar module starts in low orbit and descends until it comes to rest on the surface of the moon. \n",
    "   - This process then repeats with each new attempt at landing beginning independently of how the previous one ended. \n",
    "   - This is exactly our definition of an episodic task.\n",
    "   - We use the average reward formulation for continuing tasks, so that is not the best choice here.\n",
    "3. possible and beneficial to update the policy and value function on every time step? MC or TD?\n",
    "   - Think about landing your module on the moon. \n",
    "   - If any of our sensors becomes damaged during the episode, we want to be able to update the policy before the end of the episode. \n",
    "   - It's like what we discussed in the driving home example, we expect the TD method to do better in this kind of problem.\n",
    "4. Objective? \n",
    "   - We want to learn a safe and a robust policy in our simulator so that we can use it on the moon. We want to learn a policy that maximizes reward, and so this is a control task.\n",
    "\n",
    "This leaves us with three algorithms, SARSA, expected SARSA and Q-learning. \n",
    "\n",
    "- Since we are using function approximation, learning and epsilon soft policy will be more robust than learning a deterministic policy. \n",
    "- Remember the example where due to state aliasing, a deterministic policy was suboptimal. \n",
    "- Expected SARSA and SARSA, both allow us to learn an optimal epsilon soft policy, but Q-learning does not. Now we need to choose between expected SARSA and SARSA. \n",
    "- We mentioned in an earlier video that expected SARSA usually performs better than SARSA. So, let's eliminate SARSA.\n",
    "\n",
    "> ${\\textcolor{red}{\\textbf{Expected SARSA}}}$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <a id='toc2_'></a>[Review: Expected SARSA](#toc0_)\n",
    "\n",
    "<p align=\"center\">\n",
    "  <img width=\"700\" height=\"400\" src=\"imgs/c4m3-sarsa.png\">\n",
    "</p>\n",
    "\n",
    "Recall the Bellman equation for action-values. Here you can see the expectation over values of possible next state action pairs. Breaking this expectation apart, we see a sum over possible next states as well as possible next action. Sarsa estimates this expectation by sampling the next date from the environment and the next action from its policy. But the agent already knows this policy, so why should it have to sample its next action? Instead, it should just compute the expectation directly. In this case, we can take a weighted sum of the values of all possible next actions. The weights are the probability of taking each action under the agents policy.\n",
    "\n",
    "> Explicitly computing the expectation over next actions is the **main idea** behind the ${\\textcolor{red}{\\textbf{expected SARSA algorithm}}}$.\n",
    "\n",
    "The algorithm is nearly identical to Sarsa, except the TD-error uses the *expected estimate* of the next action value instead of a sample of the next action value.\n",
    "\n",
    "- That means that on every time step, the agent has to average the next state's action values according to how likely they are under the policy.\n",
    "\n",
    "<p align=\"center\">\n",
    "  <img width=\"700\" height=\"400\" src=\"imgs/c4m3-expected-sarsa.png\">\n",
    "</p>\n",
    "\n",
    "- In general, expected Sarsas update targets are much lower variance than Sarsas.\n",
    "\n",
    "The lower variance comes with a downside though. Computing the average over next actions becomes more expensive as the number of actions increases. When there are many actions, computing the average might take a long time, especially since the average has to be computed every time step."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <a id='toc3_'></a>[Review: Q-Learning](#toc0_)\n",
    "\n",
    "<p align=\"center\">\n",
    "  <img width=\"700\" height=\"400\" src=\"imgs/c4m3-q-learning-algo.png\">\n",
    "</p>\n",
    "\n",
    "This differs from Sarsa, which uses the value of the next state action pair in its target.\n",
    "\n",
    "<p align=\"center\">\n",
    "  <img width=\"700\" height=\"400\" src=\"imgs/c4m3-revisiting-bellman-equations.png\">\n",
    "</p>\n",
    "\n",
    "If you look at the update equation for Sarsa, it's suspiciously similar to the Bellman equation for action values. In fact, Sarsa is a sample-based algorithm to solve the Bellman equation for action values. Q-learning also solves the Bellman equation using samples from the environment. But instead of using the standard Bellman equation, Q-learning uses the Bellman's Optimality Equation for action values. The optimality equations enable Q-learning to directly learn Q-star instead of switching between policy improvement and policy evaluation steps. Even though Sarsa and Q-learning are both based on Bellman equations, they're based on very different Bellman equations. Sarsa is sample-based version of policy iteration which uses Bellman equations for action values, that each depend on a fixed policy. Q-learning is a sample-based version of value iteration which iteratively applies the Bellman optimality equation. Applying the Bellman's Optimality Equation strictly improves the value function, unless it is already optimal. So value iteration continually improves as value function estimate, which eventually converges to the optimal solution. For the same reason, Q-learning also converges to the optimal value function as long as the aging continues to explore and samples all areas of the state action space. \n",
    "\n",
    "$$\n",
    "\\begin{align*}\n",
    "    \\text{SARSA}      \\sim \\text{Policy Iteration} \\\\ \n",
    "    \\text{Q-Learning} \\sim \\text{Value Iteration}\n",
    "\\end{align*}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <a id='toc4_'></a>[Review: Average Reward- A New Way of Formulating Control Problems](#toc0_)\n",
    "\n",
    "- ${\\textcolor{red}{\\textbf{Average Reward Formulation}}}=$ new way of formulating continuing problems\n",
    "- If the agents goal is to maximize this average reward, then it cares equally about nearby and distant rewards.\n",
    "\n",
    "<p align=\"center\">\n",
    "  <img width=\"600\" height=\"400\" src=\"imgs/c4m3-avg-reward-objective.png\">\n",
    "</p>\n",
    "\n",
    "$$\n",
    "\\begin{align}\n",
    "    r(\\pi) = \\underbrace{\n",
    "                \\sum_{s}\\mu(s) \n",
    "                \\underbrace{\n",
    "                    \\sum_{a}\\pi(a | s,\\theta) \n",
    "                        \\underbrace{\n",
    "                            \\sum_{s',r}p(s',r' | s,a)r\n",
    "                        }_{\\mathbb{E}_{\\pi}[R_t | S_t=s, A_t=a]}\n",
    "                }_{\\mathbb{E}_{\\pi}[R_t | S_t=s]}\n",
    "             }_{\\mathbb{E}_{\\pi}[R_t]}\n",
    "\\end{align}\n",
    "$$\n",
    "\n",
    "- This inner term is the expected reward in a state under policy pi. \n",
    "- The outer sum takes the expectation over how frequently the policy is in that state. \n",
    "- Together, we get the expected reward across states. In other words, the average reward for a policy.\n",
    "\n",
    "> We can see the average reward puts preference on the policy that receives more reward in total without having to consider larger and larger discounts.\n",
    "\n",
    "$$\n",
    "\\begin{align}\n",
    "    \\text{Episodic:}   \\qquad &G_t = \\sum_{t=0}^{T} R_t\\\\\n",
    "    \\text{Continuing:} \\qquad &G_t = \\sum_{t=0}^{\\infty} \\gamma^t R_t \\qquad \\underbrace{G_t = \\sum_{t=0}^{\\infty}\\underbrace{\\overbrace{R_t}^{\\text{Immediate}} - \\overbrace{r(\\pi)}^{\\text{Average}}}_{{\\textcolor{red}{\\textbf{Differential Return}}}}}_{\\text{Average Reward Formulation}}\n",
    "\\end{align}\n",
    "$$\n",
    "\n",
    "In the average reward setting, returns are defined in terms of differences between rewards and the average reward R pi. This is called the ${\\textcolor{red}{\\textbf{differential return}}}$.\n",
    "\n",
    "<p align=\"center\">\n",
    "  <img width=\"700\" height=\"400\" src=\"imgs/c4m3-differential-sarsa.png\">\n",
    "</p>\n",
    "\n",
    "## <a id='toc4_1_'></a>[Summary](#toc0_)\n",
    "\n",
    "Discounting vs. Average Reward\n",
    "- Discounting emphasizes near-term rewards.\n",
    "- Average reward treats all time steps equally.\n",
    "- In the “nearsighted MDP”:\n",
    "  - Small $\\gamma$: prefer immediate rewards\n",
    "  - Large $\\gamma$: prefer delayed, higher rewards\n",
    "- Average reward eliminates the need to tune $\\gamma$\n",
    "\n",
    "Differential Return\n",
    "- Measures how much better a trajectory is than the average\n",
    "- Converges **only if** the correct $\\bar{r}$ is subtracted\n",
    "\n",
    "Differential SARSA\n",
    "- Similar to standard SARSA\n",
    "- Tracks average reward $\\bar{r}$\n",
    "- Uses $R_{t+1} - \\bar{r}$ in its TD update\n",
    "- Improved performance with variance-reducing average reward update\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <a id='toc5_'></a>[Review: Actor-Critic Algorithm](#toc0_)\n",
    "\n",
    "In this setup, the parameterized policy plays the role of an actor, while the value function plays the role of a critic, evaluating the actions selected by the actor. These so-called actor-critic methods, were some of the earliest TD-based methods introduced in RL.\n",
    "\n",
    "<p align=\"center\">\n",
    "  <img width=\"600\" height=\"400\" src=\"imgs/c4m3-actor-critic.png\">\n",
    "</p>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Thinking About RL Problems\n",
    "\n",
    "$\\textbf{State the goal!}$\n",
    "\n",
    "1. Is the goal to solve a (type of a) problem?\n",
    "2. Is it to develop a tool? For what? \n",
    "\n",
    "$\\textbf{Specify the problem!}$\n",
    "\n",
    "1. How is data collected? \n",
    "2. How will algorithms be evaluated? \n",
    "3. Are there specific properties of the environments? \n",
    "\n",
    "$\\textbf{Data sources}$\n",
    "\n",
    "1. Interact with the real world?\n",
    "2. Interact with simulator? \n",
    "3. No interaction: Learn from a batch of data.\n",
    "\n",
    "## Algorithm Evaluation\n",
    "\n",
    "$\\textbf{Efficient learning?}$\n",
    "\n",
    "1. How well does it do at the end? \n",
    "2. How well does it do during learning?\n",
    "\n",
    "$\\textbf{Performance metrics}$\n",
    "\n",
    "1. Total (discounted?) reward\n",
    "2. Risk-sensitivity\n",
    "3. Average vs. worst-case\n",
    "4. Knowledge acquired, skills\n",
    "\n",
    "## Aspects of environments\n",
    "\n",
    "1. #states, #actions\n",
    "2. Bounded rewards? Random rewards?\n",
    "3. Deterministic transitions?\n",
    "4. Factored state? Factored actions? \n",
    "5. Special dynamics (e.g. linear)?\n",
    "6. Observations: How much do they reveal of the state of the environment? \n",
    "7. How much memory is needed to solve the task?\n",
    "8. Factored observations?\n",
    "9. Diameter of environment?\n",
    "10. ..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <a id='toc6_'></a>[tldr](#toc0_)\n",
    "\n",
    "- The ${\\textcolor{red}{\\textbf{expected SARSA}}}$ algorithm explicitly computes the **expectation under its policy**, which is more expensive than sampling but has **lower variance**."
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
