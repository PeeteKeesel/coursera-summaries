{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Table of contents**<a id='toc0_'></a>    \n",
    "- [Lunar Lander](#toc1_)    \n",
    "- [Neurons](#toc2_)    \n",
    "- [Markov Decision Processes](#toc3_)    \n",
    "- [Episodic vs. Continuing Tasks](#toc4_)    \n",
    "- [Notes on Lunar Lander Environment](#toc5_)    \n",
    "\n",
    "<!-- vscode-jupyter-toc-config\n",
    "\tnumbering=false\n",
    "\tanchor=true\n",
    "\tflat=false\n",
    "\tminLevel=1\n",
    "\tmaxLevel=6\n",
    "\t/vscode-jupyter-toc-config -->\n",
    "<!-- THIS CELL WILL BE REPLACED ON TOC UPDATE. DO NOT WRITE YOUR TEXT IN THIS CELL -->"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <a id='toc1_'></a>[Lunar Lander](#toc0_)\n",
    "\n",
    "<p align=\"center\">\n",
    "  <img width=\"700\" height=\"400\" src=\"imgs/c4m2-reward-function.png\">\n",
    "</p>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <a id='toc2_'></a>[Neurons](#toc0_)\n",
    "\n",
    "- A neuron's output, or action, is called an **action potential**, transmitted to other neurons by its **axon**.\n",
    "- When a neuron generates an action potential it is said to **fire**.\n",
    "- Action potentials act on other neurons via **synapses**.\n",
    "- Synapses have **efficacies** (weights or strengths) which change during learning. \n",
    "\n",
    "We could say \n",
    "- Neurons are RL agents.\n",
    "- The brain is a **society** of RL agents.\n",
    "\n",
    "$\\textbf{Klopf's specific hypothesis:}$\n",
    "\n",
    "> - When a neuron fires an action potential, **all the contributing synapses** become ${\\textcolor{red}{\\textbf{eligible}}}$ to undergo changes in their efficacies, or weights. \n",
    ">\n",
    "> - If the action potential is followed **withing an appropriate time period** by an increase in reward, the efficacies of all eligible synpases increase (or decrease in the case of punishment).\n",
    "\n",
    "$\\textbf{What are Eligbility Traces?}$\n",
    "\n",
    "- ${\\textcolor{red}{\\textbf{Eligibility Traces}}}$ are a **memory mechanism** that tracks which **synapses** (connections) were active recently, so that **delayed reward or punishments** can update the correct connections.\n",
    "- They act like **\"credit markers\"** for learning: if a reward comes later, which synapses should get credit? \n",
    "\n",
    "In practice, RL uses a **simplified exponential decay** trace:\n",
    "\n",
    "$$\n",
    "    e_t(s) = \\gamma\\lambda e_{t-1}(s) + \\nabla\\hat{v}(s)\n",
    "$$\n",
    "\n",
    "$\\textbf{Two Types of Eligbility Traces:}$\n",
    "\n",
    "- ${\\textcolor{blue}{\\textbf{Contingent eligibility}}}$ (requires pre- and post-synaptic activity) $\\rightarrow$ Used by the actor\n",
    "  - Purpose: learns to act\n",
    "- ${\\textcolor{blue}{\\textbf{Non-contingent eligibility}}}$ (only pre-synaptic activity) $\\rightarrow$ Used by the critic\n",
    "  - Purpose: predicts reward\n",
    "\n",
    "$\\textbf{Key Takeaways:}$\n",
    "\n",
    "- Eligibility traces **connect past activity to delayed feedback**.\n",
    "- They're used in nearly all RL algorithms, often in a simple one-step form.\n",
    "- They help implement **time-sensitive learning** in both artificial and biological systems.\n",
    "- Neuroscience increasingly supports their existence in the brain.\n",
    "\n",
    "Before we tried to maximize RL now we try to ***predict*** RL. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <a id='toc3_'></a>[Markov Decision Processes](#toc0_)\n",
    "\n",
    "- ${\\textcolor{red}{\\textbf{MDPs}}}$ provide a general framework for sequential decision-making\n",
    "- The **dynamics** of an MDP are defined by a probability distribution\n",
    "\n",
    "The k-Armed Bandit problem we looked at previously, introduces many interesting questions. However, it doesn't include many aspects of real-world problems. The agent is presented with the same situation and each time and the same action is always optimal. In many problems, different situations call for different responses. The actions we choose now affect the amount of reward we can get into the future. The Markov Decision Process formalism captures these two aspects of real-world problems.\n",
    "\n",
    "$\\textbf{Markov Property:}$\n",
    "\n",
    "> Future state $s'$ and reward $r$ **only depends** on the current state $s$ and action $a$.\n",
    ">\n",
    "> Or differently said: \n",
    ">\n",
    "> ${\\textcolor{green}{\\textbf{The present state contains all the information necessary to predict the future.}}}$\n",
    "\n",
    "It means that the present state is sufficient and remembering earlier states would not improve predictions about the future."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <a id='toc4_'></a>[Episodic vs. Continuing Tasks](#toc0_)\n",
    "\n",
    "**üéÆ Episodic Task Example: Video Game Agent**\n",
    "\n",
    "- Agent collects treasure blocks for +1 reward.\n",
    "- Episode ends when the agent touches a green enemy.\n",
    "- Each episode **resets** to the same initial state.\n",
    "- Objective: maximize total reward **per episode**.\n",
    "- Naturally modeled as an **episodic task**.\n",
    "\n",
    "**üñ•Ô∏è Continuing Task Example: Job Scheduler**\n",
    "\n",
    "- Agent schedules jobs on servers based on priority.\n",
    "- Accepting high-priority jobs gives reward; rejecting gives penalty.\n",
    "- Jobs keep arriving; servers are reused.\n",
    "- Task **never ends** ‚Äî no natural episodes.\n",
    "- Modeled as a **continuing task**.\n",
    "\n",
    "$\\textbf{Key Takeaways:}$\n",
    "\n",
    "- ${\\textcolor{red}{\\textbf{Episodic tasks}}}$:\n",
    "  - break naturally into independent episodes\n",
    "  - have clear beginnings and ends, often reset to initial state.\n",
    "- ${\\textcolor{red}{\\textbf{Continuing tasks}}}$:\n",
    "  - are assumed to continue indefinitely\n",
    "  - go on indefinitely, agent continually interacts with the environment.\n",
    "- Choose the formulation that fits the **structure and goals** of the problem.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <a id='toc5_'></a>[Notes on Lunar Lander Environment](#toc0_)\n",
    "\n",
    "In this notebook, you will be applying these functions to __structure the reward signal__ based on the following criteria:\n",
    "\n",
    "1. **The lander will crash if** it touches the ground when ``y_velocity < -3`` (the downward velocity is greater than three).\n",
    "\n",
    "\n",
    "2. **The lander will crash if** it touches the ground when ``x_velocity < -10 or 10 < x_velocity`` (horizontal speed is greater than $10$).\n",
    "\n",
    "\n",
    "3. The lander's angle taken values in $[0, 359]$. It is completely vertical at $0$ degrees. **The lander will crash if** it touches the ground when ``5 < angle < 355`` (angle differs from vertical by more than $5$ degrees).\n",
    "\n",
    "\n",
    "4. **The lander will crash if** it has yet to land and ``fuel <= 0`` (it runs out of fuel).\n",
    "\n",
    "\n",
    "5. MST would like to save money on fuel when it is possible **(using less fuel is preferred)**.\n",
    "\n",
    "\n",
    "6. The lander can only land in the landing zone. **The lander will crash if** it touches the ground when ``x_position`` $\\not\\in$ ``landing_zone`` (it lands outside the landing zone).\n",
    "\n",
    "\n",
    "Fill in the methods below to create an environment for the lunar lander."
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
