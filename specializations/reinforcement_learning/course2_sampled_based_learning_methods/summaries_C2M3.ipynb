{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Table of contents**<a id='toc0_'></a>    \n",
    "- [Temporal Difference Learning Methods for Prediction](#toc1_)    \n",
    "  - [Introduction to Temporal Difference Learning](#toc1_1_)    \n",
    "    - [What is Temporal Difference (TD) Learning?](#toc1_1_1_)    \n",
    "  - [Advantages of TD](#toc1_2_)    \n",
    "\n",
    "<!-- vscode-jupyter-toc-config\n",
    "\tnumbering=false\n",
    "\tanchor=true\n",
    "\tflat=false\n",
    "\tminLevel=1\n",
    "\tmaxLevel=6\n",
    "\t/vscode-jupyter-toc-config -->\n",
    "<!-- THIS CELL WILL BE REPLACED ON TOC UPDATE. DO NOT WRITE YOUR TEXT IN THIS CELL -->"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> This notebook contains notes and summaries for week 2️⃣ from course 2️⃣ from the `Reinforcement Learning Specialization` from Coursera and the University of Alberta."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <a id='toc1_'></a>[Temporal Difference Learning Methods for Prediction](#toc0_)\n",
    "\n",
    "**Learning Objectives**\n",
    "\n",
    "- Define temporal-difference learning\n",
    "- Define the temporal-difference error\n",
    "- Understand the TD(0) algorithm\n",
    "- Understand the benefits of learning online with TD\n",
    "- Identify key advantages of TD methods over Dynamic Programming and Monte Carlo methods\n",
    "- Identify the empirical benefits of TD learning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <a id='toc1_1_'></a>[Introduction to Temporal Difference Learning](#toc0_)\n",
    "\n",
    "### <a id='toc1_1_1_'></a>[What is Temporal Difference (TD) Learning?](#toc0_)\n",
    "\n",
    "Since now, we estimated Values from Returns: \n",
    "\n",
    "$$\n",
    "\\begin{align}\n",
    "    v_{\\pi}(s) &= \\mathbb{E}_{\\pi}[G_t | S_t = s] \\\\ \n",
    "    V(S_t)     &\\leftarrow V(S_t) + \\alpha[ G_t - V(S_t) ]\n",
    "\\end{align}\n",
    "$$\n",
    "\n",
    "**Bootstrapping**\n",
    "$$\n",
    "\\begin{align}\n",
    "    G_t &= R_{t+1} + \\gamma R_{t+2} + \\gamma^2 R_{t+3} + \\dots \\qquad && \\textcolor{grey}{\\text{Definition of Discounted Return}} \\\\\n",
    "        &= R_{t+1} + \\gamma G_{t+1} \\qquad && \\textcolor{grey}{\\text{Written recursively}}\n",
    "\\end{align}\n",
    "$$\n",
    "\n",
    "We then saw that the value of a state at time $t$ is the expected return at time $t$:\n",
    "$$\n",
    "\\begin{align}\n",
    "    v_{\\pi}(s) &= \\mathbb{E}_{\\pi}[ G_t | S_t = s ] \\\\ \n",
    "               &= \\mathbb{E}_{\\pi}[ R_{t+1} + \\gamma G_{t+1} | S_t = s ] \\\\ \n",
    "               &= R_{t+1} + \\gamma v_{\\pi}(S_{t+1}) \\qquad && \\textcolor{grey}{\\text{Value function written recursively}}\n",
    "\\end{align}\n",
    "$$\n",
    "\n",
    "Let's go back to our incremental Monte Carlo update rule. We want to update toward the return but we don't want to wait for it. \n",
    "$$\n",
    "\\begin{align}\n",
    "    V(S_t) &\\leftarrow V(S_t) + \\alpha [ G_T - V(S_t) ] \\qquad && G_t \\approx R_{t+1} + \\gamma V(S_{t+1}) \\\\ \n",
    "    \\colorbox{#CCCCFF}{$V(S_t) \\leftarrow V(S_t) + \\alpha [ R_{t+1} + \\gamma V(S_{t+1}) - V(S_t)]$}\n",
    "\\end{align}\n",
    "$$\n",
    "\n",
    "$\\colorbox{#D3D3D3}{TD updates the value of one state towards its own estimate of the value in the next state}$. As the estimated value for the next state improves, so does our target. In fact, we've done something like this before in dynamic programming. In DP, we update it toward the value of all possible next states. The primary difference is in DP, we use an expectation over all possible next states. We needed a model of the environment to compute this expectation, in TD we only need the next state. We can get that directly from the environment without a model. \n",
    "\n",
    "But how do we get that next state directly from the environment? \n",
    "\n",
    "Think of time $t+1$ as the current time step and $t$ as the previous. So we simply store the state from the previous time step in order to make our TD updates. We see a stream of experience: state, action, reward, next state and so on.\n",
    "\n",
    "> S, A, R, S, A, R, S, A, R, ...\n",
    "\n",
    "\n",
    "<p align=\"center\">\n",
    "  <img width=\"700\" height=\"350\" src=\"imgs/td-0.png\">\n",
    "</p>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <a id='toc1_2_'></a>[Advantages of TD](#toc0_)\n",
    "\n",
    "- ✅Online and incremental\n",
    "- ✅Converge faster than MC methods\n",
    "- ✅Do not require a model of the environment"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
