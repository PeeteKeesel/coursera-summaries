{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Table of contents**<a id='toc0_'></a>    \n",
    "- [Monte-Carlo Methods for Prediction & Control](#toc1_)    \n",
    "  - [Introduction to Monte-Carlo Methods](#toc1_1_)    \n",
    "    - [What is Monte-Carlo?](#toc1_1_1_)    \n",
    "    - [Using Monte Carlo for Prediction](#toc1_1_2_)    \n",
    "  - [Monte-Carlo for Control](#toc1_2_)    \n",
    "    - [Using Monte Carlo for Action Values](#toc1_2_1_)    \n",
    "    - [Using Monte Carlo Methods for Generalized Policy Iteration](#toc1_2_2_)    \n",
    "  - [Exploration Methods for Monte-Carlo](#toc1_3_)    \n",
    "    - [Episolon-Soft Policies](#toc1_3_1_)    \n",
    "  - [Off-Policy Learning for Prediction](#toc1_4_)    \n",
    "    - [Why Does Off-Policy Learning Matter?](#toc1_4_1_)    \n",
    "    - [Importance Sampling](#toc1_4_2_)    \n",
    "    - [Off-Policy Monte Carlo Prediction](#toc1_4_3_)    \n",
    "\n",
    "<!-- vscode-jupyter-toc-config\n",
    "\tnumbering=false\n",
    "\tanchor=true\n",
    "\tflat=false\n",
    "\tminLevel=1\n",
    "\tmaxLevel=6\n",
    "\t/vscode-jupyter-toc-config -->\n",
    "<!-- THIS CELL WILL BE REPLACED ON TOC UPDATE. DO NOT WRITE YOUR TEXT IN THIS CELL -->"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> This notebook contains notes and summaries for week :one: from course :two: from the `Reinforcement Learning Specialization` from Coursera and the University of Alberta.\n",
    ">\n",
    "> :books: This module's chapter summary can be found in the Reinforcement Learning textbook (2018) in **Chapter 5.10 (pp. 115-116)**."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <a id='toc1_'></a>[Monte-Carlo Methods for Prediction & Control](#toc0_)\n",
    "\n",
    "<details>\n",
    "  <summary>Learning Objectives:</summary>\n",
    "\n",
    "- Understand how Monte-Carlo methods can be used to estimate value functions from sampled interaction\n",
    "- Identify problems that can be solved using Monte-Carlo methods\n",
    "- Use Monte-Carlo prediction to estimate the value function for a given policy.\n",
    "- Estimate action-value functions using Monte-Carlo\n",
    "- Understand the importance of maintaining exploration in Monte-Carlo algorithms\n",
    "- Understand how to use Monte-Carlo methods to implement a GPI algorithm\n",
    "- Apply Monte-Carlo with exploring starts to solve an MDP\n",
    "- Understand why Exploring Starts can be problematic in real problems\n",
    "- Describe an alternative exploration method for Monte-Carlo control\n",
    "- Understand how off-policy learning can help deal with the exploration problem\n",
    "- Produce examples of target policies and examples of behavior policies.\n",
    "- Understand importance sampling\n",
    "- Use importance sampling to estimate the expected value of a target distribution using samples from a different distribution.\n",
    "- Understand how to use importance sampling to correct returns\n",
    "- Understand how to modify the Monte-Carlo prediction algorithm for off-policy learning.\n",
    "\n",
    "</details>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <a id='toc1_1_'></a>[Introduction to Monte-Carlo Methods](#toc0_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <a id='toc1_1_1_'></a>[What is Monte-Carlo?](#toc0_)\n",
    "\n",
    "- Monte Carlo methods estimate values by **averaging** over a large number of **random samples**\n",
    "\n",
    "Recall that \n",
    "\n",
    "$$\n",
    "\\begin{align*}\n",
    "\n",
    "V_{\\pi}(s) \\dot{=} \\mathbb{E}_{\\pi}[ G_t | S_t = s ]\n",
    "\n",
    "\\end{align*}\n",
    "$$\n",
    "\n",
    "and that e.g. $G_0 = R_1 + \\gamma R_2 + \\gamma^2 R_3 + \\gamma^3 R_4$ for an episode task of $4$ states.\n",
    "\n",
    "$$\n",
    "\\begin{align*}\n",
    "\n",
    "& \\underline{\\phantom{\\textbf{MC prediction, for estimating} \\; V \\approx v_{\\pi}}} \\\\\n",
    "& \\underline{\\textbf{MC prediction, for estimating} \\; V \\approx v_{\\pi}} \\\\\n",
    "& \\text{Input:} \\; \\text{a policy} \\; \\pi \\; \\text{to be evaluated} \\\\\n",
    "& \\text{Initialize:} \\\\\n",
    "& \\qquad V(s) \\in \\mathbb{R}, \\; \\text{arbitrarily, for all} \\; s \\in \\mathcal{S} \\\\\n",
    "& \\qquad Returns(s) \\leftarrow \\text{an empty list, for all} \\; s \\in \\mathcal{S} \\\\\n",
    "\\\\ \n",
    "& \\text{Loop forever (for each episode)}: \\\\ \n",
    "& \\qquad \\text{Generate an episode following} \\; \\pi: S_0, A_0, R_1, S_1, A_1, R_2, \\dots, S_{T-1}, A_{T-1}, R_T \\\\ \n",
    "& \\qquad G \\leftarrow 0 \\\\\n",
    "& \\qquad \\text{Loop for each step of episode,} \\; t = T-1, T-2, \\dots, 0: \\\\\n",
    "& \\qquad \\qquad G \\leftarrow \\gamma G + R_{t+1} \\\\\n",
    "& \\qquad \\qquad \\text{Append} \\; G \\; \\text{to} \\; Returns(S_t) \\\\\n",
    "& \\qquad \\qquad V(S_t) \\leftarrow \\text{average}(Returns(S_t)) \\\\\n",
    "\n",
    "\\end{align*}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <a id='toc1_1_2_'></a>[Using Monte Carlo for Prediction](#toc0_)\n",
    "\n",
    "**Implications of Monte Carlo learning** \n",
    "\n",
    "- We do not need to keep a **large model** of the environment\n",
    "- We are estimating the value of an individual state **independently** of the values of other states\n",
    "- The **computation** needed to update the value of each state does not depend on the size of the MDP"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <a id='toc1_2_'></a>[Monte-Carlo for Control](#toc0_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <a id='toc1_2_1_'></a>[Using Monte Carlo for Action Values](#toc0_)\n",
    "\n",
    "<details>\n",
    "<summary>Content of Section:</summary>\n",
    "\n",
    "- *Estimate* **action-value functions** using Monte Carlo\n",
    "- *Understand* the importance of **maintaining exploration** in Monte Carlo algorithms\n",
    "\n",
    "</details>\n",
    "\n",
    "Recall that \n",
    "\n",
    "$$\n",
    "\\begin{align*}\n",
    "\n",
    "q_{\\pi}(s, a) \\dot{=} \\mathbb{E}_{\\pi}[ G_t | S_t = s, A_t = a ]\n",
    "\n",
    "\\end{align*}\n",
    "$$\n",
    "\n",
    "- Action-values are useful for learning a policy \n",
    "- They allow us to compare different actions in a state\n",
    "\n",
    "In a deterministic policy, it could happen that an action is never selected, and thus the agent never observes returns corresponding to that action. Consequently, we won't be able to form an accurate MC estimate. Therefore, \n",
    "\n",
    "> the agent must try all the actions in each state in order to learn their values. We need to **mainting exploration**.\n",
    "\n",
    "One way to maintain exploration is called $\\colorbox{#CCCCFF}{Exploring Starts}$.\n",
    "\n",
    "$$\n",
    "\n",
    "\\begin{align*}\n",
    "\n",
    "\\underbrace{\\colorbox{#F4C2C2}{$s_0, a_0$}}_{Random}, \\underbrace{\\colorbox{#CCCCFF}{$s_1, a_1, s_2, a_2, \\dots$}}_{From \\; \\pi \\; and \\; p}\n",
    "\n",
    "\\end{align*}\n",
    "\n",
    "$$\n",
    "\n",
    "- We must guarantee that episodes start in every state-action pair.\n",
    "- Afterwards, the agent simply follows its policy\n",
    "\n",
    "> *Exploring Starts* can be used to evaluate deterministic policies.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <a id='toc1_2_2_'></a>[Using Monte Carlo Methods for Generalized Policy Iteration](#toc0_)\n",
    "\n",
    "<details>\n",
    "<summary>Content of Section:</summary>\n",
    "\n",
    "- *Understand* how to use MC methods to implement a **Generalized Policy Iteration** algorithm\n",
    "\n",
    "</details>\n",
    "\n",
    "**Policy Improvement Step**: (make the policy greedy with respect to the agent's current action value estimates)\n",
    "\n",
    "$$\n",
    "\\begin{align*}\n",
    "\n",
    "\\pi_{k+1}(s) \\dot{=} argmax_a q_{\\pi_k} (s, a)\n",
    "\n",
    "\\end{align*}\n",
    "$$\n",
    "\n",
    "**Policy Evaluation Step**: (to estimate the action values)\n",
    "\n",
    "$$\n",
    "\\begin{align*}\n",
    "\n",
    "Monte \\; Carlo \\; Prediction\n",
    "\n",
    "\\end{align*}\n",
    "$$\n",
    "\n",
    "$$\n",
    "\\begin{align*}\n",
    "\n",
    "& \\underline{\\phantom{\\textbf{Monte Carlo ES (Exploring Starts), for estimating} \\; \\pi \\approx \\pi_*}} \\\\\n",
    "& \\underline{\\textbf{Monte Carlo ES (Exploring Starts), for estimating} \\; \\pi \\approx \\pi_*} \\\\\n",
    "\n",
    "& \\text{Initialize:} \\\\\n",
    "& \\qquad \\pi(s) \\in \\mathcal{A}(s) \\; (\\text{arbitrarily}), \\; \\text{for all} \\; s \\in \\mathcal{S} \\\\\n",
    "& \\qquad Q(s, a) \\in \\mathbb{R} \\; (\\text{arbitrarily}), \\; \\text{for all} \\; s \\in \\mathcal{S}, a \\in \\mathcal{A}(s) \\\\\n",
    "& \\qquad Returns(s,a) \\leftarrow \\text{empty list, for all} \\; s \\in \\mathcal{S}, a \\in \\mathcal{A}(s) \\\\\n",
    "\\\\\n",
    "& \\text{Loop forever (for each episode)}: \\\\ \n",
    "& \\qquad \\text{Choose} \\; S_0 \\in \\mathcal{S}, A_0 \\in \\mathcal{A}(S_0) \\; \\text{randomly such that all paids have probability} \\; > 0 \\\\\n",
    "& \\qquad \\text{Generate an episode from} \\; S_0, A_0, \\; \\text{following} \\; \\pi: S_0, A_0, R_1, \\dots, S_{T-1}, A_{T-1}, R_T \\\\\n",
    "& \\qquad G \\leftarrow 0 \\\\\n",
    "& \\qquad \\text{Loop for each step of episode,} \\; t=T-1, T-2, \\dots, 0: \\\\\n",
    "& \\qquad \\qquad G \\leftarrow \\gamma G + R_{t+1} \\\\\n",
    "& \\qquad \\qquad \\text{Append} \\; G \\; \\text{to} \\; Returns(S_t, A_t) \\\\\n",
    "& \\qquad \\qquad Q(S_t, A_t) \\leftarrow \\text{average}(Returns(S_t, A_t)) && \\color{Grey}{\\text{policy evaluation}} \\\\\n",
    "& \\qquad \\qquad \\pi(S_t) \\leftarrow argmax_a Q(S_t, a) && \\color{Grey}{\\text{policy improvement}} \\\\\n",
    "\n",
    "\\end{align*}\n",
    "$$\n",
    "\n",
    "- Is problematic because \n",
    "  - it $\\colorbox{#C4C3D0}{must be able to start from every possible state-action pair}$.\n",
    "  - it can be difficult to randomly sample an initial State action pair (e.g. self-driving car)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <a id='toc1_3_'></a>[Exploration Methods for Monte-Carlo](#toc0_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <a id='toc1_3_1_'></a>[Episolon-Soft Policies](#toc0_)\n",
    "\n",
    "<details>\n",
    "<summary>Content of Section:</summary>\n",
    "\n",
    "- *Understand* why Exploring Starts can be problematic in real problems\n",
    "- *Describe* an alternative method to **maintain exploration** in MC control\n",
    "\n",
    "</details>\n",
    "\n",
    "- Combines $\\epsilon$-greedy with MC to learn near optimal policies\n",
    "\n",
    "<p align=\"center\">\n",
    "  <img width=\"600\" height=\"300\" src=\"imgs/eps-soft-policies.png\">\n",
    "</p>\n",
    "\n",
    "- The uniform-random-policy is for example an $\\epsilon$-soft policy.\n",
    "- with $\\epsilon$-greedy exploration the agent will find an $\\epsilon$-soft policy, which is stochastic\n",
    "\n",
    "$\\epsilon$-soft policies: \n",
    "- assign non-zero probability to each action in every sate\n",
    "- are always stochastic\n",
    "- may not be optimal\n",
    "- but finds the optimal $\\epsilon$-soft policy\n",
    "\n",
    "<p align=\"center\">\n",
    "  <img width=\"600\" height=\"350\" src=\"imgs/eps-soft-policies-algo.png\">\n",
    "</p>\n",
    "\n",
    "Cons: \n",
    "- Values based on $\\colorbox{#C4C3D0}{suboptimal policy}$\n",
    "- Actions based on $\\colorbox{#C4C3D0}{suboptimal policy}$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <a id='toc1_4_'></a>[Off-Policy Learning for Prediction](#toc0_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <a id='toc1_4_1_'></a>[Why Does Off-Policy Learning Matter?](#toc0_)\n",
    "\n",
    "\n",
    "<details>\n",
    "<summary>Content of Section:</summary>\n",
    "\n",
    "- *Understand* how **off-policy learning** can help deal with the exploration problem\n",
    "- *Produce* examples of **target policies**\n",
    "- *Produce* examples of **behavior policies**\n",
    "\n",
    "</details>\n",
    "\n",
    "- In policy evaluation we mean to learn the value function\n",
    "- In control we mean learning the optimal policy\n",
    "\n",
    "**On-Policy** versus **Off-Policy**\n",
    "\n",
    "| On Policy | Off Policy | \n",
    "| --------- | ---------- |\n",
    "| improve and evaluate the policy being used to select actions | improve and evaluate a different policy from the one used to select actions | \n",
    "| The policy that we are learning is the *same* policy we are using for action selection | The policy that we are learning is *off* the policy we are using for action selection |\n",
    "\n",
    "- **Target Policy** $\\pi(a | s)$: policy the agent is learning\n",
    "  - Learn *values* for this policy\n",
    "  - e.g. the optimal policy \n",
    "- **Behavior Policy** $b(a | s)$: policy the agent is using to select actions\n",
    "  - Select *actions* from this policy\n",
    "  - Generally and *exploratory* policy\n",
    "\n",
    "> - **Off-policy learning** allows learning an *optimal policy* from *suboptimal behavior*\n",
    "> - The policy that we are *learning* is the **target policy**\n",
    "> - The polcy that we are choosing *action* from is the **behavior policy**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <a id='toc1_4_2_'></a>[Importance Sampling](#toc0_)\n",
    "\n",
    "<details>\n",
    "<summary>Content of Section:</summary>\n",
    "\n",
    "- *Use* **importance sampling** to estimate the expected value of a distribution using samples from a different distribution\n",
    "\n",
    "</details>\n",
    "\n",
    "Let's see how we can estimate the expected value of $x$ under distribution $\\pi$ by using the sample average, the samples drawn from distribution $b$.\n",
    "\n",
    "$$\n",
    "\\begin{align*}\n",
    "\n",
    "\\text{Sample}: \\; x \\sim b  \\\\ \n",
    "\\text{Estimate}: \\; \\mathbb{E}_{\\pi}[X]  \\\\ \n",
    "\\\\\n",
    "\\mathbb{E}_{\\pi}[X] &\\dot{=} \\sum_{x \\in X}x\\pi(x) \\\\ \n",
    "                    &= \\sum_{x \\in X}x\\pi(x)\\frac{b(x)}{b(x)} \\\\ \n",
    "                    &= \\sum_{x \\in X}x\\frac{\\pi(x)}{b(x)}b(x) \\\\ \n",
    "                    &= \\sum_{x \\in X}\\underbrace{x\\rho(x)}_{\\text{new rv}}b(x) \\\\ \n",
    "                    &= \\mathbb{E}_b[X\\rho(X)] \\\\ \n",
    "\\\\\n",
    "\\mathbb{E}_b[X\\rho(X)] &\\approx \\frac{1}{n} \\sum_{i=1}^{n}x_i\\rho(x_i) \\\\ \n",
    "x_i &\\sim b\n",
    "\n",
    "\\end{align*}\n",
    "$$\n",
    "\n",
    "\n",
    "> - **Importance sampling** uses samples from one probability distribution to estimate the *expectation* of a different distribution"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <a id='toc1_4_3_'></a>[Off-Policy Monte Carlo Prediction](#toc0_)\n",
    "\n",
    "<details>\n",
    "<summary>Content of Section:</summary>\n",
    "\n",
    "- *Understand* how to use  **importance sampling** to correct returns\n",
    "- *Understand* how to modify the **Monte Carlo prediction algorithm** for off-policy learning\n",
    "\n",
    "</details>\n",
    "\n",
    "**Off-Policy Monte Carlo**\n",
    "\n",
    "$$\n",
    "\\begin{align*}\n",
    "    V_{\\pi}(s) &\\dot{=} \\mathbb{E}_{\\pi}[G_t | S_t = s] \\\\ \n",
    "               &\\approx average(\\rho_0 Returns[0], \\rho_1 Returns[1], \\rho_2 Returns[2]) && \\color{Grey}{\\text{corrected each return in the average}}\n",
    "\\end{align*}\n",
    "$$\n",
    "\n",
    "We only need to figure out the value of $\\rho_i$ for each of the sampled returns $Returns[i]$.\n",
    "\n",
    "$$\n",
    "\\begin{align*}\n",
    "    \\rho = \\frac{\\mathbb{P}(\\text{trajectory under} \\; \\pi)}{\\mathbb{P}(\\text{trajectory under} \\;b)}\n",
    "\\end{align*}\n",
    "$$\n",
    "\n",
    "With this correction we get the expectation of the return under $\\pi$:\n",
    "$$\n",
    "\\begin{align*}\n",
    "    V_{\\pi}(s) = \\mathbb{E}_b [ \\rho G_t | S_t = s ]\n",
    "\\end{align*}\n",
    "$$\n",
    "\n",
    "To compute $\\rho$, we need to figure out how to compute the probability of a trajectory under a policy.\n",
    "\n",
    "**Off-Policy Trajectories**\n",
    "\n",
    "$$\n",
    "\\begin{align*}\n",
    "    P(A_t, S_{t+1}, A_{t+1}, \\dots, S_T | S_t, A_{t:T}) &= b(A_t | S_t) p(S_{t+1} | S_t, A_t) b(A_{t+1} | S_{t+1}) p(S_{t+2} | S_{t+1}, A_{t+1}) \\dots p(S_T | S_{T-1}, A_{T-1}) \\\\\n",
    "               \\mathbb{P}(\\text{trajectory under} \\; b) &= \\prod_{k=t}^{T-1} b(A_k | S_k) p(S_{k+1} | S_k, A_k)\n",
    "\\end{align*}\n",
    "$$\n",
    "\n",
    "where $b(A_t | S_t)$ is the probability that the agent selects action $A_t$ when being in state $S_t$, and $p(S_{t+1} | S_t, A_t)$ is the probability that the environment transitions into state $S_{t+1}$. Now we can define $\\rho$ using the probabilities of the trajectories under $\\pi$ and $b$. \n",
    "\n",
    "$$\n",
    "\\begin{align*}\n",
    "    \\rho_{t:T-1} &\\dot{=} \\frac{\\mathbb{P}(\\text{trajectory under} \\; \\pi)}{\\mathbb{P}(\\text{trajectory under} \\;b)} \\\\ \n",
    "                 &\\dot{=} \\prod_{k=t}^{T-1} \\frac{\\pi(A_k | S_k) p(S_{k+1} | S_k, A_k)}{b(A_k | S_k) p(S_{k+1} | S_k, A_k)} \\\\ \n",
    "                 &\\dot{=} \\prod_{k=t}^{T-1} \\frac{\\pi(A_k | S_k)}{b(A_k | S_k)} \\\\\n",
    "                 &=       \\rho_t\\rho_{t+1}\\rho_{t+1}\\dots\\rho_{T-2}\\rho_{T-1}\n",
    "\\end{align*}\n",
    "$$\n",
    "\n",
    "**Off-Policy Value**\n",
    "\n",
    "$$\n",
    "\\begin{align*}\n",
    "    \\mathbb{E}_b[\\rho_{t:T-1} G_t | S_t = s] = v_{\\pi}(s)\n",
    "\\end{align*}\n",
    "$$\n",
    "\n",
    "<p align=\"center\">\n",
    "  <img width=\"600\" height=\"350\" src=\"imgs/off-policy-every-visit-mc-prediction.png\">\n",
    "</p>\n",
    "\n",
    "**Summary**\n",
    "\n",
    "- Used **importance sampling ratios** to correct the *returns*\n",
    "- Modified the *on-policy* **MC prediction algorithm** for *off-policy* learning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
