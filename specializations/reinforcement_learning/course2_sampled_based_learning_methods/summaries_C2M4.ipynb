{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Table of contents**<a id='toc0_'></a>    \n",
    "- [Temporal Difference Learning Methods for Control](#toc1_)    \n",
    "  - [TD for Control](#toc1_1_)    \n",
    "    - [SARSA GPI with TD](#toc1_1_1_)    \n",
    "  - [Off-Policy TD Control: Q-Learning](#toc1_2_)    \n",
    "  - [Expected SARSA](#toc1_3_)    \n",
    "- [Summary](#toc2_)    \n",
    "\n",
    "<!-- vscode-jupyter-toc-config\n",
    "\tnumbering=false\n",
    "\tanchor=true\n",
    "\tflat=false\n",
    "\tminLevel=1\n",
    "\tmaxLevel=6\n",
    "\t/vscode-jupyter-toc-config -->\n",
    "<!-- THIS CELL WILL BE REPLACED ON TOC UPDATE. DO NOT WRITE YOUR TEXT IN THIS CELL -->"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> This notebook contains notes and summaries for week 3️⃣ from course 2️⃣ from the `Reinforcement Learning Specialization` from Coursera and the University of Alberta."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <a id='toc1_'></a>[Temporal Difference Learning Methods for Control](#toc0_)\n",
    "\n",
    "**Learning Objectives**\n",
    "\n",
    "- Explain how generalized policy iteration can be used with TD to find improved policies\n",
    "- Describe the Sarsa Control algorithm\n",
    "- Understand how the Sarsa control algorithm operates in an example MDP\n",
    "- Analyze the performance of a learning algorithm\n",
    "- Describe the Q-learning algorithm\n",
    "- Explain the relationship between Q-learning and the Bellman optimality equations.\n",
    "- Apply Q-learning to an MDP to find the optimal policy\n",
    "- Understand how Q-learning performs in an example MDP\n",
    "- Understand the differences between Q-learning and Sarsa\n",
    "- Understand how Q-learning can be off-policy without using importance sampling\n",
    "- Describe how the on-policy nature of SARSA and the off-policy nature of Q-learning affect their relative performance\n",
    "- Describe the Expected Sarsa algorithm\n",
    "- Describe Expected Sarsa’s behaviour in an example MDP\n",
    "- Understand how Expected Sarsa compares to Sarsa control\n",
    "- Understand how Expected Sarsa can do off-policy learning without using importance sampling\n",
    "- Explain how Expected Sarsa generalizes Q-learning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <a id='toc1_1_'></a>[TD for Control](#toc0_)\n",
    "\n",
    "In chapters, we talked about using generalized policy iteration to find an optimal policy. We've also talked about using TD to estimate value functions. What would it look like if we use TD to do the policy evaluation step in generalized policy iteration?\n",
    "\n",
    "### <a id='toc1_1_1_'></a>[SARSA GPI with TD](#toc0_)\n",
    "\n",
    "Sarsa makes predictions about the values of state action pairs. The agent chooses an action, in the initial state to create the first state action pair. Next, it takes that action in the current state and observes the reward RT plus 1 and next state ST plus one. In Sarsa, the agent needs to know its next state action pair before updating its value estimates. That means it has to commit to its next action before the update. Since our agent is learning action values for a specific policy, it uses that policy to sample the next action. \n",
    "\n",
    "\n",
    "<p align=\"center\">\n",
    "  <img width=\"700\" height=\"350\" src=\"imgs/sarsa-illustrated.png\">\n",
    "</p>\n",
    "\n",
    "> SARSA is an **action-value** form of TD which combines these ideas"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <a id='toc1_2_'></a>[Off-Policy TD Control: Q-Learning](#toc0_)\n",
    "\n",
    "The new element in Q-learning is the action value update. Here, the target is the reward RT plus one plus Gamma times the maximum action value in the following state. This differs from Sarsa, which uses the value of the next state action pair in its target. \n",
    "\n",
    "<p align=\"center\">\n",
    "  <img width=\"700\" height=\"350\" src=\"imgs/q-learning-algo.png\">\n",
    "</p>\n",
    "\n",
    "> In fact, $\\textsf{Sarsa}$ is a sample-based algorithm to solve the Bellman equation for action values. $\\textsf{Q-learning}$ also solves the Bellman equation using samples from the environment. But instead of using the standard Bellman equation, Q-learning uses the Bellman's Optimality Equation for action values. \n",
    "\n",
    "<p align=\"center\">\n",
    "  <img width=\"700\" height=\"350\" src=\"imgs/q-learning-and-bellman.png\">\n",
    "</p>\n",
    "\n",
    "The optimality equations enable Q-learning to directly learn Q-star instead of switching between policy improvement and policy evaluation steps. \n",
    "\n",
    "> Even though Sarsa and Q-learning are both based on Bellman equations, they're based on very different Bellman equations. Sarsa is sample-based version of policy iteration which uses Bellman equations for action values, that each depend on a fixed policy. Q-learning is a sample-based version of value iteration which iteratively applies the Bellman optimality equation. \n",
    "\n",
    "Applying the Bellman's Optimality Equation strictly improves the value function, unless it is already optimal.\n",
    "\n",
    "- $\\textsf{Sarsa}$ $\\sim$ Policy Iteration\n",
    "- $\\textsf{Q-learning}$ $\\sim$ Value Iteration\n",
    "  - = off-policy algo\n",
    "  - but without using importance sampling\n",
    "\n",
    "Recall that an agent estimates its value function according to expected returns under their target policy. They actually behave according to their behavior policy. When the target policy and behavior policy are the same, the agent is learning **on-policy**, otherwise, the agent is learning **off-policy**. \n",
    "\n",
    "- In $\\textsf{Sarsa}$, the agent bootstraps off of the value of the action it's going to take next, which is sampled from its behavior policy. \n",
    "- $\\textsf{Q-learning}$ instead, bootstraps off of the largest action value in its next state. This is like sampling an action under an estimate of the optimal policy rather than the behavior policy. \n",
    "\n",
    "> Since Q-learning learns about the best action it could possibly take rather than the actions it actually takes, it is learning off-policy.\n",
    "\n",
    "Whenever you see a reinforced learning algorithm, a natural question to ask is, **\"What are the target in behavior policies?\"**\n",
    "\n",
    "But if Q-learning learns off-policy, why don't we see any important sampling ratios? \n",
    "\n",
    "- It is because the agent is estimating action values with unknown policy. It does not need important sampling ratios to correct for the difference in action selection. The action value function represents the returns following each action in a given state. The agents target policy represents the probability of taking each action in a given state. Putting these two elements together, the agent can calculate the expected return under its target policy from any given state, in particular, the next state, $S_{t+1}$.\n",
    "\n",
    "\n",
    "<p align=\"center\">\n",
    "  <img width=\"700\" height=\"350\" src=\"imgs/sarsa-vs-q-learning-cliff-env.png\">\n",
    "</p>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <a id='toc1_3_'></a>[Expected SARSA](#toc0_)\n",
    "\n",
    "<p align=\"center\">\n",
    "  <img width=\"700\" height=\"350\" src=\"imgs/bellmann-equation-for-action-values.png\">\n",
    "</p>\n",
    "\n",
    "> Explicitly computing the expectation over next actions is the main idea behind the expected Sarsa algorithm.\n",
    "\n",
    "The algorithm is nearly identical to Sarsa, except the T error uses the expected estimate of the next action value instead of a sample of the next action value. \n",
    "\n",
    "> That means that on every time step, the agent has to average the next state's action values according to how likely they are under the policy. \n",
    "\n",
    "<p align=\"center\">\n",
    "  <img width=\"700\" height=\"350\" src=\"imgs/expected-sarsa-algo.png\">\n",
    "</p>\n",
    "\n",
    "> In general, expected Sarsas update targets are much lower variance than Sarsas. \n",
    "\n",
    "The lower variance comes with a downside though. Computing the average over next actions becomes more expensive as the number of actions increases.  When there are many actions, computing the average might take a long time, especially since the average has to be computed every time step. In this video, we show that the expected Sarsa algorithm explicitly computes the expectation under its policy, which is more expensive than sampling but has lower variance. \n",
    "\n",
    "- $\\textsf{Expected Sarsa}$ is **more stable and learns faster** than Sarsa, especially with large step sizes, due to averaging over policy randomness.\n",
    "- $\\textsf{Sarsa}$ is **more sensitive to $\\alpha$** and may fail to converge at higher values.\n",
    "\n",
    "Expected Sarsa generalizes Q-learning. \n",
    "\n",
    "<p align=\"center\">\n",
    "  <img width=\"700\" height=\"350\" src=\"imgs/off-policy-expected-sarsa.png\">\n",
    "</p>\n",
    "\n",
    "- $\\textsf{Expected Sarsa}$ uses the same technique as **Q-learning** to learn **off-policy** without **importance sampling**\n",
    "- $\\textsf{Expected Sarsa}$ with a **target policy** that's **greedy** with respect to its action-values is exactly **Q-learning**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <a id='toc2_'></a>[Summary](#toc0_)\n",
    "\n",
    "<p align=\"center\">\n",
    "  <img width=\"700\" height=\"350\" src=\"imgs/summary-td-control-and-bellman-equations.png\">\n",
    "</p>\n",
    "\n",
    "- Sarsa, Q-learning, and Expected Sarsa are TD control methods based on Bellman equations.\n",
    "- Expected Sarsa outperforms Sarsa and Q-learning in online performance by reducing variance through averaging over next actions."
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
