{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üß† What is TD?\n",
    "\n",
    "**TD** stands for **Temporal Difference** learning.  \n",
    "It‚Äôs a method for learning **value functions** ‚Äî how good it is to be in a state ‚Äî by learning **from experience**.\n",
    "\n",
    "### ‚úÖ Key idea:\n",
    "> Use the **difference between successive value estimates** to update the current one.\n",
    "\n",
    "That difference is called the **TD error**:\n",
    "$$\n",
    "    \\delta = R_{t+1} + \\gamma V(S_{t+1}) - V(S_t)\n",
    "$$\n",
    "\n",
    "Instead of waiting for the **final return**, like in Monte Carlo methods, TD updates **after every step**, using the next state's value as a guess for the future.\n",
    "\n",
    "---\n",
    "\n",
    "## üìò Why is it called \"Temporal Difference\"?\n",
    "\n",
    "Because the update is based on the **difference between value estimates across time** (from one timestep to the next).\n",
    "\n",
    "---\n",
    "\n",
    "## ‚öôÔ∏è What is Semi-Gradient TD?\n",
    "\n",
    "**Semi-gradient TD** is when you:\n",
    "- Use **function approximation** (e.g., a neural net or linear weights) to estimate value\n",
    "- And you **only take the gradient of your estimate**, not the target\n",
    "\n",
    "---\n",
    "\n",
    "### ‚úÖ Why is it called ‚Äúsemi-gradient‚Äù?\n",
    "\n",
    "Because you're only partially following the full gradient of the loss function.  \n",
    "You compute:\n",
    "$$\n",
    "    \\mathbf{w}_{t+1} = \\mathbf{w}_{t} + \\alpha\\delta\\nabla\\hat{v}(S_t, \\mathbf{w}_{t})\n",
    "$$\n",
    "\n",
    "\n",
    "You don‚Äôt differentiate the **target** (which contains \\( \\hat{v}(S_{t+1}, \\mathbf{w}_t) \\)), only the prediction. That‚Äôs why it‚Äôs called **semi-gradient**, not full gradient.\n",
    "\n",
    "---\n",
    "\n",
    "## üîÅ How does TD compare to other RL methods?\n",
    "\n",
    "| Method           | Learns From        | Waits for Episode End? | Variance | Bias |\n",
    "|------------------|--------------------|-------------------------|----------|------|\n",
    "| **TD**           | One step ahead     | ‚ùå No                   | Low      | Yes  |\n",
    "| **Monte Carlo**  | Full return        | ‚úÖ Yes                  | High     | No   |\n",
    "| **Dynamic Prog.**| Known environment  | ‚ùå No                   | Low      | None |\n",
    "\n",
    "TD is great because:\n",
    "- It **doesn‚Äôt need a model**\n",
    "- Learns online (after each step)\n",
    "- Balances between Monte Carlo (more accurate) and Dynamic Programming (needs full knowledge)\n",
    "\n",
    "---\n",
    "\n",
    "## ‚úÖ Summary\n",
    "\n",
    "- **TD** = learn from next step, not full return  \n",
    "- **‚ÄúTemporal Difference‚Äù** = difference in value across time  \n",
    "- **‚ÄúSemi-gradient‚Äù** = you only compute gradient on the prediction, not the whole TD target"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
